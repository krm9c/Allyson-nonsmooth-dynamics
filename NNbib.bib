@online{adegokeHigherDerivativesInverse2016,
  title = {The Higher Derivatives of the Inverse Tangent Function and Rapidly Convergent {{BBP-type}} Formulas},
  author = {Adegoke, Kunle and Layeni, Olawanle},
  date = {2016-03-20},
  eprint = {1603.08540},
  eprinttype = {arxiv},
  eprintclass = {math},
  url = {http://arxiv.org/abs/1603.08540},
  urldate = {2023-08-16},
  abstract = {We give a closed formula for the \$n\textasciicircum\{th\}\$ derivative of \$\textbackslash arctan x\$. A new expansion for \$\textbackslash arctan x\$ is also obtained and rapidly convergent series for \$\textbackslash pi\$ and \$\textbackslash pi\textbackslash sqrt 3\$ are derived.},
  pubstate = {preprint},
  keywords = {30D10 40A25,Mathematics - Number Theory},
  file = {/Users/krishnanraghavan/Zotero/storage/323K4EYC/Adegoke_Layeni_2016_The higher derivatives of the inverse tangent function and rapidly convergent.pdf;/Users/krishnanraghavan/Zotero/storage/8JX4RW77/1603.html}
}

@article{cavaliereClassicalApproximateTaylor2014,
  title = {Classical and Approximate {{Taylor}} Expansions of Weakly Differentiable Functions},
  author = {Cavaliere, Paola and Cianchi, Andrea},
  date = {2014-07},
  journaltitle = {Annales Academiae Scientiarum Fennicae Mathematica},
  shortjournal = {Ann. Acad. Sci. Fenn. Math.},
  volume = {39},
  pages = {527--544},
  issn = {1239629X, 17982383},
  doi = {10.5186/aasfm.2014.3933},
  url = {http://www.acadsci.fi/mathematica/Vol39/vol39pp527-544.pdf},
  urldate = {2023-08-16},
  abstract = {The pointwise behavior of Sobolev-type functions, whose weak derivatives up to a given order belong to some rearrangement-invariant Banach function space, is investigated. We introduce a notion of approximate Taylor expansion in norm for these functions, which extends the usual definition of Taylor expansion in Lp-sense for standard Sobolev functions. An approximate Taylor expansion for functions in arbitrary-order Sobolev-type spaces, with sharp norm, is established. As a consequence, a characterization of those Sobolev-type spaces in which all functions admit a classical Taylor expansion is derived. In particular, this provides a higher-order version of a well-known result of Stein [27] on the differentiability of weakly differentiable functions. Applications of our results to customary classes of Sobolev-type spaces are also presented.},
  langid = {english},
  file = {/Users/krishnanraghavan/Zotero/storage/PSAF6YD2/Cavaliere and Cianchi - 2014 - Classical and approximate Taylor expansions of wea.pdf}
}

@online{ClassicalApproximateTaylor,
  title = {Classical and Approximate {{Taylor}} Expansions of Weakly Differentiable Functions | {{Request PDF}}},
  url = {https://www.researchgate.net/publication/265813046_Classical_and_approximate_Taylor_expansions_of_weakly_differentiable_functions},
  urldate = {2023-08-16},
  file = {/Users/krishnanraghavan/Zotero/storage/IXWKN4EM/265813046_Classical_and_approximate_Taylor_expansions_of_weakly_differentiable_functions.html}
}

@article{cybenkoApproximationSuperpositionsSigmoidal1989,
  title = {Approximation by Superpositions of a Sigmoidal Function},
  author = {Cybenko, G.},
  date = {1989-12-01},
  journaltitle = {Mathematics of Control, Signals and Systems},
  shortjournal = {Math. Control Signal Systems},
  volume = {2},
  number = {4},
  pages = {303--314},
  issn = {1435-568X},
  doi = {10.1007/BF02551274},
  url = {https://doi.org/10.1007/BF02551274},
  urldate = {2023-08-16},
  abstract = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},
  langid = {english},
  keywords = {Approximation,Completeness,Neural networks},
  file = {/Users/krishnanraghavan/Zotero/storage/PTJJLRRS/Cybenko_1989_Approximation by superpositions of a sigmoidal function.pdf}
}

@online{guhringExpressivityDeepNeural2020,
  title = {Expressivity of {{Deep Neural Networks}}},
  author = {Gühring, Ingo and Raslan, Mones and Kutyniok, Gitta},
  date = {2020-07-09},
  eprint = {2007.04759},
  eprinttype = {arxiv},
  eprintclass = {cs, math, stat},
  url = {http://arxiv.org/abs/2007.04759},
  urldate = {2023-08-16},
  abstract = {In this review paper, we give a comprehensive overview of the large variety of approximation results for neural networks. Approximation rates for classical function spaces as well as benefits of deep neural networks over shallow ones for specifically structured function classes are discussed. While the mainbody of existing results is for general feedforward architectures, we also depict approximation results for convolutional, residual and recurrent neural networks.},
  pubstate = {preprint},
  keywords = {41-02 41-03 68T07 68Q32,Computer Science - Machine Learning,Mathematics - Functional Analysis,Statistics - Machine Learning},
  file = {/Users/krishnanraghavan/Zotero/storage/I9WWAFMV/Gühring et al_2020_Expressivity of Deep Neural Networks.pdf;/Users/krishnanraghavan/Zotero/storage/YIS79QRV/2007.html}
}

@article{hornikMultilayerFeedforwardNetworks1989,
  title = {Multilayer Feedforward Networks Are Universal Approximators},
  author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  date = {1989-01-01},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {2},
  number = {5},
  pages = {359--366},
  issn = {0893-6080},
  doi = {10.1016/0893-6080(89)90020-8},
  url = {https://www.sciencedirect.com/science/article/pii/0893608089900208},
  urldate = {2023-08-16},
  abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.},
  keywords = {Back-propagation networks,Feedforward networks,Mapping networks,Network representation capability,Sigma-Pi networks,Squashing functions,Stone-Weierstrass Theorem,Universal approximation},
  file = {/Users/krishnanraghavan/Zotero/storage/VWZ9ZGNA/Hornik et al_1989_Multilayer feedforward networks are universal approximators.pdf;/Users/krishnanraghavan/Zotero/storage/93VJ5JBB/0893608089900208.html}
}

@online{kingmaAdamMethodStochastic2017,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  date = {2017-01-29},
  eprint = {1412.6980},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1412.6980},
  url = {http://arxiv.org/abs/1412.6980},
  urldate = {2023-08-18},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/krishnanraghavan/Zotero/storage/2DBLPE3F/Kingma_Ba_2017_Adam.pdf;/Users/krishnanraghavan/Zotero/storage/IFLWUSYE/1412.html}
}

@article{koldaTensorDecompositionsApplications2009,
  title = {Tensor {{Decompositions}} and {{Applications}}},
  author = {Kolda, Tamara G. and Bader, Brett W.},
  date = {2009-08-06},
  journaltitle = {SIAM Review},
  shortjournal = {SIAM Rev.},
  volume = {51},
  number = {3},
  pages = {455--500},
  issn = {0036-1445, 1095-7200},
  doi = {10.1137/07070111X},
  url = {http://epubs.siam.org/doi/10.1137/07070111X},
  urldate = {2023-08-16},
  abstract = {This survey provides an overview of higher-order tensor decompositions, their applications, and available software. A tensor is a multidimensional or N -way array. Decompositions of higher-order tensors (i.e., N -way arrays with N ≥ 3) have applications in psychometrics, chemometrics, signal processing, numerical linear algebra, computer vision, numerical analysis, data mining, neuroscience, graph analysis, and elsewhere. Two particular tensor decompositions can be considered to be higher-order extensions of the matrix singular value decomposition: CANDECOMP/PARAFAC (CP) decomposes a tensor as a sum of rank-one tensors, and the Tucker decomposition is a higher-order form of principal component analysis. There are many other tensor decompositions, including INDSCAL, PARAFAC2, CANDELINC, DEDICOM, and PARATUCK2 as well as nonnegative variants of all of the above. The N-way Toolbox, Tensor Toolbox, and Multilinear Engine are examples of software packages for working with tensors.},
  langid = {english},
  file = {/Users/krishnanraghavan/Zotero/storage/D7E9RZER/Kolda and Bader - 2009 - Tensor Decompositions and Applications.pdf}
}

@article{koldaTensorDecompositionsApplications2009a,
  title = {Tensor {{Decompositions}} and {{Applications}}},
  author = {Kolda, Tamara G. and Bader, Brett W.},
  date = {2009-08-06},
  journaltitle = {SIAM Review},
  shortjournal = {SIAM Rev.},
  volume = {51},
  number = {3},
  pages = {455--500},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0036-1445},
  doi = {10.1137/07070111X},
  url = {https://epubs.siam.org/doi/10.1137/07070111X},
  urldate = {2023-08-16},
  abstract = {In this paper, the term tensor refers simply to a multidimensional or N-way array, and we consider how specially structured tensors allow for efficient storage and computation. First, we study sparse tensors, which have the property that the vast majority of the elements are zero. We propose storing sparse tensors using coordinate format and describe the computational efficiency of this scheme for various mathematical operations, including those typical to tensor decomposition algorithms. Second, we study factored tensors, which have the property that they can be assembled from more basic components. We consider two specific types: A Tucker tensor can be expressed as the product of a core tensor (which itself may be dense, sparse, or factored) and a matrix along each mode, and a Kruskal tensor can be expressed as the sum of rank-1 tensors. We are interested in the case where the storage of the components is less than the storage of the full tensor, and we demonstrate that many elementary operations can be computed using only the components. All of the efficiencies described in this paper are implemented in the Tensor Toolbox for MATLAB.},
  file = {/Users/krishnanraghavan/Zotero/storage/MV3WNIAJ/Kolda_Bader_2009_Tensor Decompositions and Applications.pdf}
}

@article{LpTaylorApproximationsCharacterize2015,
  title = {Lp-{{Taylor}} Approximations Characterize the {{Sobolev}} Space {{W1}},p},
  date = {2015-04-01},
  journaltitle = {Comptes Rendus Mathematique},
  volume = {353},
  number = {4},
  pages = {327--332},
  publisher = {No longer published by Elsevier},
  issn = {1631-073X},
  doi = {10.1016/j.crma.2015.01.010},
  url = {https://www.sciencedirect.com/science/article/pii/S1631073X15000369},
  urldate = {2023-08-16},
  abstract = {In this note, we introduce a variant of Calderón and Zygmund's notion of Lp-differentiability – an Lp-Taylor approximation. Our first result is that f…},
  langid = {american},
  file = {/Users/krishnanraghavan/Zotero/storage/HMFJIEDT/2015_Lp-Taylor approximations characterize the Sobolev space W1,p.pdf;/Users/krishnanraghavan/Zotero/storage/T3W9MVFV/S1631073X15000369.html}
}

@article{mahanNonclosednessSetsNeural2021a,
  title = {Nonclosedness of Sets of Neural Networks in {{Sobolev}} Spaces},
  author = {Mahan, Scott and King, Emily J. and Cloninger, Alex},
  date = {2021-05-01},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {137},
  pages = {85--96},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2021.01.007},
  url = {https://www.sciencedirect.com/science/article/pii/S0893608021000150},
  urldate = {2023-08-16},
  abstract = {We examine the closedness of sets of realized neural networks of a fixed architecture in Sobolev spaces. For an exactly m-times differentiable activation function ρ, we construct a sequence of neural networks (Φn)n∈N whose realizations converge in order-(m−1) Sobolev norm to a function that cannot be realized exactly by a neural network. Thus, sets of realized neural networks are not closed in order-(m−1) Sobolev spaces Wm−1,p for p∈[1,∞). We further show that these sets are not closed in Wm,p under slightly stronger conditions on the mth derivative of ρ. For a real analytic activation function, we show that sets of realized neural networks are not closed in Wk,p for any k∈N. The nonclosedness allows for approximation of non-network target functions with unbounded parameter growth. We partially characterize the rate of parameter growth for most activation functions by showing that a specific sequence of realized neural networks can approximate the activation function’s derivative with weights increasing inversely proportional to the Lp approximation error. Finally, we present experimental results showing that networks are capable of closely approximating non-network target functions with increasing parameters via training.},
  keywords = {Closedness,Fixed-architecture neural networks,Neural network expressivity,Sobolev space},
  file = {/Users/krishnanraghavan/Zotero/storage/CQD2IWAA/Mahan et al_2021_Nonclosedness of sets of neural networks in Sobolev spaces.pdf;/Users/krishnanraghavan/Zotero/storage/QH7RIIAW/S0893608021000150.html}
}

@article{petersenTopologicalPropertiesSet2021a,
  title = {Topological {{Properties}} of the {{Set}} of {{Functions Generated}} by {{Neural Networks}} of {{Fixed Size}}},
  author = {Petersen, Philipp and Raslan, Mones and Voigtlaender, Felix},
  date = {2021-04-01},
  journaltitle = {Foundations of Computational Mathematics},
  shortjournal = {Found Comput Math},
  volume = {21},
  number = {2},
  pages = {375--444},
  issn = {1615-3383},
  doi = {10.1007/s10208-020-09461-0},
  url = {https://doi.org/10.1007/s10208-020-09461-0},
  urldate = {2023-08-16},
  abstract = {We analyze the topological properties of the set of functions that can be implemented by neural networks of a fixed size. Surprisingly, this set has many undesirable properties. It is highly non-convex, except possibly for a few exotic activation functions. Moreover, the set is not closed with respect to \$\$L\textasciicircum p\$\$-norms, \$\$0{$<$} p {$<$} \textbackslash infty \$\$, for all practically used activation functions, and also not closed with respect to the \$\$L\textasciicircum\textbackslash infty \$\$-norm for all practically used activation functions except for the ReLU and the parametric ReLU. Finally, the function that maps a family of weights to the function computed by the associated network is not inverse stable for every practically used activation function. In other words, if \$\$f\_1, f\_2\$\$are two functions realized by neural networks and if \$\$f\_1, f\_2\$\$are close in the sense that \$\$\textbackslash Vert f\_1 - f\_2\textbackslash Vert \_\{L\textasciicircum\textbackslash infty \} \textbackslash le \textbackslash varepsilon \$\$for \$\$\textbackslash varepsilon {$>$} 0\$\$, it is, regardless of the size of \$\$\textbackslash varepsilon \$\$, usually not possible to find weights \$\$w\_1, w\_2\$\$close together such that each \$\$f\_i\$\$is realized by a neural network with weights \$\$w\_i\$\$. Overall, our findings identify potential causes for issues in the training procedure of deep learning such as no guaranteed convergence, explosion of parameters, and slow convergence.},
  langid = {english},
  keywords = {52A30,54H99,68T05,Closedness,Convexity,General topology,Learning,Neural networks},
  file = {/Users/krishnanraghavan/Zotero/storage/LKP6YFXX/Petersen et al_2021_Topological Properties of the Set of Functions Generated by Neural Networks of.pdf}
}

@online{TensorDecompositionsApplications,
  title = {Tensor {{Decompositions}} and {{Applications}} | {{SIAM Review}}},
  url = {https://epubs.siam.org/doi/10.1137/07070111X},
  urldate = {2023-08-16},
  file = {/Users/krishnanraghavan/Zotero/storage/5TUER282/07070111X.html}
}

@article{hahn2023binary,
  title={Binary optimal control by trust-region steepest descent},
  author={Hahn, Mirko and Leyffer, Sven and Sager, Sebastian},
  journal={Mathematical Programming},
  volume={197},
  number={1},
  pages={147--190},
  year={2023},
  publisher={Springer}}

@article{larson2019derivative,
  title={Derivative-free optimization methods},
  author={Larson, Jeffrey and Menickelly, Matt and Wild, Stefan M},
  journal={Acta Numerica},
  volume={28},
  pages={287--404},
  year={2019},
  publisher={Cambridge University Press}
}

@article{kolda2009tensor,
  title={Tensor decompositions and applications},
  author={Kolda, Tamara G and Bader, Brett W},
  journal={SIAM review},
  volume={51},
  number={3},
  pages={455--500},
  year={2009},
  publisher={SIAM}
}
@book{weaver2013measure,
  title={Measure theory and functional analysis},
  author={Weaver, Nik},
  year={2013},
  publisher={World Scientific Publishing Company}
}

@article{pons2014real,
  title={Real Analysis for the Undergraduate},
  author={Pons, Matthew A},
  journal={Springer-Verlag},
  volume={10},
  pages={978--1},
  year={2014},
  publisher={Springer}
}

@article{raghavan2021formalizing,
  title={Formalizing the generalization-forgetting trade-off in continual learning},
  author={Raghavan, Krishnan and Balaprakash, Prasanna},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={17284--17297},
  year={2021}
}

@article{spector2015lp,
  title={Lp-Taylor approximations characterize the Sobolev space W1, p},
  author={Spector, Daniel},
  journal={Comptes Rendus Mathematique},
  volume={353},
  number={4},
  pages={327--332},
  year={2015},
  publisher={Elsevier}
}

@book{evans2022partial,
  title={Partial differential equations},
  author={Evans, Lawrence C},
  volume={19},
  year={2022},
  publisher={American Mathematical Society}
}

@article{hsu2018re,
  title={Re-evaluating continual learning scenarios: A categorization and case for strong baselines},
  author={Hsu, Yen-Chang and Liu, Yen-Cheng and Ramasamy, Anita and Kira, Zsolt},
  journal={arXiv preprint arXiv:1810.12488},
  year={2018}
}