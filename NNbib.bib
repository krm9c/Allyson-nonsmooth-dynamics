% ------------------------------------------------------
% ------------------------------------------------------
@article{luo2025empirical,
  title={An empirical study of catastrophic forgetting in large language models during continual fine-tuning},
  author={Luo, Yun and Yang, Zhen and Meng, Fandong and Li, Yafu and Zhou, Jie and Zhang, Yue},
  journal={IEEE Transactions on Audio, Speech and Language Processing},
  year={2025},
  publisher={IEEE}
}
@article{Lake_2015,
  title={Human-level concept learning through probabilistic program induction},
  author={Lake, Brenden M and Salakhutdinov, Ruslan and Tenenbaum, Joshua B},
  journal={Science},
  volume={350},
  number={6266},
  pages={1332--1338},
  year={2015},
  publisher={American Association for the Advancement of Science},
  doi={10.1126/science.aab3050}
}
@inproceedings{Balaprakash2018DeepHyper,
  author    = {Prasanna Balaprakash and Misha Salim and Taylor Uram and Venkatram Vishwanath and Stefan Wild},
  title     = {{DeepHyper}: Asynchronous Hyperparameter Search for Deep Neural Networks},
  booktitle = {2018 IEEE 25th International Conference on High Performance Computing (HiPC)},
  pages     = {42--51},
  year      = {2018},
  address   = {Bengaluru, India},
  doi       = {10.1109/HiPC.2018.00013},
  publisher = {IEEE}
}

@inproceedings{GlorotB10,
  title={Understanding the difficulty of training deep feedforward neural networks},
  author={Glorot, Xavier and Bengio, Yoshua},
  booktitle={Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS)},
  pages={249--256},
  year={2010},
  publisher={JMLR Workshop and Conference Proceedings},
  url={proceedings.mlr.press}
}


@misc{jax,
  title={{JAX}: Composable transformations of {Python} + {NumPy} programs},
  author={frostig, jacob and sohl-dickstein, jascha and stephens, sharad and adigun, akintunde and bahri, yasaman and bard, noam and holliday, ben and doucet, arnaud and levenberg, matthew and mayne, alex and oord, aaron van den and pfau, david and simonyan, karen and slancman, paul and sussex, andy and vadgama, ashish and vanhoucke, vincent and wehnert, rory and zoph, barret},
  url={github.com},
  version={0.4.30},
  year={2018}
}
@article{kidger2021equinox,
  author={Patrick Kidger and Cristian Garcia},
  title={{E}quinox: neural networks in {JAX} via callable {P}y{T}rees and filtered transformations},
  year={2021},
  journal={Differentiable Programming workshop at Neural Information Processing Systems 2021}
}

@misc{raghavan2023learningcontinuallysequencegraphs,
      title={Learning Continually on a Sequence of Graphs -- The Dynamical System Way}, 
      author={Krishnan Raghavan and Prasanna Balaprakash},
      year={2023},
      eprint={2305.12030},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2305.12030}, 
}
@software{pyg,
  title = {Pytorch geomteric library},
  author = {Fey, Matthias and Lenssen, Jan Eric and contributors},
  year = {2025},
  howpublished = {PyTorch Geometric},
  url = {https://pytorch-geometric.readthedocs.io/}
}
@article{loshchilov2017fixing,
  title={Fixing weight decay regularization in {Adam}},
  author={Loshchilov, Ilya and Hutter, Frank and others},
  journal={arXiv preprint arXiv:1711.05101},
  volume={5},
  number={5},
  pages={5},
  year={2017}
}
@article{lai2025reinforcement,
  title={Reinforcement fine-tuning naturally mitigates forgetting in continual post-training},
  author={Lai, Song and Zhao, Haohan and Feng, Rong and Ma, Changyi and Liu, Wenzhuo and Zhao, Hongbo and Lin, Xi and Yi, Dong and Xie, Min and Zhang, Qingfu and others},
  journal={arXiv preprint arXiv:2507.05386},
  year={2025}
}
@article{biderman2024lora,
title={ {LoRA}learns less and forgets less},
  author={Biderman, Dan and Portes, Jacob and Ortiz, Jose Javier Gonzalez and Paul, Mansheej and Greengard, Philip and Jennings, Connor and King, Daniel and Havens, Sam and Chiley, Vitaliy and Frankle, Jonathan and others},
  journal={arXiv preprint arXiv:2405.09673},
  year={2024}
}
@article{lu2025rethinking,
  title={Rethinking the stability-plasticity trade-off in continual learning from an architectural perspective},
  author={Lu, Aojun and Yuan, Hangjie and Feng, Tao and Sun, Yanan},
  journal={arXiv preprint arXiv:2506.03951},
  year={2025}
}
@inproceedings{lin2023theory,
  title={Theory on forgetting and generalization of continual learning},
  author={Lin, Sen and Ju, Peizhong and Liang, Yingbin and Shroff, Ness},
  booktitle={International Conference on Machine Learning},
  pages={21078--21100},
  year={2023},
  organization={PMLR}
}
@incollection{mccloskey1989catastrophic,
  title={Catastrophic interference in connectionist networks: The sequential learning problem},
  author={McCloskey, Michael and Cohen, Neal J},
  booktitle={Psychology of learning and motivation},
  volume={24},
  pages={109--165},
  year={1989},
  publisher={Elsevier}
}

@article{lin2025continual,
  title={Continual Learning via Sparse Memory Finetuning},
  author={Lin, Jessy and Zettlemoyer, Luke and Ghosh, Gargi and Yih, Wen-Tau and Markosyan, Aram and Berges, Vincent-Pierre and O{\u{g}}uz, Barlas},
  journal={arXiv preprint arXiv:2510.15103},
  year={2025}
}
% ------------------------------------------------------
% ------------------------------------------------------
@online{adegokeHigherDerivativesInverse2016,
  title = {The Higher Derivatives of the Inverse Tangent Function and Rapidly Convergent {{BBP-type}} Formulas},
  author = {Adegoke, Kunle and Layeni, Olawanle},
  date = {2016-03-20},
  eprint = {1603.08540},
  eprinttype = {arxiv},
  eprintclass = {math},
  url = {http://arxiv.org/abs/1603.08540},
  urldate = {2023-08-16},
  abstract = {We give a closed formula for the \$n\textasciicircum\{th\}\$ derivative of \$\textbackslash arctan x\$. A new expansion for \$\textbackslash arctan x\$ is also obtained and rapidly convergent series for \$\textbackslash pi\$ and \$\textbackslash pi\textbackslash sqrt 3\$ are derived.},
  pubstate = {preprint},
  keywords = {30D10 40A25,Mathematics - Number Theory},
  file = {/Users/krishnanraghavan/Zotero/storage/323K4EYC/Adegoke_Layeni_2016_The higher derivatives of the inverse tangent function and rapidly convergent.pdf;/Users/krishnanraghavan/Zotero/storage/8JX4RW77/1603.html}
}

@article{cavaliereClassicalApproximateTaylor2014,
  title = {Classical and Approximate {{Taylor}} Expansions of Weakly Differentiable Functions},
  author = {Cavaliere, Paola and Cianchi, Andrea},
  date = {2014-07},
  journaltitle = {Annales Academiae Scientiarum Fennicae Mathematica},
  shortjournal = {Ann. Acad. Sci. Fenn. Math.},
  volume = {39},
  pages = {527--544},
  issn = {1239629X, 17982383},
  doi = {10.5186/aasfm.2014.3933},
  url = {http://www.acadsci.fi/mathematica/Vol39/vol39pp527-544.pdf},
  urldate = {2023-08-16},
  abstract = {The pointwise behavior of Sobolev-type functions, whose weak derivatives up to a given order belong to some rearrangement-invariant Banach function space, is investigated. We introduce a notion of approximate Taylor expansion in norm for these functions, which extends the usual definition of Taylor expansion in Lp-sense for standard Sobolev functions. An approximate Taylor expansion for functions in arbitrary-order Sobolev-type spaces, with sharp norm, is established. As a consequence, a characterization of those Sobolev-type spaces in which all functions admit a crssical Taylor expansion is derived. In particular, this provides a higher-order version of a well-known result of Stein [27] on the differentiability of weakly differentiable functions. Applications of our results to customary classes of Sobolev-type spaces are also presented.},
  langid = {english},
  file = {/Users/krishnanraghavan/Zotero/storage/PSAF6YD2/Cavaliere and Cianchi - 2014 - Classical and approximate Taylor expansions of wea.pdf}
}

@online{ClassicalApproximateTaylor,
  title = {Classical and Approximate {{Taylor}} Expansions of Weakly Differentiable Functions | {{Request PDF}}},
  url = {https://www.researchgate.net/publication/265813046_Classical_and_approximate_Taylor_expansions_of_weakly_differentiable_functions},
  urldate = {2023-08-16},
  file = {/Users/krishnanraghavan/Zotero/storage/IXWKN4EM/265813046_Classical_and_approximate_Taylor_expansions_of_weakly_differentiable_functions.html}
}

@article{cybenkoApproximationSuperpositionsSigmoidal1989,
  title = {Approximation by Superpositions of a Sigmoidal Function},
  author = {Cybenko, G.},
  date = {1989-12-01},
  journaltitle = {Mathematics of Control, Signals and Systems},
  shortjournal = {Math. Control Signal Systems},
  volume = {2},
  number = {4},
  pages = {303--314},
  issn = {1435-568X},
  doi = {10.1007/BF02551274},
  url = {https://doi.org/10.1007/BF02551274},
  urldate = {2023-08-16},
  abstract = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},
  langid = {english},
  keywords = {Approximation,Completeness,Neural networks},
  file = {/Users/krishnanraghavan/Zotero/storage/PTJJLRRS/Cybenko_1989_Approximation by superpositions of a sigmoidal function.pdf}
}

@online{guhringExpressivityDeepNeural2020,
  title = {Expressivity of Deep Neural Networks},
  author = {Gühring, Ingo and Raslan, Mones and Kutyniok, Gitta},
  date = {2020-07-09},
  eprint = {2007.04759},
  eprinttype = {arxiv},
  eprintclass = {cs, math, stat},
  url = {http://arxiv.org/abs/2007.04759},
  urldate = {2023-08-16},
  abstract = {In this review paper, we give a comprehensive overview of the large variety of approximation results for neural networks. Approximation rates for classical function spaces as well as benefits of deep neural networks over shallow ones for specifically structured function classes are discussed. While the mainbody of existing results is for general feedforward architectures, we also depict approximation results for convolutional, residual and recurrent neural networks.},
  pubstate = {preprint},
  keywords = {41-02 41-03 68T07 68Q32,Computer Science - Machine Learning,Mathematics - Functional Analysis,Statistics - Machine Learning},
  file = {/Users/krishnanraghavan/Zotero/storage/I9WWAFMV/Gühring et al_2020_Expressivity of Deep Neural Networks.pdf;/Users/krishnanraghavan/Zotero/storage/YIS79QRV/2007.html}
}
@article{son2021sobolev,
  title={Sobolev training for physics informed neural networks},
  author={Son, Hwijae and Jang, Jin Woo and Han, Woo Jin and Hwang, Hyung Ju},
  journal={arXiv preprint arXiv:2101.08932},
  year={2021}
}
@book{bertsekas2012dynamic,
  title={Dynamic programming and optimal control: {Volume I}},
  author={Bertsekas, Dimitri},
  volume={4},
  year={2012},
  publisher={Athena scientific}
}
@article{hornikMultilayerFeedforwardNetworks1989,
  title = {Multilayer Feedforward Networks Are Universal Approximators},
  author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  date = {1989-01-01},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {2},
  number = {5},
  pages = {359--366},
  issn = {0893-6080},
  doi = {10.1016/0893-6080(89)90020-8},
  url = {https://www.sciencedirect.com/science/article/pii/0893608089900208},
  urldate = {2023-08-16},
  abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.},
  keywords = {Back-propagation networks,Feedforward networks,Mapping networks,Network representation capability,Sigma-Pi networks,Squashing functions,Stone-Weierstrass Theorem,Universal approximation},
  file = {/Users/krishnanraghavan/Zotero/storage/VWZ9ZGNA/Hornik et al_1989_Multilayer feedforward networks are universal approximators.pdf;/Users/krishnanraghavan/Zotero/storage/93VJ5JBB/0893608089900208.html}
}

@online{kingmaAdamMethodStochastic2017,
  title = {Adam: {A} Method for Stochastic Optimization},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  date = {2017-01-29},
  eprint = {1412.6980},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1412.6980},
  url = {http://arxiv.org/abs/1412.6980},
  urldate = {2023-08-18},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/krishnanraghavan/Zotero/storage/2DBLPE3F/Kingma_Ba_2017_Adam.pdf;/Users/krishnanraghavan/Zotero/storage/IFLWUSYE/1412.html}
}

@article{koldaTensorDecompositionsApplications2009,
  title = {Tensor Decompositions and Applications},
  author = {Kolda, Tamara G. and Bader, Brett W.},
  date = {2009-08-06},
  journaltitle = {SIAM Review},
  shortjournal = {SIAM Rev.},
  volume = {51},
  number = {3},
  pages = {455--500},
  issn = {0036-1445, 1095-7200},
  doi = {10.1137/07070111X},
  url = {http://epubs.siam.org/doi/10.1137/07070111X},
  urldate = {2023-08-16},
  abstract = {This survey provides an overview of higher-order tensor decompositions, their applications, and available software. A tensor is a multidimensional or N -way array. Decompositions of higher-order tensors (i.e., N -way arrays with N ≥ 3) have applications in psychometrics, chemometrics, signal processing, numerical linear algebra, computer vision, numerical analysis, data mining, neuroscience, graph analysis, and elsewhere. Two particular tensor decompositions can be considered to be higher-order extensions of the matrix singular value decomposition: CANDECOMP/PARAFAC (CP) decomposes a tensor as a sum of rank-one tensors, and the Tucker decomposition is a higher-order form of principal component analysis. There are many other tensor decompositions, including INDSCAL, PARAFAC2, CANDELINC, DEDICOM, and PARATUCK2 as well as nonnegative variants of all of the above. The N-way Toolbox, Tensor Toolbox, and Multilinear Engine are examples of software packages for working with tensors.},
  langid = {english},
  file = {/Users/krishnanraghavan/Zotero/storage/D7E9RZER/Kolda and Bader - 2009 - Tensor Decompositions and Applications.pdf}
}

@article{koldaTensorDecompositionsApplications2009a,
  title = {Tensor Decompositions and Applications},
  author = {Kolda, Tamara G. and Bader, Brett W.},
  date = {2009-08-06},
  journaltitle = {SIAM Review},
  shortjournal = {SIAM Rev.},
  volume = {51},
  number = {3},
  pages = {455--500},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0036-1445},
  doi = {10.1137/07070111X},
  url = {https://epubs.siam.org/doi/10.1137/07070111X},
  urldate = {2023-08-16},
  abstract = {In this paper, the term tensor refers simply to a multidimensional or N-way array, and we consider how specially structured tensors allow for efficient storage and computation. First, we study sparse tensors, which have the property that the vast majority of the elements are zero. We propose storing sparse tensors using coordinate format and describe the computational efficiency of this scheme for various mathematical operations, including those typical to tensor decomposition algorithms. Second, we study factored tensors, which have the property that they can be assembled from more basic components. We consider two specific types: A Tucker tensor can be expressed as the product of a core tensor (which itself may be dense, sparse, or factored) and a matrix along each mode, and a Kruskal tensor can be expressed as the sum of rank-1 tensors. We are interested in the case where the storage of the components is less than the storage of the full tensor, and we demonstrate that many elementary operations can be computed using only the components. All of the efficiencies described in this paper are implemented in the Tensor Toolbox for MATLAB.},
  file = {/Users/krishnanraghavan/Zotero/storage/MV3WNIAJ/Kolda_Bader_2009_Tensor Decompositions and Applications.pdf}
}

@article{LpTaylorApproximationsCharacterize2015,
  title = {Lp-{{Taylor}} Approximations Characterize the {{Sobolev}} Space {{W1}},p},
author = {Spector, D.},
  date = {2015-04-01},
  journaltitle = {Comptes Rendus Mathematique},
  volume = {353},
  number = {4},
  pages = {327--332},
  publisher = {No longer published by Elsevier},
  issn = {1631-073X},
  doi = {10.1016/j.crma.2015.01.010},
  url = {https://www.sciencedirect.com/science/article/pii/S1631073X15000369},
  urldate = {2023-08-16},
  abstract = {In this note, we introduce a variant of Calderón and Zygmund's notion of Lp-differentiability – an Lp-Taylor approximation. Our first result is that f…},
  langid = {american},
  file = {/Users/krishnanraghavan/Zotero/storage/HMFJIEDT/2015_Lp-Taylor approximations characterize the Sobolev space W1,p.pdf;/Users/krishnanraghavan/Zotero/storage/T3W9MVFV/S1631073X15000369.html}
}

@article{mahanNonclosednessSetsNeural2021a,
  title = {Nonclosedness of Sets of Neural Networks in {Sobolev} Spaces},
  author = {Mahan, Scott and King, Emily J. and Cloninger, Alex},
  date = {2021-05-01},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {137},
  pages = {85--96},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2021.01.007},
  url = {https://www.sciencedirect.com/science/article/pii/S0893608021000150},
  urldate = {2023-08-16},
  abstract = {We examine the closedness of sets of realized neural networks of a fixed architecture in Sobolev spaces. For an exactly m-times differentiable activation function ρ, we construct a sequence of neural networks (Φn)n∈N whose realizations converge in order-(m−1) Sobolev norm to a function that cannot be realized exactly by a neural network. Thus, sets of realized neural networks are not closed in order-(m−1) Sobolev spaces Wm−1,p for p∈[1,∞). We further show that these sets are not closed in Wm,p under slightly stronger conditions on the mth derivative of ρ. For a real analytic activation function, we show that sets of realized neural networks are not closed in Wk,p for any k∈N. The nonclosedness allows for approximation of non-network target functions with unbounded parameter growth. We partially characterize the rate of parameter growth for most activation functions by showing that a specific sequence of realized neural networks can approximate the activation function’s derivative with weights increasing inversely proportional to the Lp approximation error. Finally, we present experimental results showing that networks are capable of closely approximating non-network target functions with increasing parameters via training.},
  keywords = {Closedness,Fixed-architecture neural networks,Neural network expressivity,Sobolev space},
  file = {/Users/krishnanraghavan/Zotero/storage/CQD2IWAA/Mahan et al_2021_Nonclosedness of sets of neural networks in Sobolev spaces.pdf;/Users/krishnanraghavan/Zotero/storage/QH7RIIAW/S0893608021000150.html}
}

@article{petersenTopologicalPropertiesSet2021a,
  title = {Topological Properties of the Set of Functions Generated by Neural Networks of Fixed Size},
  author = {Petersen, Philipp and Raslan, Mones and Voigtlaender, Felix},
  date = {2021-04-01},
  journaltitle = {Foundations of Computational Mathematics},
  shortjournal = {Found Comput Math},
  volume = {21},
  number = {2},
  pages = {375--444},
  issn = {1615-3383},
  doi = {10.1007/s10208-020-09461-0},
  url = {https://doi.org/10.1007/s10208-020-09461-0},
  urldate = {2023-08-16},
  abstract = {We analyze the topological properties of the set of functions that can be implemented by neural networks of a fixed size. Surprisingly, this set has many undesirable properties. It is highly non-convex, except possibly for a few exotic activation functions. Moreover, the set is not closed with respect to \$\$L\textasciicircum p\$\$-norms, \$\$0{$<$} p {$<$} \textbackslash infty \$\$, for all practically used activation functions, and also not closed with respect to the \$\$L\textasciicircum\textbackslash infty \$\$-norm for all practically used activation functions except for the ReLU and the parametric ReLU. Finally, the function that maps a family of weights to the function computed by the associated network is not inverse stable for every practically used activation function. In other words, if \$\$f\_1, f\_2\$\$are two functions realized by neural networks and if \$\$f\_1, f\_2\$\$are close in the sense that \$\$\textbackslash Vert f\_1 - f\_2\textbackslash Vert \_\{L\textasciicircum\textbackslash infty \} \textbackslash le \textbackslash varepsilon \$\$for \$\$\textbackslash varepsilon {$>$} 0\$\$, it is, regardless of the size of \$\$\textbackslash varepsilon \$\$, usually not possible to find weights \$\$w\_1, w\_2\$\$close together such that each \$\$f\_i\$\$is realized by a neural network with weights \$\$w\_i\$\$. Overall, our findings identify potential causes for issues in the training procedure of deep learning such as no guaranteed convergence, explosion of parameters, and slow convergence.},
  langid = {english},
  keywords = {52A30,54H99,68T05,Closedness,Convexity,General topology,Learning,Neural networks},
  file = {/Users/krishnanraghavan/Zotero/storage/LKP6YFXX/Petersen et al_2021_Topological Properties of the Set of Functions Generated by Neural Networks of.pdf}
}

@online{TensorDecompositionsApplications,
  title = {Tensor Decompositions and Applications | {SIAM Review}},
  url = {https://epubs.siam.org/doi/10.1137/07070111X},
  urldate = {2023-08-16},
  file = {/Users/krishnanraghavan/Zotero/storage/5TUER282/07070111X.html}
}

@article{hahn2023binary,
  title={Binary optimal control by trust-region steepest descent},
  author={Hahn, Mirko and Leyffer, Sven and Sager, Sebastian},
  journal={Mathematical Programming},
  volume={197},
  number={1},
  pages={147--190},
  year={2023},
  publisher={Springer}}

@article{larson2019derivative,
  title={Derivative-free optimization methods},
  author={Larson, Jeffrey and Menickelly, Matt and Wild, Stefan M},
  journal={Acta Numerica},
  volume={28},
  pages={287--404},
  year={2019},
  publisher={Cambridge University Press}
}

@article{kolda2009tensor,
  title={Tensor decompositions and applications},
  author={Kolda, Tamara G and Bader, Brett W},
  journal={SIAM Review},
  volume={51},
  number={3},
  pages={455--500},
  year={2009},
  publisher={SIAM}
}
@book{weaver2013measure,
  title={Measure theory and functional analysis},
  author={Weaver, Nik},
  year={2013},
  publisher={World Scientific Publishing Company}
}

@article{pons2014real,
  title={Real Analysis for the Undergraduate},
  author={Pons, Matthew A},
  journal={Springer-Verlag},
  volume={10},
  pages={978--1},
  year={2014},
  publisher={Springer}
}


@article{spector2015lp,
  title={Lp-Taylor approximations characterize the {Sobolev space W1, p}},
  author={Spector, Daniel},
  journal={Comptes Rendus Mathematique},
  volume={353},
  number={4},
  pages={327--332},
  year={2015},
  publisher={Elsevier}
}

@book{evans2022partial,
  title={Partial differential equations},
  author={Evans, Lawrence C},
  volume={19},
  year={2022},
  publisher={American Mathematical Society}
}

@article{hsu2018re,
  title={Re-evaluating continual learning scenarios: A categorization and case for strong baselines},
  author={Hsu, Yen-Chang and Liu, Yen-Cheng and Ramasamy, Anita and Kira, Zsolt},
  journal={arXiv preprint arXiv:1810.12488},
  year={2018}
}

@article{yoon2017lifelong,
  title={Lifelong learning with dynamically expandable networks},
  author={Yoon, Jaehong and Yang, Eunho and Lee, Jeongtae and Hwang, Sung Ju},
  journal={arXiv preprint arXiv:1708.01547},
  year={2017}
}
@article{rusu2016progressive,
  title={Progressive neural networks},
  author={Rusu, Andrei A and Rabinowitz, Neil C and Desjardins, Guillaume and Soyer, Hubert and Kirkpatrick, James and Kavukcuoglu, Koray and Pascanu, Razvan and Hadsell, Raia},
  journal={arXiv preprint arXiv:1606.04671},
  year={2016}
}

@article{liu2021survey,
  title={A survey on evolutionary neural architecture search},
  author={Liu, Yuqiao and Sun, Yanan and Xue, Bing and Zhang, Mengjie and Yen, Gary G and Tan, Kay Chen},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  volume={34},
  number={2},
  pages={550--570},
  year={2021},
  publisher={IEEE}
}

@article{SEALgambella2025SEAL,
  title={{SEAL: Searching} Expandable Architectures for Incremental Learning},
  author={Gambella, Matteo and Solar, Vicente Javier Castro and Roveri, Manuel},
  journal={arXiv preprint arXiv:2505.10457},
  year={2025}
}

@article{CLEASgao2022CLEAS,
  title={Efficient architecture search for continual learning},
  author={Gao, Qiang and Luo, Zhipeng and Klabjan, Diego and Zhang, Fengli},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  volume={34},
  number={11},
  pages={8555--8565},
  year={2022},
  publisher={IEEE}
}

@article{CLORAmuralidhara2025,
  title={{CLoRA: Parameter}-Efficient Continual Learning with Low-Rank Adaptation},
  author={Muralidhara, Shishir and Stricker, Didier and Schuster, Ren{\'e}},
  journal={arXiv preprint arXiv:2507.19887},
  year={2025}
}

@article{LORAhu2022lora,
  title={Lora: Low-rank adaptation of large language models.},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu and others},
  journal={ICLR},
  volume={1},
  number={2},
  pages={3},
  year={2022}
}

@article{han2024parameter,
  title={Parameter-efficient fine-tuning for large models: A comprehensive survey},
  author={Han, Zeyu and Gao, Chao and Liu, Jinyang and Zhang, Jeff and Zhang, Sai Qian},
  journal={arXiv preprint arXiv:2403.14608},
  year={2024}
}

@article{he2024sparse,
  title={Sparse matrix in large language model fine-tuning},
  author={He, Haoze and Li, Juncheng Billy and Jiang, Xuan and Miller, Heather},
  journal={arXiv preprint arXiv:2405.15525},
  year={2024}
}

@article{CASpasunuru2019continual,
  title={Continual and multi-task architecture search},
  author={Pasunuru, Ramakanth and Bansal, Mohit},
  journal={arXiv preprint arXiv:1906.05226},
  year={2019}
}

@article{wistuba2023continual,
  title={Continual learning with low rank adaptation},
  author={Wistuba, Martin and Sivaprasad, Prabhu Teja and Balles, Lukas and Zapper, Giovanni},
  journal={arXiv preprint arXiv:2311.17601},
  year={2023}
}

@article{chakraborty2025understanding,
  title={On Understanding of the Dynamics of Model Capacity in Continual Learning},
  author={Chakraborty, Supriyo and Raghavan, Krishnan},
  journal={arXiv preprint arXiv:2508.08052},
  year={2025}
}

@article{raghavan2021formalizing,
  title={Formalizing the generalization-forgetting trade-off in continual learning},
  author={Raghavan, Krishnan and Balaprakash, Prasanna},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={17284--17297},
  year={2021}
}
@article{huang2023normalization,
  title={Normalization techniques in training dnns: Methodology, analysis and application},
  author={Huang, Lei and Qin, Jie and Zhou, Yi and Zhu, Fan and Liu, Li and Shao, Ling},
  journal={IEEE transactions on Pattern Analysis and Machine Intelligence},
  volume={45},
  number={8},
  pages={10173--10196},
  year={2023},
  publisher={IEEE}
}
@book{rudin1976principles,
  title={Principles of mathematical analysis},
  author={Rudin, Walter},
  publisher = {McGraw-Hill},
  edition={3rd ed.},
  year={1976}
}