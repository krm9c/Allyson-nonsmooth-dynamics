\section{Standard CL Implementation}
\label{app:standard_cl}

This appendix provides comprehensive implementation details for the Hamiltonian Continual Learning (HCL) framework with Adaptive Weight Basis (AWB). We present the complete algorithmic pipeline broken down into modular components.

\subsection{Overview}

The standard CL implementation consists of:
\begin{itemize}
    \item \textbf{Task 0}: Standard Hamiltonian-based training establishing the initial model
    \item \textbf{Tasks $t \geq 1$}: Full AWB pipeline with adaptive architecture morphing
    \item \textbf{Core Heuristics}: Task warmup, adaptive learning rates, adaptive gradient weights, balanced experience replay, and gradient normalization
\end{itemize}

All algorithms use the notation from the main text: $\weight(t)$ denotes weights at task $t$, $\psi(t)$ denotes architecture, $\tx(t)$ denotes task data, $\fhat(\weight,\psi)$ denotes the neural network, $J(\weight(t),\psi(t),\tx(t))$ denotes forgetting loss, and $V(t,\weight(t))$ denotes the cumulative loss (value function).

\subsection{Main Algorithms}

\subsubsection{High-Level Pipeline}

{\small
\begin{algorithm}[H]
\caption{Complete HCL Pipeline}\label{alg:complete_hcl}
\KwIn{Initial $\weight(0), \psi(0)$; tasks $\{\tx(0), \ldots, \tx(T)\}$}
\KwOut{Final $\weight(T), \psi(T)$}

\textbf{Initialize:} $\mathcal{E} \gets \emptyset$, $\mathcal{O}$, $[\alpha_0, \beta_0, \gamma_0] = [0.01, 0.98, 0.1]$, $\eta_0 = 10^{-4}$\;

\For{$t = 0$ \KwTo $T$}{
    \uIf{$t = 0$}{
        $\weight(t), \psi(t), \mathcal{E} \gets$ \textsc{Task0Training}$(\weight(0), \psi(0), \tx(0), \mathcal{E})$\;
    }
    \Else{
        $\weight(t), \psi(t), \mathcal{E} \gets$ \textsc{CLwithAWB}$(\weight(t-1), \psi(t-1), \tx(t), \mathcal{E}, t)$\;
    }
}
\textbf{Return} $\weight(T), \psi(T)$\;
\end{algorithm}
}

\subsubsection{Task 0 Training}

{\small
\begin{algorithm}[H]
\caption{Task 0: Standard Hamiltonian Training}\label{alg:task0}
\KwIn{$\weight(0), \psi(0), \tx(0), \mathcal{E}$}
\KwOut{Trained $\weight(0), \psi(0)$, updated $\mathcal{E}$}

$\eta \gets 10^{-4}$, $\alpha \gets 0.01$\;

\For{$e = 1$ \KwTo $N_{\text{epochs}}$}{
    $\mathcal{B} \gets$ \texttt{sample}$(\tx(0))$\;
    $\nabla_{\weight} H \gets \alpha \cdot \nabla_{\weight} \ell(\fhat(\weight(0), \psi(0)), \mathcal{B})$\;

    \If{$\|\nabla_{\weight} H\|_2 > 1.0$}{
        $\nabla_{\weight} H \gets \nabla_{\weight} H / \|\nabla_{\weight} H\|_2$\;
    }

    $\weight(0) \gets$ \texttt{optimizer\_step}$(\weight(0), \nabla_{\weight} H, \eta, \mathcal{O})$\;
    $\eta \gets$ \texttt{lr\_schedule}$(\eta, e, N_{\text{epochs}}, 10^{-6})$\;
}

$\mathcal{E} \gets \mathcal{E} \cup \{\text{samples from } \tx(0)\}$\;
\textbf{Return} $\weight(0), \psi(0), \mathcal{E}$\;
\end{algorithm}
}

\subsubsection{Continual Learning with AWB}

{\small
\begin{algorithm}[H]
\caption{CL with Adaptive Weight Basis}\label{alg:cl_awb}
\KwIn{$\weight(t-1), \psi(t-1), \tx(t), \mathcal{E}, t$}
\KwOut{$\weight(t), \psi(t), \mathcal{E}$}

$\weight(t) \gets \weight(t-1)$, $\psi(t) \gets \psi(t-1)$\;

\tcp{Step 1: Warmup}
$\weight(t) \gets$ \textsc{TaskWarmup}$(\weight(t), \psi(t), \tx(t))$\;

\tcp{Step 2: Adaptive hyperparameters}
$J_{\text{prev}} \gets J(\weight(t-1), \psi(t-1), \tx(t-1))$\;
$[\alpha, \beta, \gamma], \eta_{\min} \gets$ \textsc{AdaptiveHyperparams}$(J(\weight(t), \psi(t), \tx(t)), J_{\text{prev}})$\;

\tcp{Step 3: Preliminary training}
$\weight(t), J_{\text{new}} \gets$ \textsc{PreliminaryTraining}$(\weight(t), \psi(t), \tx(t), \mathcal{E}, t, [\alpha, \beta, \gamma], \eta_{\min})$\;

\tcp{Step 4: Architecture decision}
$\text{change} \gets$ \textsc{ShouldChangeArch}$(J_{\text{prev}}, J_{\text{new}})$\;

\uIf{$\text{change}$}{
    $\weight(t), \psi(t) \gets$ \textsc{AWBPipeline}$(\weight(t), \psi(t), \tx(t), \mathcal{E}, t, [\alpha, \beta, \gamma], \eta_{\min})$\;
}
\Else{
    $\weight(t) \gets$ \textsc{ContinueTraining}$(\weight(t), \psi(t), \tx(t), \mathcal{E}, t, [\alpha, \beta, \gamma], \eta_{\min})$\;
}

$\mathcal{E} \gets \mathcal{E} \cup \{\text{samples from } \tx(t)\}$\;
\textbf{Return} $\weight(t), \psi(t), \mathcal{E}$\;
\end{algorithm}
}

\subsubsection{Preliminary Training}

{\small
\begin{algorithm}[H]
\caption{Preliminary Training}\label{alg:prelim}
\KwIn{$\weight, \psi, \tx(t), \mathcal{E}, t, [\alpha, \beta, \gamma], \eta_{\min}$}
\KwOut{$\weight, J_{\text{new}}$}

$\eta \gets 10^{-4}$\;

\For{$e = 1$ \KwTo $N_{\text{prelim}} = 100$}{
    $\mathcal{B}_{\text{curr}} \gets$ \texttt{sample}$(\tx(t))$\;
    $\mathcal{B}_{\text{exp}} \gets$ \textsc{BalancedReplay}$(\mathcal{E}, t)$\;

    $\nabla_{\weight} H \gets$ \textsc{HamiltonianGrad}$(\weight, \psi, \mathcal{B}_{\text{curr}}, \mathcal{B}_{\text{exp}}, [\alpha, \beta, \gamma], t)$\;

    \If{$\|\nabla_{\weight} H\|_2 > 1.0$}{$\nabla_{\weight} H \gets \nabla_{\weight} H / \|\nabla_{\weight} H\|_2$\;}

    $\weight \gets$ \texttt{optimizer\_step}$(\weight, \nabla_{\weight} H, \eta, \mathcal{O})$\;
    $\eta \gets$ \texttt{lr\_schedule}$(\eta, e, N_{\text{prelim}}, \eta_{\min})$\;
}

$J_{\text{new}} \gets J(\weight, \psi, \tx(t))$\;
\textbf{Return} $\weight, J_{\text{new}}$\;
\end{algorithm}
}

\subsubsection{Continue Training}

{\small
\begin{algorithm}[H]
\caption{Continue Training (No Architecture Change)}\label{alg:continue_train}
\KwIn{$\weight, \psi, \tx(t), \mathcal{E}, t, [\alpha, \beta, \gamma], \eta_{\min}$}
\KwOut{$\weight$}

$\eta \gets 10^{-4}$, $N_{\text{rem}} \gets N_{\text{epochs}} - N_{\text{prelim}}$\;

\For{$e = 1$ \KwTo $N_{\text{rem}}$}{
    $\mathcal{B}_{\text{curr}} \gets$ \texttt{sample}$(\tx(t))$, $\mathcal{B}_{\text{exp}} \gets$ \textsc{BalancedReplay}$(\mathcal{E}, t)$\;

    $\nabla_{\weight} H \gets$ \textsc{HamiltonianGrad}$(\weight, \psi, \mathcal{B}_{\text{curr}}, \mathcal{B}_{\text{exp}}, [\alpha, \beta, \gamma], t)$\;

    \If{$\|\nabla_{\weight} H\|_2 > 1.0$}{$\nabla_{\weight} H \gets \nabla_{\weight} H / \|\nabla_{\weight} H\|_2$\;}

    $\weight \gets$ \texttt{optimizer\_step}$(\weight, \nabla_{\weight} H, \eta, \mathcal{O})$\;
    $\eta \gets$ \texttt{lr\_schedule}$(\eta, e, N_{\text{rem}}, \eta_{\min})$\;
}

\textbf{Return} $\weight$\;
\end{algorithm}
}

\subsubsection{AWB Pipeline}

{\small
\begin{algorithm}[H]
\caption{Adaptive Weight Basis Pipeline}\label{alg:awb_pipeline}
\KwIn{$\weight(t), \psi(t), \tx(t), \mathcal{E}, t, [\alpha, \beta, \gamma], \eta_{\min}$}
\KwOut{$\weight(t+1), \psi(t+1)$}

$\psi^*(t) \gets$ \textsc{NDDSSearch}$(\psi(t), \tx(t), \weight(t))$\;

\For{$i = 1, \ldots, d$}{
    $\mathrm{A}_i(t), \mathrm{B}_i(t) \gets$ \texttt{init\_AB}$(a_i, b_i, r_i, s_i)$\;
}

$\mathrm{A}(t), \mathrm{B}(t) \gets$ \textsc{TrainAB}$(\weight(t), \mathrm{A}(t), \mathrm{B}(t), \psi^*(t), \tx(t), t)$\;

$\weight(t+1) \gets \mathrm{A}(t) \cdot \weight(t) \cdot \mathrm{B}^T(t)$, $\psi(t+1) \gets \psi^*(t)$\;

$\weight(t+1) \gets$ \textsc{TrainV}$(\weight(t+1), \psi(t+1), \mathrm{A}(t), \mathrm{B}(t), \tx(t), \mathcal{E}, t, [\alpha, \beta, \gamma], \eta_{\min})$\;

\textbf{Return} $\weight(t+1), \psi(t+1)$\;
\end{algorithm}
}

\subsubsection{Train A/B Matrices}

{\small
\begin{algorithm}[H]
\caption{Train A/B with W Frozen}\label{alg:train_ab}
\KwIn{Frozen $\weight$, $\mathrm{A}(t), \mathrm{B}(t), \psi^*, \tx(t), t$}
\KwOut{$\mathrm{A}(t), \mathrm{B}(t)$}

$\epsilon_{\text{AB}} \gets 0.01 \cdot (1 + 0.1 \cdot t)^{-1}$, $\eta \gets 10^{-4}$\;

\For{$e = 1$ \KwTo $N_{\text{AB}} = 50$}{
    $\mathcal{B} \gets$ \texttt{sample}$(\tx(t))$\;
    $C(t) \gets \mathrm{A}(t) \cdot \weight \cdot \mathrm{B}^T(t)$\;
    $\nabla_{\mathrm{A}, \mathrm{B}} \gets \nabla_{\mathrm{A}, \mathrm{B}} \ell(\fhat(C(t), \psi^*), \mathcal{B})$\;
    $\mathrm{A}(t), \mathrm{B}(t) \gets$ \texttt{optimizer\_step}$(\mathrm{A}(t), \mathrm{B}(t), \nabla_{\mathrm{A}, \mathrm{B}}, \eta, \mathcal{O})$\;

    \If{loss converged within $\epsilon_{\text{AB}}$}{\textbf{break}\;}
}

\textbf{Return} $\mathrm{A}(t), \mathrm{B}(t)$\;
\end{algorithm}
}

\subsubsection{Train V}

{\small
\begin{algorithm}[H]
\caption{Train V with A/B Frozen}\label{alg:train_v}
\KwIn{$V, \psi$, frozen $\mathrm{A}, \mathrm{B}$, $\tx(t), \mathcal{E}, t, [\alpha, \beta, \gamma], \eta_{\min}$}
\KwOut{$V$}

$N_{\text{rem}} \gets N_{\text{epochs}} - N_{\text{prelim}} - N_{\text{AB}}$, $\eta \gets 10^{-4}$\;

\For{$e = 1$ \KwTo $N_{\text{rem}}$}{
    $\mathcal{B}_{\text{curr}} \gets$ \texttt{sample}$(\tx(t))$, $\mathcal{B}_{\text{exp}} \gets$ \textsc{BalancedReplay}$(\mathcal{E}, t)$\;

    $\nabla_{V} H \gets$ \textsc{HamiltonianGrad}$(V, \psi, \mathcal{B}_{\text{curr}}, \mathcal{B}_{\text{exp}}, [\alpha, \beta, \gamma], t)$\;

    \If{$\|\nabla_{V} H\|_2 > 1.0$}{$\nabla_{V} H \gets \nabla_{V} H / \|\nabla_{V} H\|_2$\;}

    $V \gets$ \texttt{optimizer\_step}$(V, \nabla_{V} H, \eta, \mathcal{O})$\;
    $\eta \gets$ \texttt{lr\_schedule}$(\eta, e, N_{\text{rem}}, \eta_{\min})$\;
}

\textbf{Return} $V$\;
\end{algorithm}
}

\subsection{Supporting Algorithms}

\subsubsection{Hamiltonian Gradient}

{\small
\begin{algorithm}[H]
\caption{Hamiltonian Gradient Computation}\label{alg:hamiltonian}
\KwIn{$\weight, \psi, \mathcal{B}_{\text{curr}}, \mathcal{B}_{\text{exp}}, [\alpha, \beta, \gamma], t$}
\KwOut{$\nabla_{\weight} H$}

$\nabla_{\weight} \ell_{\text{curr}} \gets \nabla_{\weight} \ell(\fhat(\weight, \psi), \mathcal{B}_{\text{curr}})$\;
$\nabla_{\weight} V \gets \nabla_{\weight} \ell(\fhat(\weight, \psi), \mathcal{B}_{\text{exp}})$\;

$\sigma_x^2 \gets 10^{-4}$, $\sigma_{\weight}^2 \gets 10^{-8}$\;

\tcp{Input perturbations}
$V_0 \gets \ell(\fhat(\weight, \psi), \mathcal{B}_{\text{exp}})$\;
\For{$k = 1$ \KwTo $5$}{
    $\mathcal{B}_{\text{exp}}^{(k)} \gets \mathcal{B}_{\text{exp}} + \epsilon_x^{(k)}$ where $\epsilon_x^{(k)} \sim \mathcal{N}(0, \sigma_x^2 I)$\;
    $V_k \gets \ell(\fhat(\weight, \psi), \mathcal{B}_{\text{exp}}^{(k)})$\;
}
$\nabla_x V \gets \frac{1}{5} \sum_{k=1}^{5} (V_k - V_0) / \sigma_x$\;

\tcp{Parameter perturbations}
\For{$k = 1$ \KwTo $5$}{
    $\epsilon_{\weight}^{(k)} \sim \mathcal{N}(0, \sigma_{\weight}^2 I)$, $\weight^{(k)} \gets \weight + \epsilon_{\weight}^{(k)}$\;
    $V_k^{\weight} \gets \ell(\fhat(\weight^{(k)}, \psi), \mathcal{B}_{\text{exp}})$\;
}
$\nabla_{\weight} V_{\text{perturb}} \gets \frac{1}{5} \sum_{k=1}^{5} (V_k^{\weight} - V_0) / \sigma_{\weight}$\;

$\nabla_{\weight} \delta V \gets \nabla_x V + \nabla_{\weight} V_{\text{perturb}}$\;
$\text{dV\_norm} \gets \|\nabla_{\weight} \delta V\|_2 / (t + 1)$\;

$\nabla_{\weight} H \gets \alpha \cdot \nabla_{\weight} \ell_{\text{curr}} + \beta \cdot \nabla_{\weight} V + \gamma \cdot \text{dV\_norm}$\;

\textbf{Return} $\nabla_{\weight} H$\;
\end{algorithm}
}

\subsubsection{Task Warmup}

{\small
\begin{algorithm}[H]
\caption{Task Transition with Warmup}\label{alg:warmup}
\KwIn{$\weight(t), \psi(t), \tx(t)$}
\KwOut{$\weight(t)$}

$\eta_{\text{warmup}} \gets 10^{-5}$, $[\alpha, \beta, \gamma] \gets [1.0, 0.0, 0.0]$\;

\For{$e = 1$ \KwTo $N_{\text{warmup}} = 5$}{
    $\mathcal{B} \gets$ \texttt{sample}$(\tx(t))$\;
    $\nabla_{\weight} \ell \gets \nabla_{\weight} \ell(\fhat(\weight(t), \psi(t)), \mathcal{B})$\;
    $\weight(t) \gets$ \texttt{optimizer\_step}$(\weight(t), \nabla_{\weight} \ell, \eta_{\text{warmup}}, \mathcal{O})$\;
}

\textbf{Return} $\weight(t)$\;
\end{algorithm}
}

\subsubsection{Adaptive Hyperparameters}

{\small
\begin{algorithm}[H]
\caption{Adaptive LR and Gradient Weights}\label{alg:adaptive}
\KwIn{$J_{\text{curr}}, J_{\text{prev}}$}
\KwOut{$[\alpha, \beta, \gamma], \eta_{\min}$}

$r_{\text{loss}} \gets J_{\text{curr}} / J_{\text{prev}}$\;
$\eta_{\min} \gets \min(10^{-6}, 10^{-5} / r_{\text{loss}})$\;

$w_{\text{curr}} \gets \min(1.0, r_{\text{loss}})$, $w_{\text{exp}} \gets 1.0 - w_{\text{curr}}$\;
$\alpha \gets w_{\text{curr}} \cdot 0.01$, $\beta \gets w_{\text{exp}} \cdot 0.98$, $\gamma \gets 0.1$\;

\textbf{Return} $[\alpha, \beta, \gamma], \eta_{\min}$\;
\end{algorithm}
}

\subsubsection{Balanced Replay}

{\small
\begin{algorithm}[H]
\caption{Balanced Experience Replay}\label{alg:replay}
\KwIn{$\mathcal{E}, t, B$}
\KwOut{$\mathcal{B}_{\text{exp}}$}

$n_{\text{rec}} \gets \lfloor 0.1 B \rfloor$, $n_{\text{old}} \gets \lfloor 0.8 B \rfloor$, $n_{\text{rand}} \gets B - n_{\text{rec}} - n_{\text{old}}$\;

\uIf{$t > 0$}{
    $\mathcal{S}_{\text{rec}} \gets$ \texttt{sample}$(\mathcal{E}_{t-1}, n_{\text{rec}})$\;
}
\Else{
    $\mathcal{S}_{\text{rec}} \gets \emptyset$\;
}

\uIf{$t > 1$}{
    $n_{\text{per}} \gets \lceil n_{\text{old}} / (t-1) \rceil$\;
    $\mathcal{S}_{\text{old}} \gets \bigcup_{i=0}^{t-2}$ \texttt{sample}$(\mathcal{E}_i, n_{\text{per}})$\;
}
\Else{
    $\mathcal{S}_{\text{old}} \gets \emptyset$\;
}

$\mathcal{S}_{\text{rand}} \gets$ \texttt{sample}$(\mathcal{E}, n_{\text{rand}})$\;
$\mathcal{B}_{\text{exp}} \gets \mathcal{S}_{\text{rec}} \cup \mathcal{S}_{\text{old}} \cup \mathcal{S}_{\text{rand}}$\;

\textbf{Return} $\mathcal{B}_{\text{exp}}$\;
\end{algorithm}
}

\subsubsection{AWB Decision}

{\small
\begin{algorithm}[H]
\caption{Architecture Change Decision}\label{alg:awb_decision}
\KwIn{$J_{\text{prev}}, J_{\text{new}}$}
\KwOut{Boolean decision}

$r_{\text{loss}} \gets J_{\text{new}} / J_{\text{prev}}$, $\Delta J \gets J_{\text{new}} - J_{\text{prev}}$\;
$\theta_{\text{high}} \gets 0.9$\;

\uIf{$(r_{\text{loss}} > \theta_{\text{high}})$ \textbf{and} $(\Delta J > 0)$}{
    \textbf{Return} True\;
}
\Else{
    \textbf{Return} False\;
}
\end{algorithm}
}

\subsubsection{Architecture Search}

{\small
\begin{algorithm}[H]
\caption{NDDS Architecture Search}\label{alg:arch_search}
\KwIn{$\psi(t), \tx(t), \weight(t)$}
\KwOut{$\psi^*(t)$}

$\{d_1, \ldots, d_L\} \gets$ \texttt{extract\_dims}$(\psi(t))$\;
$\mathcal{S} \gets \{d_i \pm k \cdot s : i \in \{1, \ldots, L\}, k \in \{1, 2, 3\}\}$ where $s = 16$\;

$\psi^* \gets \psi(t)$, $J_{\text{best}} \gets \infty$\;

\ForEach{$\psi' \in \mathcal{S}$}{
    $\weight' \gets$ \texttt{init\_weights}$(\psi')$\;
    $J' \gets$ \texttt{quick\_eval}$(\weight', \psi', \tx_{\text{val}}(t))$\;

    \If{$J' < J_{\text{best}}$}{
        $J_{\text{best}} \gets J'$, $\psi^* \gets \psi'$\;
    }
}

\textbf{Return} $\psi^*$\;
\end{algorithm}
}

\subsection{Hyperparameter Reference}

Table~\ref{tab:hyperparameters} summarizes all default hyperparameters used in the standard CL implementation.

{\small
\begin{table}[h]
\centering
\caption{Default Hyperparameters}
\label{tab:hyperparameters}
\begin{tabular}{lll}
\hline
\textbf{Parameter} & \textbf{Default} & \textbf{Description} \\
\hline
\multicolumn{3}{l}{\textit{Gradient Computation}} \\
$\alpha$ & 0.01 & Current task gradient weight \\
$\beta$ & 0.98 & Experience replay gradient weight \\
$\gamma$ & 0.1 & Regularization gradient weight \\
$\sigma_x^2$ & $10^{-4}$ & Input perturbation variance \\
$\sigma_{\weight}^2$ & $10^{-8}$ & Parameter perturbation variance \\
\hline
\multicolumn{3}{l}{\textit{Optimization}} \\
$\eta_0$ & $10^{-4}$ & Base learning rate \\
$\eta_{\min}$ & $10^{-6}$ & Minimum learning rate \\
Optimizer & Adam & Default optimizer \\
Gradient clip & 1.0 & Maximum gradient norm \\
\hline
\multicolumn{3}{l}{\textit{Task Warmup}} \\
$N_{\text{warmup}}$ & 5 & Warmup epochs \\
LR factor & 0.1 & Warmup LR multiplier \\
\hline
\multicolumn{3}{l}{\textit{Experience Replay}} \\
Buffer size & 200,000 & Maximum samples \\
Recent quota & 10\% & From task $t-1$ \\
Older quota & 80\% & From tasks $0$ to $t-2$ \\
Random quota & 10\% & Uniform random \\
\hline
\multicolumn{3}{l}{\textit{AWB Pipeline}} \\
$N_{\text{prelim}}$ & 100 & Preliminary training epochs \\
$N_{\text{AB}}$ & 50 & A/B training epochs \\
$\theta_{\text{high}}$ & 0.9 & Architecture change threshold \\
$\epsilon_{\text{AB}}$ & $0.01(1 + 0.1t)^{-1}$ & A/B convergence threshold \\
Search step & 16 & Dimension search step \\
\hline
\end{tabular}
\end{table}
}

\subsection{Implementation Notes}

\subsubsection{Gradient Normalization}

The $\nabla_{\weight} \delta V$ component is normalized by task count:
\begin{equation}
    \text{dV\_norm} = \frac{\|\nabla_{\weight} \delta V\|_2}{t + 1}
\end{equation}
This prevents the regularization term from dominating as more tasks are learned.

\subsubsection{Learning Rate Schedules}

The framework supports multiple LR schedules: constant, step, exponential, cosine, and linear. Default is cosine annealing with warm restarts.

\subsubsection{JAX Implementation}

All gradient computations are JIT-compiled using JAX with 8 variants for different problem types (regression/classification, standard/AWB, vector/graph).

\subsubsection{Model Partitioning}

Equinox models are partitioned for selective training: (1) standard training (all weights), (2) A/B training (freeze W), (3) V training (freeze A, B).
