\section{Experimental Results}
In this section we evaluate the algorithm in \ref{alg:three} and seek to answer the following questions: Does training with weights for a large number of tasks lead to a saturation as discussed in Section~\ref{sec:motivation}? Does changing the architecture actually helps with learning over consecutive tasks~(Section~\ref{sec:upper_weight})? Does changing the architecture help improve saturation over a series of tasks? And do these ideas carry forward to different types of neural network architectures? 


\subsection{Datasets and Metrics:} 
We examine these questions for three continual learning problems: regression, image classification, and graph classification. 

\textbf{Datasets:} For the regression problem, we consider the randomly generated sine dataset. For the image classification problem, we consider the Omniglot dataset, which consists of handwritten characters. For the graph classification problem, we consider a set of randomly generated graphs utilizing the FakeDataset class from the PyTorch Geometric library. We introduce additional details about these datasets in their respective sections.

\textbf{Metrics:} For all the experiments, we introduce a new task every $500$ epochs. For each such update, we record training and test loss values. For the regression problem we use the mean squared error loss function, and for the classification problems we use the cross-entropy loss function.

\subsection{Implementation Details}
 
\textbf{Experimental Infrastructure:}  All experiments were conducted locally on a 14-inch MacBook Pro (2024) equipped with an Apple M4 Pro system-on-chip, featuring a 14-core CPU, 20-core GPU, 16-core Neural Engine, 24 GB of unified memory, and 512 GB of SSD storage. The system ran macOS (Sequoia 15.6.1) and utilized Metal Performance Shaders (MPS) for hardware acceleration on Apple Silicon. The experimental codebase was implemented in Python using multiple machine learning frameworks, including JAX~\cite{jax} and the Equinox library~\cite{kidger2021equinox}. All training and evaluation scripts were executed directly through the macOS terminal. Experiment outputs, including learning curves, losses, and metrics, were logged and visualized by using TensorBoard. Environment isolation and package management were handled via conda/miniforge, and all runs were performed with fixed random seeds to ensure reproducibility. The code is publicly available on git at \url{https://github.com/krm9c/ContLearn.git}.

\subsubsection{Step 1: Baseline Continual Learning Approach}  While our approach is generic and can be used in conjunction with any available continual learning algorithm, we focus on baseline CL approaches and demonstrate viability with respect to different neural network architectures in this work. For our baseline comparison we use a replay-driven continual learning approach that seeks to balance forgetting and generalization demonstrated in \cite{raghavan2021formalizing} for image classification and regression experiments. We use a replay-driven  continual graph learning approach outlined in \cite{raghavan2023learningcontinuallysequencegraphs} for all comparisons. We use the replay buffer size of $1,000$ for each of these comparisons. 

\subsubsection{Step 2: Architecture Search:} To optimize the network architectures, we employed the direct-directional search method, a derivative-free approach that iteratively explores candidate architectures using only performance evaluations. The search focused on the number of neurons in each hidden layer, while keeping the number of layers and all other architectural components fixed. 

\textbf{Feedforward Neural Network:} For feedforward networks, the search begins with the current architecture and generates candidate architectures by incrementally adding neurons to each hidden layer. For example, given an initial architecture $[10, 40, 40, 10]$ and a step size of $5,$ the first candidate architectures evaluated are $[10, 45, 40, 10]$, $[10, 40, 45, 10]$, and $[10, 45, 45, 10]$. Each candidate is trained for a fixed number of epochs (e.g., 100), and the architecture that achieves the lowest training loss is selected as the new architecture for the next round. This process is repeated for a specified number of rounds. While effective at optimizing layer widths, this approach is computationally intensive, since each round requires full training of multiple candidate architectures.

\textbf{Convolutional/Graph Convolutional  Neural Network:}  For CNNs and GCNs, the search was extended to jointly optimize the number of neurons in the feedforward layers and the filter size of the convolutional or graph convolutional layers. The procedure follows a nested loop structure: for each candidate filter size (e.g., 2, 3, or 4), multiple rounds of neuron-width optimization are performed using the same incremental step-size procedure as discussed earlier. Starting from the current architecture, candidate architectures are generated by adding neurons to each hidden layer in the feedforward network blocks in the CNN and GCN. As in the feedforward neural network, each candidate is trained for a fixed number of epochs to evaluate performance. The architecture achieving the lowest training loss is selected for the next round, and the process is repeated for all considered filter sizes. The final architecture is the combination of filter size and layer widths that minimizes the loss. This double-loop procedure is also computationally expensive, because of the need to train multiple candidates for each filter size in every round.

\subsubsection{ Step 3: Knowledge transfer}
A key component of Algorithm~\ref{alg:three} is the need to transfer learning from the $\weight(t) \rightarrow \weight(t+1)$ through updates of $A$ and $B$ matrices. Since the size and shape of $\weight, A, B$ change with change in the architecture, the implementation differs across different architectures. We realize our algorithm for three different architectures---a feedforward network, graph neural network, and convolutional neural network---by implementing additional methods in the neural network class and manipulating the weights of the model on the fly. 

To adapt the network to the case of evolving weights, we add an additional method into the model that calculates the output of each layer when the weights of that layer are set to $A \weight B^T.$ instead of just $\weight.$ At the onset of each new architecture change, we use this method to provide the forward pass of the neural network. Toward this end, we set $\weight$ to be nontrainable and set $A$ and $B$ to be trainable. Once the new architecture is generated, we update $A$ and $B$ through an Adam optimizer~(initialized from scratch for the inner loop: step 3 in Algorithm~\ref{alg:three}) for a predetermined number of iterations~(steps 4 and 5 in Algorithm~\ref{alg:three}) and then set $\weight = A \weight B^T$ (step 6 in Algorithm~\ref{alg:three}) to adapt the model to the new task~(steps 4 and 5 in Algorithm~\ref{alg:three}).

For the convolutional network, there are two sets of  $A$ and $B$ matrices. The first group pertain to the convolutional layers, where we change the filter-size and $\weight$ is the filter. On the other hand, for the feedforward layers,  $A$ and $B$ correspond to standard weight matrices. In the graph classification problem, these matrices exist for each graph convolutional layer as well as the feedforward layers, resulting in multiple distinct auxiliary matrices. In the case of graph convolution layers, $A$ and $B$ matrices pertain to the graph convolution filters and, for the feedforward layer, the standard weight matrices.

% -----------------------------
\subsection{Regression}~\label{sec:reg}

\textbf{Dataset and Task Splits:}
The sine dataset is a standard benchmark for continual learning in regression settings. Each task is defined by a sine function with a distinct amplitude and/or phase, where inputs $x,$ are sampled from a fixed domain $x \sim \mathcal{U}([-90, 90]).$ This controlled variation across tasks provides a simple yet nontrivial setting to evaluate a modelâ€™s ability to learn sequentially and resist catastrophic forgetting. Task division is achieved by systematically varying the amplitude and/or phase for each task, creating functionally distinct mappings that require continual adaptation.

\textbf{Hyperparameter Choices:}  For the regression problem, we utilize a standard feedforward neural network with four layers, where we begin with $75$ neurons in each of the hidden layers at the first task. We use AdamW~\cite{loshchilov2017fixing} as the optimizer with a learning rate of $1\times 10^{-4}$ in the training regime and use Glorot uniform~\cite{GlorotB10} to initialize the weights. We conduct three sets of experiments.


\begin{figure}
        \centering
        \includegraphics[width=1\linewidth]{paperFigures/regtask2.png}
        \caption{Experiment 1: Comparing loss values on training data for baseline continual learning method and method of learning optimal task architecture}\label{reg5taskstrain}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{paperFigures/origregtest.png}
    \caption{Experiment 1: Comparing loss values on test data for baseline continual learning method and method of learning optimal task architecture}\label{reg5taskstest}
\end{figure}
\textbf{Experiment 1:} For the first experiment, we learn five tasks and train $500$ epochs on each task. We  record in Figure \ref{reg5taskstrain} the loss function progression with respect to training epochs. In this experiment we first examine the behavior of a standard CL regime where no change in the architecture is performed. This serves as our baseline and is depicted by the red graph. Next, the blue graph describes the loss value if we change the architecture at each task with random re-initialization. The green graph shows the loss value when our method of changing the architecture is used at each step low-rank transfer is utilized to retain learning.

We note from the red graph that, with the introduction of new tasks, the model's loss function does not reduce, indicating a stagnation of learning. This behavior suggests that, with changing tasks, the change in the weight parameters has no effect on the training and the learning stagnates. Notably, we argued in our theory, particularly in Lemma~\ref{lem:upper_weight}, that the change in the tasks can lead to a loss of continuity of the forgetting cost, leading to the conclusion that just changing the weights is not enough. This behavior is observed through the red graph.

To obviate this behavior, we seek to change the architecture on the fly. The progression of the loss function with changing architecture on the fly is shown in the blue curve. However, this process introduces a small spike at the onset of each new task. Notably, with the blue curve the weights are initialized at random after the new architecture is chosen, and therefore the continuity of the loss function across two subsequent task is not upheld by the model, indicated by the spikes in the loss progression. We note that even with the new architecture, the model does improve over the red curve. There are two reasons. First, in many cases the initialization point of the model for the new task biases the learning for the new task, and new information therefore is not learned effectively. Second, we simply needed to learn the new task for a larger number of epochs.  The blue graph suggests that changing architecture without transfer of learning from previous tasks to the next task leads to loss of learning. 

The green graph displays a decrease in the loss value on the training data in comparison with the baseline continual learning and the case when the architecture is changed without transfer. We can deduce that changing the architecture and transferring learning across tasks leads to increased performance. We see that at the onset of each new task there is still a small spike in the loss function. This spike is expected because there is a change in the size of the parameter space. This implies  that with the presence of transfer between the parameters spaces, the loss function drops drastically. We note that the transfer between parameter spaces not only counters the effect of changing tasks, as proved in Theorem~\ref{thm:lower_HJB}, but also intuitively provides a better starting point for learning the new tasks. The difference is that learning behavior between the red curve and the green curve implies that the conclusions from Lemmas~\ref{lem:upper_arch} and \ref{lem:upper_weight} are indeed valid where change in the architecture  lowers in the upper bound indicated by the lower loss values. All conclusions from Figure~\ref{reg5taskstrain} extend to Figure \ref{reg5taskstest}, where the test loss values are graphed.

% Added by Claude: Publication-quality 3-panel figure for 2-task sine regression
\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.95\linewidth]{paperFigures/sine_2task_results.pdf}
    \caption{Comparison of four experimental conditions on the 2-task sine regression benchmark. (a) Test MSE on experience replay data showing C4 (AWB Full) achieving the lowest error throughout training. (b) Hamiltonian loss evolution demonstrating improved optimization dynamics with AWB transfer. (c) Average MSE comparison: C4 achieves 33\% lower MSE than baseline (0.019 vs 0.028).}
    \label{fig:sine_2task_comprehensive}
\end{figure}

\textbf{Experiment 2:} For the second experiment, we seek to determine whether  the performance from the previous experiment extends to a larger number of task. In this process we learn $10$ tasks of data and train $500$ epochs on each task and plot the loss values in Figure \ref{reg10taskstrain}. We compare the performance of the standard continual learning method used in \cite{raghavan2021formalizing} with no change made to the architecture across tasks and with our approach (represented by the blue curve) where the architecture is modified and the learning is transferred between the two tasks. We observe similar conclusions from Figures \ref{reg5taskstrain} and \ref{reg5taskstest} where changing the architecture and transferring learning are   helpful. In particular, the red curve is saturated after the first task and does not provide any improvement while the blue curve keeps on improving. After Task 6, however,  no more reduction in loss is seen. It is expected that after a few iterations of architecture search and transfer of learning, a new architecture with better performance is not found. Therefore, the approach does not provide any additional improvement. This behavior is an artifact of the architecture search method where better architectures cannot be found. We noted that  any architecture search approach can be introduced here, including popular  methods such as DeepHyper~\cite{Balaprakash2018DeepHyper}. However, the theoretical conclusions are still valid with this approach, and the conclusions extend to the test dataset as in Figure \ref{reg10taskstest}.

    
    \begin{figure}
        \centering
        \includegraphics[width=.95\linewidth]{paperFigures/reg10tasks.png}
        \caption{Regression Experiment 2: Comparing loss values on training data for baseline continual learning method and method of learning optimal task architecture}\label{reg10taskstrain}
    \end{figure}
    \begin{figure}
        \centering
        \includegraphics[width=.95\linewidth]{paperFigures/reg10Test.png}
        \caption{Regression Experiment 2: Comparing loss values on test data for baseline continual learning method and method of learning optimal task architecture}\label{reg10taskstest}
    \end{figure}

% Added by Claude: Multi-seed quantitative comparison table for sine regression
\begin{table}[!htb]
\centering
\caption{Continual learning metrics for sine regression (10 tasks, 3 seeds). Values reported as mean $\pm$ std. Best values are highlighted in \textbf{bold}. Lower is better for Avg MSE and Forgetting; higher (more positive) is better for BWT and FWT.}
\label{tab:sine_cl_metrics}
\begin{tabular}{lcccc}
\toprule
\textbf{Condition} & \textbf{Avg MSE ($\downarrow$)} & \textbf{BWT ($\uparrow$)} & \textbf{FWT ($\uparrow$)} & \textbf{Forgetting ($\downarrow$)} \\
\midrule
C1: Baseline         & $0.0256 \pm 0.0000$ & $0.0014 \pm 0.0000$ & $0.0000 \pm 0.0000$ & $0.0227 \pm 0.0000$ \\
C2: Heuristics       & $0.0255 \pm 0.0001$ & $0.0012 \pm 0.0000$ & $0.0000 \pm 0.0000$ & $0.0226 \pm 0.0000$ \\
C3: Arch Search      & $0.0251 \pm 0.0001$ & $0.0014 \pm 0.0001$ & $0.0000 \pm 0.0000$ & $0.0223 \pm 0.0000$ \\
C4: AWB Full         & $\mathbf{0.0156 \pm 0.0027}$ & $\mathbf{0.0005 \pm 0.0018}$ & $\mathbf{0.0000 \pm 0.0000}$ & $\mathbf{0.0144 \pm 0.0026}$ \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:sine_cl_metrics} provides a quantitative comparison across all four experimental conditions using standard continual learning metrics averaged over three random seeds. C4 (AWB Full) achieves a 39\% reduction in average MSE compared with the baseline (0.0156 vs 0.0256) and a 37\% reduction in forgetting (0.0144 vs 0.0227). Notably, the comparison between C3 (architecture search without transfer) and C4 (architecture search with AWB transfer) validates that the low-rank transfer mechanism is critical: architecture adaptation alone provides only marginal improvement, while the combination of architecture search and knowledge transfer yields substantial gains.

% Added by Claude: Publication-quality 6-panel figure for 10-task sine regression with error bands
\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.95\linewidth]{paperFigures/sine_10task_3seed_results.pdf}
    \caption{Comprehensive comparison on the 10-task sine regression benchmark (3 seeds, shaded regions show $\pm$1 std). (a) Test MSE on experience replay showing C4 maintaining consistently lower error across all tasks. (b) Hamiltonian loss with C4 showing improved convergence. (c) Gradient norm evolution demonstrating training stability. (d) Average MSE: C4 achieves 39\% reduction (0.016 vs 0.026). (e) Backward transfer (BWT): positive values indicate improved performance on previous tasks. (f) Forgetting: C4 reduces forgetting by 37\% (0.014 vs 0.023).}
    \label{fig:sine_10task_comprehensive}
\end{figure}

\begin{figure}
        \centering
\includegraphics[width=1\linewidth]{paperFigures/noisydata1.png}
        \caption{Regression Experiment 3: Comparing loss values on training data for baseline continual learning method and method of learning optimal task architecture when noise in the data is increased}\label{reg5noise}
    \end{figure}
\textbf{Experiment 3:} For the third experiment, we learn $5$ tasks of data and train $500$ epochs on each task. To  understand the implications of Lemma~\ref{lem:upper_arch} and \ref{lem:upper_weight}, we introduce noise into each subsequent task and observe the learning behavior. Figure \ref{reg5noise} reveals the loss values as the neural network training (on the training data). The red graph is again the standard continual learning method used in \cite{raghavan2021formalizing}, where no change is made to the architecture across tasks. Supporting the conclusions from Theorem~\ref{thm:contMeasure} and Lemmas \ref{lem:upper_arch} and \ref{lem:upper_weight}, we observe that with increasing noise there is an increase in the difference in the performance between two subsequent tasks. In other words, in the presence of changing tasks, the capacity diverges as in Theorem~\ref{thm:task_nonstationary_weights}, and changing weights is not enough to maintain performance as in Lemma~\ref{lem:upper_weight}. The blue graph shows that even in the presence of noise, there is improvement in the performance of the model 
%when the architecture is changed loss value 
when we change the architecture at each step and we complete the low-rank transfer to retain learning. Since the blue graph displays a decrease in the loss value on the training data in comparison with the baseline continual learning, we conclude that changing the architecture while also transferring learning at each task increases performance even when subsequent tasks  introduce large jumps in the loss function, thus violating Theorem~\ref{thm:contMeasure}.

\subsection{Classification}


\textbf{Dataset and Task Splits:} The Omniglot dataset~\cite{Lake_2015} is a cornerstone benchmark for few-shot and meta-learning, designed to test how well models can learn new concepts from extremely limited data. Comprising $1,623$ handwritten characters from $50$ diverse alphabets, each rendered $20$ times by different individuals, it captures rich variation in writing style and structure. Its $105 \times 105$ grayscale images provide clean, simple inputs while still offering enough complexity to challenge learning algorithms. We split the Omniglot dataset into multiple tasks by randomly selecting three classes and applying rotational transformations on them to generate a task.

\textbf{Hyperparameter Choices:} For the classification experiment, we use a convolutional neural network with one feedforward layer. The convolutional layers are followed by max-pooling and a $ReLU$ activation function, which extract spatial features from the input. Flattened convolutional outputs are passed through a small feedforward network with fully connected layers. We initialize the architecture with a convolutional filter size of $3$ and hidden layers sizes of $512$ and $64.$  We learn $5$ tasks of data based on the Omniglot dataset and train $500$ epochs on each task. We use AdamW~\cite{loshchilov2017fixing}  with a learning rate of $1\times 10^{-4}$ in the training regime. 


\textbf{Results:} 
 Figure \ref{cnn} shows the loss function values as the CNN trains~(on the training data) with respect to different methods as discussed in Section~\ref{sec:reg} on the five tasks. The red graph is the standard continual learning method used in \cite{raghavan2021formalizing}, where no change is made to the architecture across tasks; this  serves as our baseline for comparison. The blue graph shows the progression of loss values when our proposed method is used. 

For the regression experiment, we learned the number of neurons per layer through our architecture search. With this experiment, however, we determine the optimal filter size for the convolutional layers and the optimal number of neurons in the feedforward neural network. Thus, the low-rank transfer is performed for the filters, and the weights matrices are determined by the feedforward neural network. In particular, this highlights the flexibility of our method, where we can perform transfer of information between convolutional filters as well as standard weight matrices. 

With the first 500 epochs of training, we observe from Figure \ref{cnn} that both the blue curve and the red curve coincide. After the first task is introduced, however, the red curve has a large jump. This jump is common in classification problem because, unlike regression, classification problems do not have task-to-task dependency:  each task is sampled independently, by virtue of iid sampling. This leads to a lot of jumps in the loss function and its gradients and incurs large forgetting. We note, however, that our approach counters this jump significantly with increased performance for each subsequent task. Notably, the jump in some tasks is larger compared with other tasks. We note that some of the jumps pertain to how well $A$ and $B$ matrices transfer information between tasks. 

It was shown in Theorem~\ref{thm:lower_HJB} that the eigenvalues of the Jacobian induced by the change in the data must be countered by the singular values of $A$ and $B$ matrices. However, this is hard to ensure in practice, especially when the tasks are independently sampled as in the Omniglot dataset case. Although it is feasible to devise a method that exactly establishes the conditions outlined in Theorem~\ref{thm:lower_HJB},  such an approach requires careful engineering and has been relegated to future work. Despite this, the conclusions from Section~\ref{sec:reg} carry forward to this case with the key difference that the jumps between tasks are large.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{paperFigures/cnn5tasks1.png}
    \caption{Classification experiment: Comparing loss values on training data for baseline continual learning method and method of learning optimal task architecture}
    \label{cnn}
\end{figure}

% Added by Claude: Publication-quality 3-panel figure for MNIST classification
\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.95\linewidth]{paperFigures/mnist_2task_results.pdf}
    \caption{Comparison on the 2-task MNIST classification benchmark. (a) Test accuracy on experience replay showing C4 (AWB Full) achieving highest accuracy throughout training. (b) Hamiltonian loss evolution with C4 showing faster convergence. (c) Average accuracy: C4 achieves 94.5\% compared with 92.8\% for baseline, demonstrating the effectiveness of AWB transfer for classification tasks.}
    \label{fig:mnist_comprehensive}
\end{figure}

