\section{Proofs}
%%%
\subsection{Proof of Lemma~\ref{lem:upper_weight}}~\label{sec:upper_weight}
\begin{proof}
    By Lemma \ref{lem:taylor} and the assumption that $\mu\left( \bigcup_{\tau = 0}^{t}\tx(\tau)\Delta \bigcup_{\tau = 0}^{t+\Delta t}\tx(\tau)\right)\geq \delta,$ we have
    \begin{align*}
       E\left(\bigcup_{\tau = 0}^{t}\tx(\tau)\right) & = E\left(\bigcup_{\tau = 0}^{t+\Delta t}\tx(\tau)\right) - \Delta t\Big[\mu\left( \bigcup_{\tau = 0}^{t}\tx(\tau)\Delta \bigcup_{\tau = 0}^{t+\Delta t}\tx(\tau)\right)\cdot \displaystyle\int_{\bigcup_{\tau = 0}^{t}\tx(\tau)} \ell(\hat{f}(\weight,\psi))d\mu\\
        & +\displaystyle\int_{\bigcup_{\tau = 0}^{t}\tx(\tau)} \ell'(\hat{f}(\weight,\psi))\cdot \partial_{\weight}^1 \hat{f}(\weight,\psi)\cdot \Delta w \hspace{1mm}  d\mu\Big]- o(\Delta t)\\
        & = E\left(\bigcup_{\tau = 0}^{t+\Delta t}\tx(\tau)\right) + \Delta t\left(-\mu\left( \bigcup_{\tau = 0}^{t}\tx(\tau)\Delta \bigcup_{\tau = 0}^{t+\Delta t}\tx(\tau)\right)\right)\cdot \displaystyle\int_{\bigcup_{\tau = 0}^{t}\tx(\tau)\Delta \bigcup_{\tau = 0}^{t+\Delta t}\tx(\tau)} \ell(\hat{f}(\weight,\psi))d\mu\\
        & - \Delta t\displaystyle\int_{\bigcup_{\tau = 0}^{t}\tx(\tau)} \ell'(\hat{f}(\weight,\psi))\cdot \partial_{\weight}^1 \hat{f}(\weight,\psi)\cdot \Delta w \hspace{1mm}  d\mu- o(\Delta t)\\
        & \leq E\left(\bigcup_{\tau = 0}^{t+\Delta t}\tx(\tau)\right) + \Delta t \cdot \delta \cdot \displaystyle\int_{\bigcup_{\tau = 0}^{t}\tx(\tau)\Delta \bigcup_{\tau = 0}^{t+\Delta t}\tx(\tau)} \ell(\hat{f}(\weight,\psi))d\mu\\
        & - \Delta t\displaystyle\int_{\bigcup_{\tau = 0}^{t}\tx(\tau)} \ell'(\hat{f}(\weight,\psi))\cdot \partial_{\weight}^1 \hat{f}(\weight,\psi)\cdot \Delta w \hspace{1mm}  d\mu- o(\Delta t)\\
    \end{align*}
    Subtracting $E\left(\bigcup_{\tau = 0}^{t+\Delta t}\tx(\tau)\right)$ from both sides and combining like terms,
    \begin{align*}
         E\left(\bigcup_{\tau = 0}^{t}\tx(\tau)\right) - E\left(\bigcup_{\tau = 0}^{t+\Delta t}\tx(\tau)\right)
         & \leq \Delta t \cdot \delta \cdot \displaystyle\int_{\bigcup_{\tau = 0}^{t}\tx(\tau)\Delta \bigcup_{\tau = 0}^{t+\Delta t}\tx(\tau)} \ell(\hat{f}(\weight,\psi))d\mu\\
        & - \Delta t\displaystyle\int_{\bigcup_{\tau = 0}^{t}\tx(\tau)} \ell'(\hat{f}(\weight,\psi))\cdot \partial_{\weight}^1 \hat{f}(\weight,\psi)\cdot \Delta w \hspace{1mm}  d\mu- o(\Delta t).
    \end{align*}
    Additionally, we can assume that the loss function $\ell$ is bounded above by a constant $M_0,$ so
    \begin{align*}
        E\left(\bigcup_{\tau = 0}^{t}\tx(\tau)\right) - E\left(\bigcup_{\tau = 0}^{t+\Delta t}\tx(\tau)\right)
        & \leq \Delta t \cdot \delta \cdot M_0- \Delta t\displaystyle\int_{\bigcup_{\tau = 0}^{t}\tx(\tau)} \ell'(\hat{f}(\weight,\psi))\cdot \partial_{\weight}^1 \hat{f}(\weight,\psi)\cdot \Delta w \hspace{1mm}  d\mu- o(\Delta t).
    \end{align*}
    To consider this difference in expected values across future tasks, set $\Delta t =1 $ and let us sum across such future tasks, 
    \begin{align*}
         \sum_{\tau = t}^{T} \left(E\left(\bigcup_{\nu = 0}^{\tau}\tx(\nu)\right) - E\left(\bigcup_{\nu = 0}^{\tau+1}\tx(\nu)\right)\right)
        & \leq \sum_{\tau =t}^{T} \Big( M_0\cdot \delta - \displaystyle\int_{\bigcup_{\nu = 0}^{\tau}\tx(\nu)} \ell'(\hat{f}(\weight,\psi))\cdot \partial_{\weight}^1 \hat{f}(\weight,\psi)\cdot \Delta w \hspace{1mm}  d\mu \Big).
    \end{align*}
    Now, we can choose the following for each $\tau$
    \begin{align*}
        \Delta \weight & =  \ell'(\hat{f}(\weight,\psi))\cdot \partial_{\weight}^1 \hat{f}(\weight,\psi).
    \end{align*}
    Thus,
    \begin{align*}
         \sum_{\tau = t}^{T} \left(E\left(\bigcup_{\nu = 0}^{\tau}\tx(\nu)\right) - E\left(\bigcup_{\nu = 0}^{\tau+1}\tx(\nu)\right)\right)
        & \leq \sum_{\tau =t}^{T} \Big( M_0\cdot \delta - \displaystyle\int_{\bigcup_{\nu = 0}^\tau\tx(\nu)} \left(\ell'(\hat{f}(\weight,\psi))\cdot \partial_{\weight}^1 \hat{f}(\weight,\psi)\right)^2 \hspace{1mm}  d\mu \Big).
    \end{align*}
    Now, notice the following
    \begin{align*}
        J(\weight(\tau),\psi(\tau),\tx(\tau)) & = \int_{\bigcup_{\nu = 0}^\tau \tx(\nu)} \ell(\hat{f}(\weight,\psi))d\mu = E\left(\bigcup_{\nu = 0}^{\tau}\tx(\nu)\right).
    \end{align*}
    Thus,
    \begin{align*}
         \sum_{\tau = t}^{T} \left(J(\tx(\tau)) - J(\tx(\tau+1))\right)
         & = \sum_{\tau = t}^{T} \left(E\left(\bigcup_{\nu = 0}^{\tau}\tx(\nu)\right) - E\left(\bigcup_{\nu = 0}^{\tau+1}\tx(\nu)\right)\right)\\
        & \leq \sum_{\tau =t}^{T} \Big( M_0\cdot \delta - \displaystyle\int_{\bigcup_{\nu = 0}^\tau\tx(\nu)} \left(\ell'(\hat{f}(\weight,\psi))\cdot \partial_{\weight}^1 \hat{f}(\weight,\psi)\right)^2 \hspace{1mm}  d\mu \Big),
    \end{align*}
    as desired.
\end{proof}



\subsection{Proof of Lemma~\ref{lem:upper_arch}}~\label{sec:upper_arch}
\begin{proof}
    By Lemma \ref{lem:taylor} and the assumption that $\mu\left( \bigcup_{\tau = 0}^{t}\tx(\tau)\Delta \bigcup_{\tau = 0}^{t+\Delta t}\tx(\tau)\right)\geq \delta,$ we have
    \begin{align*}
       E\left(\bigcup_{\tau = 0}^{t}\tx(\tau)\right) & = E\left(\bigcup_{\tau = 0}^{t+\Delta t}\tx(\tau)\right) - \Delta t\Big[\mu\left( \bigcup_{\tau = 0}^{t}\tx(\tau)\Delta \bigcup_{\tau = 0}^{t+\Delta t}\tx(\tau)\right)\cdot \displaystyle\int_{\bigcup_{\tau = 0}^{t}\tx(\tau)} \ell(\hat{f}(\weight,\psi))d\mu\\
        & +\displaystyle\int_{\bigcup_{\tau = 0}^{t}\tx(\tau)} \ell'(\hat{f}(\weight,\psi))\cdot \partial_{\weight}^1 \hat{f}(\weight,\psi)\cdot \Delta w \hspace{1mm}  d\mu\\
        & +\displaystyle\int_{\bigcup_{\tau = 0}^{t}\tx(\tau)} \ell'(\hat{f}(\weight,\psi))\cdot \partial_{\psi}^1 \hat{f}(\weight,\psi)\cdot \Delta \psi \hspace{1mm}  d\mu\Big]- o(\Delta t)\\
        & = E\left(\bigcup_{\tau = 0}^{t+\Delta t}\tx(\tau)\right) + \Delta t\left(-\mu\left( \bigcup_{\tau = 0}^{t}\tx(\tau)\Delta \bigcup_{\tau = 0}^{t+\Delta t}\tx(\tau)\right)\right)\cdot \displaystyle\int_{\bigcup_{\tau = 0}^{t}\tx(\tau)\Delta \bigcup_{\tau = 0}^{t+\Delta t}\tx(\tau)} \ell(\hat{f}(\weight,\psi))d\mu\\
        & - \Delta t\displaystyle\int_{\bigcup_{\tau = 0}^{t}\tx(\tau)} \ell'(\hat{f}(\weight,\psi))\cdot \partial_{\weight}^1 \hat{f}(\weight,\psi)\cdot \Delta w \hspace{1mm}  d\mu\\
        &-\Delta t \displaystyle\int_{\bigcup_{\tau = 0}^{t}\tx(\tau)} \ell'(\hat{f}(\weight,\psi))\cdot \partial_{\psi}^1 \hat{f}(\weight,\psi)\cdot \Delta \psi \hspace{1mm}  d\mu- o(\Delta t)\\
        & \leq E\left(\bigcup_{\tau = 0}^{t+\Delta t}\tx(\tau)\right) + \Delta t \cdot \delta \cdot \displaystyle\int_{\bigcup_{\tau = 0}^{t}\tx(\tau)\Delta \bigcup_{\tau = 0}^{t+\Delta t}\tx(\tau)} \ell(\hat{f}(\weight,\psi))d\mu\\
        & - \Delta t\displaystyle\int_{\bigcup_{\tau = 0}^{t}\tx(\tau)} \ell'(\hat{f}(\weight,\psi))\cdot \partial_{\weight}^1 \hat{f}(\weight,\psi)\cdot \Delta w \hspace{1mm}  d\mu\\
        & -\Delta t \displaystyle\int_{\bigcup_{\tau = 0}^{t}\tx(\tau)} \ell'(\hat{f}(\weight,\psi))\cdot \partial_{\psi}^1 \hat{f}(\weight,\psi)\cdot \Delta \psi \hspace{1mm}  d\mu- o(\Delta t).
    \end{align*}
    Subtracting $E\left(\bigcup_{\tau = 0}^{t+\Delta t}\tx(\tau)\right)$ from both sides and combining like terms,
    \begin{align*}
         E\left(\bigcup_{\tau = 0}^{t}\tx(\tau)\right) - E\left(\bigcup_{\tau = 0}^{t+\Delta t}\tx(\tau)\right)
         & \leq \Delta t \cdot \delta \cdot \displaystyle\int_{\bigcup_{\tau = 0}^{t}\tx(\tau)\Delta \bigcup_{\tau = 0}^{t+\Delta t}\tx(\tau)} \ell(\hat{f}(\weight,\psi))d\mu\\
        & - \Delta t\displaystyle\int_{\bigcup_{\tau = 0}^{t}\tx(\tau)} \ell'(\hat{f}(\weight,\psi))\cdot \partial_{\weight}^1 \hat{f}(\weight,\psi)\cdot \Delta w \hspace{1mm}  d\mu\\
        & -\Delta t \displaystyle\int_{\bigcup_{\tau = 0}^{t}\tx(\tau)} \ell'(\hat{f}(\weight,\psi))\cdot \partial_{\psi}^1 \hat{f}(\weight,\psi)\cdot \Delta \psi \hspace{1mm}  d\mu- o(\Delta t).
    \end{align*}
    Additionally, we can assume that the loss function $\ell$ is bounded above by a constant $M_0,$ so
    \begin{align*}
        E\left(\bigcup_{\tau = 0}^{t}\tx(\tau)\right) - E\left(\bigcup_{\tau = 0}^{t+\Delta t}\tx(\tau)\right)
        & \leq \Delta t \cdot \delta \cdot M_0- \Delta t\displaystyle\int_{\bigcup_{\tau = 0}^{t}\tx(\tau)} \ell'(\hat{f}(\weight,\psi))\cdot \partial_{\weight}^1 \hat{f}(\weight,\psi)\cdot \Delta w \hspace{1mm}  d\mu\\
        & -\Delta t \displaystyle\int_{\bigcup_{\tau = 0}^{t}\tx(\tau)} \ell'(\hat{f}(\weight,\psi))\cdot \partial_{\psi}^1 \hat{f}(\weight,\psi)\cdot \Delta \psi \hspace{1mm}  d\mu- o(\Delta t).
    \end{align*}
    To consider this difference in expected values across future tasks, set $\Delta t =1 $ and let us sum across such future tasks, 
    \begin{align*}
         \sum_{\tau = t}^{T} \left(E\left(\bigcup_{\nu = 0}^{\tau}\tx(\nu)\right) - E\left(\bigcup_{\nu = 0}^{\tau+1}\tx(\nu)\right)\right)
        & \leq \sum_{\tau =t}^{T} \Big( M_0\cdot \delta - \displaystyle\int_{\bigcup_{\nu = 0}^{\tau}\tx(\nu)} \ell'(\hat{f}(\weight,\psi))\cdot \partial_{\weight}^1 \hat{f}(\weight,\psi)\cdot \Delta w \hspace{1mm}  d\mu\\
        & -\displaystyle\int_{\bigcup_{\tau = 0}^{t}\tx(\tau)} \ell'(\hat{f}(\weight,\psi))\cdot \partial_{\psi}^1 \hat{f}(\weight,\psi)\cdot \Delta \psi \hspace{1mm}  d\mu\Big).
    \end{align*}
    Now, we can choose the following for each $\nu$
    \begin{align*}
        \Delta \weight & =  \ell'(\hat{f}(\weight,\psi))\cdot \partial_{\weight}^1 \hat{f}(\weight,\psi).
    \end{align*}
    and
    \begin{align*}
        \Delta \psi & =  \ell'(\hat{f}(\weight,\psi))\cdot \partial_{\psi}^1 \hat{f}(\weight,\psi).
    \end{align*}
    Thus,
    \begin{align*}
         \sum_{\tau = t}^{T} \left(E\left(\bigcup_{\nu = 0}^{\tau}\tx(\nu)\right) - E\left(\bigcup_{\nu = 0}^{\tau+1}\tx(\nu)\right)\right)
        & \leq \sum_{\tau =t}^{T} \Big( M_0\cdot \delta - \displaystyle\int_{\bigcup_{\nu = 0}^\tau\tx(\nu)} \left(\ell'(\hat{f}(\weight,\psi))\cdot \partial_{\weight}^1 \hat{f}(\weight,\psi)\right)^2 \hspace{1mm}  d\mu\\
        & -\displaystyle\int_{\bigcup_{\nu = 0}^\tau\tx(\nu)} \left(\ell'(\hat{f}(\weight,\psi))\cdot \partial_{\psi}^1 \hat{f}(\weight,\psi)\right)^2 \hspace{1mm}  d\mu \Big).
    \end{align*}
    Now, notice the following
    \begin{align*}
        J(\weight(\tau),\psi(\tau),\tx(\tau)) & = \int_{\bigcup_{\nu = 0}^\tau \tx(\nu)} \ell(\hat{f}(\weight,\psi))d\mu = E\left(\bigcup_{\nu = 0}^{\tau}\tx(\nu)\right).
    \end{align*}
    Thus,
    \begin{align*}
         \sum_{\tau = t}^{T} \left(J(\tx(\tau)) - J(\tx(\tau+1))\right)
         & = \sum_{\tau = t}^{T} \left(E\left(\bigcup_{\nu = 0}^{\tau}\tx(\nu)\right) - E\left(\bigcup_{\nu = 0}^{\tau+1}\tx(\nu)\right)\right)\\
        & \leq \sum_{\tau =t}^{T} \Big( M_0\cdot \delta - \displaystyle\int_{\bigcup_{\nu = 0}^\tau\tx(\nu)} \left(\ell'(\hat{f}(\weight,\psi))\cdot \partial_{\weight}^1 \hat{f}(\weight,\psi)\right)^2 \hspace{1mm}  d\mu\\
        & -\displaystyle\int_{\bigcup_{\nu = 0}^\tau\tx(\nu)} \left(\ell'(\hat{f}(\weight,\psi))\cdot \partial_{\psi}^1 \hat{f}(\weight,\psi)\right)^2 \hspace{1mm}  d\mu \Big)\\
    \end{align*}
    as desired.
\end{proof}

\subsection{Proof of Proposition~\ref{HJB}}\label{sec:HJB}
\begin{proof}
    Let $J, V,$ and $V^*$ be as defined above. To begin, we split the sum in $V(t)$ over the discrete intervals $[t,t+\Delta t]$ and $[t+\Delta t, T].$ Observe,
    \begin{align}\label{b}
        V^*(t) & = \min_{w\in \mathcal{W}(\psi^*(t))} \int_{t}^T J(w(\tau),\psi^*(t), \tx(\tau))d\tau\nonumber\\
                & = \min_{w\in \mathcal{W}(\psi^*(t))} \biggr[\int_{t}^{t+\Delta t} J(w(\tau),\psi^*(t), \tx(\tau))d\tau+ \int_{t+\Delta t}^{T} J(w(\tau),\psi^*(t), \tx(\tau))d\tau\biggr]\nonumber\\
                & = \min_{w\in \mathcal{W}(\psi^*(t))} \int_{t}^{t+\Delta t} J(w(\tau),\psi^*(t),\tx(\tau))d\tau + V^*(t+\Delta t)\nonumber\\
                & = \min_{w\in \mathcal{W}(\psi^*(t))} J(\weight(t),\psi^*(t),\tx(t))\Delta t + V^*(t+\Delta t).
    \end{align}
    Now, we provide the Taylor series expansion of $V^*(t+\Delta t)$ about $t.$ Notice,
    \begin{align}
        V^*(t+\Delta t) & = V^*(t) + \Delta t \left[\partials_{t}V^*(t)  + \partials_{\tx}V^*(t) d_{t} \tx + \partials_{\weight}V^*(t)  \partials_t \weight \right] + o(\Delta t)\nonumber\\
                & = V^*(t) + \Delta t\biggr[\partials_{t} V^*(t)  + \partials_{\tx}V^*(t) d_{t} \tx + \partials_{\weight}V^*(t) [\mathrm{A}^*(t)\weight(t)( \mathrm{B}^*(t))^T + u(t)]\biggr] + o(\Delta t),\label{a}
    \end{align}
    where $u(t)$ represents the updates made to the each weights matrix of the new dimensions.
    Substituting \ref{a} into \ref{b}, we have
    \begin{align*}
        V^*(t) & = \min_{w\in \mathcal{W}(\psi^*(t))} J(\weight(t),\psi^*(t),\tx(t))\Delta t+ V^*(t) + \Delta t\biggr[\partials_{t} V^*(t)  + \partials_{\tx}V^*(t) d_{t} \tx\\ 
                &+ \partials_{\weight}V^*(t) [\mathrm{A}^*(t)\weight(t)( \mathrm{B}^*(t))^T + u(t)]\biggr] + o(\Delta t).
    \end{align*}
    Canceling $V^*(t)$ gives
    \begin{align*}
        0 & = \min_{w\in \mathcal{W}(\psi^*(t))} J(\weight(t),\psi^*(t),\tx(t))\Delta t+ \Delta t\left[\partials_{t}V^*(t)  + \partials_{\tx}V^*(t) d_{t} \tx \right.  \\ &+
        \left. \partials_{\weight}V^*(t) [\mathrm{A}^*(t)\weight(t)( \mathrm{B}^*(t))^T + u(t)]\right] + o(\Delta t).
    \end{align*}
    Dividing both sides by $\Delta t$ produces
    \begin{align*}
        0 & = \min_{w\in \mathcal{W}(\psi^*(t))} J(\weight(t),\psi^*(t),\tx(t)) +
        \partials_{t}V^*(t)  + \partials_{\tx}V^*(t) V^*(t) d_{t} \tx + \partials_{\weight}V^*(t) [\mathrm{A}^*(t)\weight(t)( \mathrm{B}^*(t))^T + u(t)].
    \end{align*}
    Finally, reordering gives
    \begin{align*}
        -\partials_{t} V^*(t) & = \min_{w\in \mathcal{W}(\psi^*(t))} J(\weight(t),\psi^*(t),\tx(t))+ \partials_{\tx}V^*(t) d_{t} \tx + \partials_{\weight}V^*(t) V^*(t) [\mathrm{A}^*(t)\weight(t)( \mathrm{B}^*(t))^T + u(t)],
    \end{align*}
    as desired.
\end{proof}

\subsection{Proof of Theorem~\ref{thm:lower_HJB}}\label{sec:lower_HJB}
\begin{proof}
Given proposition~\ref{HJB}, assume there exists an stochastic gradient based optimization procedure  and choose $ u(t) =- \sum^{I} \alpha(i) \mathrm{g}^{(i)}$  such that $\min_{\weight \in \mathcal{W}(\psi^*(t))} J(\weight(t),\psi^*(t),\tx(t)) \leq \varepsilon,$ where $I$ is the number of updates. Then, we have 
\begin{align*}
    -\partials_{t}V^*(t) & \leq \varepsilon + \partials_{\tx}V^*(t) d_{t} \tx +  \partials_{\weight}V^*(t)  \left( \mathrm{A}^*(t)\weight(t)( \mathrm{B}^*(t))^T - \sum^{I} \alpha(i) \mathrm{g}^{(i)}  \right)  \\    
\end{align*}
For any $dt$ in terms of tasks $t$  let $\partials_{\tx}V^*(t) d_{t} \tx \leq \norm{\partials_{\tx}V^*(t) } \norm{d_{t} \tx} \leq \sing \delta$ as $\norm{d_{t} \tx}  \geq \mu(\tx(t)\bigtriangleup \tx(t+1)) \geq \delta$ and $\norm{\partials_{\tx}V^*(t) }$ is less than the largest singular value of $\partials_{\tx}V^*(t) .$  Thus we write 
\begin{align*}
    -\partials_{t}V^*(t) & \leq \varepsilon + \sing \delta +  \partials_{\weight}V^*(t)  \left( \mathrm{A}^*(t)\weight(t)( \mathrm{B}^*(t))^T - \sum^{I} \alpha(i) \mathrm{g}^{(i)}  \right) 
\end{align*}
Taking $g_{MIN}$ to be the smallest value of the gradient over all update iterations, we have 
\begin{align*}
    -\partials_{t} & \leq \varepsilon + \sing \delta +  \partials_{\weight}V^*(t)  \left( \mathrm{A}^*(t)\weight(t) (\mathrm{B}^*(t))^T - \mathrm{g}_{\mathrm{MIN} } \sum^{I} \alpha(i)  \right) \\
     & \leq \varepsilon + \sing \delta +  \norm{\partials_{\weight}V^*(t) } \norm{\mathrm{A}^*(t)\weight(t)( \mathrm{B}^*(t))^T} - \partials_{\weight}V^*(t)  \left(  \mathrm{g}_{\mathrm{MIN} } \sum^{I} \alpha(i)  \right) \\
     & \leq \varepsilon + \left[\sing \delta +  \singw \norm{\mathrm{A}^*(t)\weight(t)( \mathrm{B}^*(t))^T} - \singmin \norm{\mathrm{g}_{\mathrm{MIN} }} \norm{\sum^{I} \alpha(i) } \right]
\end{align*}
This finally provides 
\begin{align*}
    \partials_{t} & \leq \mathrm{sup} \varepsilon - \mathrm{inf} \left[\sing \delta +  \singw \norm{\mathrm{A}^*(t)\weight(t)( \mathrm{B}^*(t))^T} - \singmin \norm{\mathrm{g}_{\mathrm{MIN} }} \norm{\sum^{I} \alpha(i) } \right]
\end{align*}
For the total variation to be upper bounded, we need the quantity in the brackets to go to zero, this provides.
\begin{align*}
\left[\sing \delta +  \singw \norm{\mathrm{A}^*(t)\weight(t)( \mathrm{B}^*(t))^T} - \singmin \norm{\mathrm{g}_{\mathrm{MIN} }} \norm{\sum^{I} \alpha(i) } \right] = 0 \\
\end{align*}
\begin{align*}
   \sing \delta &= \left[ \singmin \norm{\mathrm{g}_{\mathrm{MIN} }}  \norm{\sum^{I} \alpha(i) } - \singw \norm{\mathrm{A}^*(t)\weight(t)( \mathrm{B}^*(t))^T} \right]
\end{align*}
providing the result
\end{proof}


% \section{Introduction}

% \subsection{AWB Transformation Principle}

% The Adaptive Weight Basis (AWB) enables architecture morphing during continual learning through the transformation:

% \begin{equation}
% \mathbf{V} = \mathbf{A} \mathbf{W} \mathbf{B}^T
% \end{equation}

% where:
% \begin{itemize}
%     \item $\mathbf{W}$: Original weight matrix from the previous architecture
%     \item $\mathbf{A}$: Output transformation matrix
%     \item $\mathbf{B}$: Input transformation matrix
%     \item $\mathbf{V}$: New weight matrix in the expanded architecture
% \end{itemize}

% \subsection{Key Invariant}

% \textbf{Architecture morphing rule:} Input and output dimensions must be preserved, while hidden layer dimensions can be expanded.

% \subsection{Notation}

% For a layer with weight matrix $\mathbf{W} \in \mathbb{R}^{m \times n}$:
% \begin{itemize}
%     \item $n$ = input dimension (number of input features)
%     \item $m$ = output dimension (number of output features)
%     \item Forward pass: $\mathbf{y} = \mathbf{W}\mathbf{x}$ where $\mathbf{x} \in \mathbb{R}^n$, $\mathbf{y} \in \mathbb{R}^m$
% \end{itemize}

\newpage

\section{Feedforawrd Neural Network $AWB$ Matrix Calculation Example}\label{app: MLP AB}
In this section, complete an explicit example determining the sizes for $A$ and $B$ matrices for each layer of a feedforward neural network to transfer learning from the architecture of $[64,128,128,10]$ to the architecture of $[64,256,256,10].$ 

\subsection{Original Architecture and Weights Matrix Specifications}
We begin with the following architecture:
\begin{equation}
\text{Original architecture} = [64, 128, 128, 10]
\end{equation}

This defines a 3-layer network:
\begin{itemize}
    \item Layer 0: $64 \rightarrow 128$ (input layer to first hidden)
    \item Layer 1: $128 \rightarrow 128$ (first hidden to second hidden)
    \item Layer 2: $128 \rightarrow 10$ (second hidden to output)
\end{itemize}

This 3-layer network provides us with the following weights matrices:
\begin{align}
\mathbf{W}_0 &\in \mathbb{R}^{128 \times 64} \quad \text{(maps $64 \rightarrow 128$)} \\
\mathbf{W}_1 &\in \mathbb{R}^{128 \times 128} \quad \text{(maps $128 \rightarrow 128$)} \\
\mathbf{W}_2 &\in \mathbb{R}^{10 \times 128} \quad \text{(maps $128 \rightarrow 10$)}
\end{align}


\subsection{Optimal Architecture Specifications}
After completing an architecture search, it was determined that the following architecture was optimal:
\begin{equation}
\text{new\_arch} = [64, 256, 256, 10]
\end{equation}

This expands hidden layers while preserving input (64) and output (10):
\begin{itemize}
    \item Layer 0: $64 \rightarrow 256$
    \item Layer 1: $256 \rightarrow 256$
    \item Layer 2: $256 \rightarrow 10$
\end{itemize}

\subsection{Determining Appropriate $A$ and $B$ Matrices}

Our goal is to obtain weights matrices $V$ such that which correspond to the new architecture. This is completed by determing matrices $A_i$ and $B_i$ for each layer $i$ such that $A_iW_iB_i^T = V_i.$  The transformation matrices are constructed as:

\begin{align}
\mathbf{A}_i &\in \mathbb{R}^{\text{new\_out}_i \times \text{old\_out}_i} \\
\mathbf{B}_i &\in \mathbb{R}^{\text{new\_in}_i \times \text{old\_in}_i}
\end{align}

For each layer:

\textbf{Layer 0:}
\begin{align}
\mathbf{A}_0 &\in \mathbb{R}^{256 \times 128} \quad \text{(output: $128 \rightarrow 256$)} \\
\mathbf{B}_0 &\in \mathbb{R}^{64 \times 64} \quad \text{(input: $64 \rightarrow 64$, identity-like)}
\end{align}

\textbf{Layer 1:}
\begin{align}
\mathbf{A}_1 &\in \mathbb{R}^{256 \times 128} \quad \text{(output: $128 \rightarrow 256$)} \\
\mathbf{B}_1 &\in \mathbb{R}^{256 \times 128} \quad \text{(input: $128 \rightarrow 256$)}
\end{align}

\textbf{Layer 2:}
\begin{align}
\mathbf{A}_2 &\in \mathbb{R}^{10 \times 10} \quad \text{(output: $10 \rightarrow 10$, identity-like)} \\
\mathbf{B}_2 &\in \mathbb{R}^{256 \times 128} \quad \text{(input: $128 \rightarrow 256$)}
\end{align}


\subsection{Computing and Verifying Matrix $V$}

Below we visually represent each layer transformation and complete a dimension verification.

\textbf{Layer 0 Transformation:}

\begin{equation}
\mathbf{V}_0 = \mathbf{A}_0 \mathbf{W}_0 \mathbf{B}_0^T
\end{equation}

\begin{center}
\begin{tikzpicture}[scale=0.8]
    % A0 matrix (256 x 128)
    \draw[fill=matrixA, draw=black, thick] (0,1) rectangle (2,5);
    \draw[step=0.5,black,thin] (0,1) grid (2,5);
    \node at (1, 3) {\Large $\mathbf{A}_0$};
    \node[below] at (1, 0.7) {128};
    \node[left] at (-0.3, 4) {256};

    % Multiplication symbol
    \node at (2.7, 3) {\huge $\cdot$};

    % W0 matrix (128 x 64)
    \draw[fill=matrixW, draw=black, thick] (3.4,1) rectangle (5.4,5);
    \draw[step=0.5,black,thin] (3.4,1) grid (5.4,5);
    \node at (4.4, 3) {\Large $\mathbf{W}_0$};
    \node[below] at (4.4, 0.7) {64};
    \node[left] at (3.1, 4) {128};

    % Multiplication symbol
    \node at (6.1, 3) {\huge $\cdot$};

    % B0^T matrix (64 x 64)
    \draw[fill=matrixB, draw=black, thick] (6.8,1) rectangle (8.8,5);
    \draw[step=0.5,black,thin] (6.8,1) grid (8.8,5);
    \node at (7.8, 3) {\Large $\mathbf{B}_0^T$};
    \node[below] at (7.8, 0.7) {64};
    \node[left] at (6.5, 4) {64};

    % Equals symbol
    \node at (9.5, 3) {\huge $=$};

    % V0 matrix (256 x 64)
    \draw[fill=matrixV, draw=black, thick] (10.2,1) rectangle (12.2,5);
    \draw[step=0.5,black,thin] (10.2,1) grid (12.2,5);
    \node at (11.2, 3) {\Large $\mathbf{V}_0$};
    \node[below] at (11.2, 0.7) {64};
    \node[left] at (9.9, 4) {256};
\end{tikzpicture}
\end{center}

\textbf{Dimension verification:}
\begin{align}
\mathbf{V}_0 &= \mathbf{A}_0 \mathbf{W}_0 \mathbf{B}_0^T \\
&= (256 \times 128) \cdot (128 \times 64) \cdot (64 \times 64) \\
&= (256 \times 64) \quad \checkmark
\end{align}

\textbf{Layer 1 Transformation}

\begin{equation}
\mathbf{V}_1 = \mathbf{A}_1 \mathbf{W}_1 \mathbf{B}_1^T
\end{equation}

\begin{center}
\begin{tikzpicture}[scale=0.8]
    % A1 matrix (256 x 128)
    \draw[fill=matrixA, draw=black, thick] (0,1) rectangle (2,5);
    \draw[step=0.5,black,thin] (0,1) grid (2,5);
    \node at (1, 3) {\Large $\mathbf{A}_1$};
    \node[below] at (1, 0.7) {128};
    \node[left] at (-0.3, 3.6) {256};

    % Multiplication symbol
    \node at (2.7, 3) {\huge $\cdot$};

    % W1 matrix (128 x 128)
    \draw[fill=matrixW, draw=black, thick] (3.4,1) rectangle (5.4,5);
    \draw[step=0.5,black,thin] (3.4,1) grid (5.4,5);
    \node at (4.4, 3) {\Large $\mathbf{W}_1$};
    \node[below] at (4.4, 0.7) {128};
    \node[left] at (3.1, 3.6) {128};

    % Multiplication symbol
    \node at (6.1, 3) {\huge $\cdot$};

    % B1^T matrix (128 x 256)
    \draw[fill=matrixB, draw=black, thick] (6.8,1.5) rectangle (10.8,4.5);
    \draw[step=0.5,black,thin] (6.8,1.5) grid (10.8,4.5);
    \node at (8.8, 3) {\Large $\mathbf{B}_1^T$};
    \node[below] at (8.8, 1.2) {256};
    \node[left] at (6.5, 3.6) {128};

    % Equals symbol
    \node at (11.5, 3) {\huge $=$};

    % V1 matrix (256 x 256)
    \draw[fill=matrixV, draw=black, thick] (12.2,1.5) rectangle (16.2,4.5);
    \draw[step=0.5,black,thin] (12.2,1.5) grid (16.2,4.5);
    \node at (14.2, 3) {\Large $\mathbf{V}_1$};
    \node[below] at (14.2, 1.2) {256};
    \node[left] at (11.9, 3.6) {256};
\end{tikzpicture}
\end{center}

\textbf{Dimension verification:}
\begin{align}
\mathbf{V}_1 &= \mathbf{A}_1 \mathbf{W}_1 \mathbf{B}_1^T \\
&= (256 \times 128) \cdot (128 \times 128) \cdot (128 \times 256) \\
&= (256 \times 256) \quad \checkmark
\end{align}

\textbf{Layer 2 Transformation}

\begin{equation}
\mathbf{V}_2 = \mathbf{A}_2 \mathbf{W}_2 \mathbf{B}_2^T
\end{equation}

\begin{center}
\begin{tikzpicture}[scale=0.8]
    % A2 matrix (10 x 10)
    \draw[fill=matrixA, draw=black, thick] (0,1.5) rectangle (1.5,3.5);
    \draw[step=0.5,black,thin] (0,1.5) grid (1.5,3.5);
    \node at (0.75, 2.5) {\Large $\mathbf{A}_2$};
    \node[below] at (0.75, 1.2) {10};
    \node[left] at (-0.3, 3.6) {10};

    % Multiplication symbol
    \node at (2.2, 2.5) {\huge $\cdot$};

    % W2 matrix (10 x 128)
    \draw[fill=matrixW, draw=black, thick] (2.9,1.5) rectangle (6.9,3.5);
    \draw[step=0.5,black,thin] (2.9,1.5) grid (6.9,3.5);
    \node at (4.9, 2.5) {\Large $\mathbf{W}_2$};
    \node[below] at (4.9, 1.2) {128};
    \node[left] at (2.6, 3.6) {10};

    % Multiplication symbol
    \node at (7.6, 2.5) {\huge $\cdot$};

    % B2^T matrix (128 x 256)
    \draw[fill=matrixB, draw=black, thick] (8.3,1) rectangle (12.3,4);
    \draw[step=0.5,black,thin] (8.3,1) grid (12.3,4);
    \node at (10.3, 2.5) {\Large $\mathbf{B}_2^T$};
    \node[below] at (10.3, 0.7) {256};
    \node[left] at (8, 3.6) {128};

    % Equals symbol
    \node at (13, 2.5) {\huge $=$};

    % V2 matrix (10 x 256)
    \draw[fill=matrixV, draw=black, thick] (13.7,1) rectangle (17.7,4);
    \draw[step=0.5,black,thin] (13.7,1) grid (17.7,4);
    \node at (15.7, 2.5) {\Large $\mathbf{V}_2$};
    \node[below] at (15.7, 0.7) {256};
    \node[left] at (13.4, 3.6) {10};
\end{tikzpicture}
\end{center}

\textbf{Dimension verification:}
\begin{align}
\mathbf{V}_2 &= \mathbf{A}_2 \mathbf{W}_2 \mathbf{B}_2^T \\
&= (10 \times 10) \cdot (10 \times 128) \cdot (128 \times 256) \\
&= (10 \times 256) \quad \checkmark
\end{align}

\subsection{Summary}
In the table below, we summarize the matrix size information. 

\begin{table}[h]
\centering
\begin{tabular}{@{}lcccc@{}}
\toprule
Layer & Original $\mathbf{W}$ & $\mathbf{A}$ & $\mathbf{B}$ & Result $\mathbf{V = AWB^T}$ \\
\midrule
0 & $128 \times 64$ & $256 \times 128$ & $64 \times 64$ & $256 \times 64$ \\
1 & $128 \times 128$ & $256 \times 128$ & $256 \times 128$ & $256 \times 256$ \\
2 & $10 \times 128$ & $10 \times 10$ & $256 \times 128$ & $10 \times 256$ \\
\bottomrule
\end{tabular}
\caption{FFN transformation dimensions}
\end{table}

\newpage

\section{CNN3D AWB Calculation}\label{app: CNN3d AB}\label{app: CNN AB}
CNN processes single-channel images (e.g., MNIST) with one convolutional layer followed by feed-forward layers. In this section, we complete an explicit example that determines the sizes for $A$ and $B$ matrices for each layer of a convolutional neural network to transfer learning. This includes the $A$ and $B$ matrices for both the filter and the feedforward portion of the neural network.

\subsection{Original Architecture}
We begin with the following original architecture: 
\begin{equation}
\text{feed\_sizes} = [2304, 256, 10]
\end{equation}
and
\begin{equation}
    \text{filter\_size} = 3.
\end{equation}
After two conv+pool layers, the flattened feature size is 2304. This feeds into:
\begin{itemize}
    \item Layer 0: $2304 \rightarrow 256$
    \item Layer 1: $256 \rightarrow 10$
\end{itemize}

\subsection{Optimal Architecture Specifications}

After completing an architecture search, it was determined that the following architecture was optimal:
\begin{equation}
\text{new\_arch} = [1600, 512, 10]
\end{equation}
and
\begin{equation}
    \text{filter\_size} = 5.
\end{equation}

\subsection{Determining Appropriate $A$ and $B$ Matrices}

We need to obtain weights matrices for both the covolutional layer and the feed forward layers which correspond to the new architecture. This is completed by determines transfer matrices. We let $A_\text{filter}$ and $B_\text{filter}$ be the transfer matrices used to determine the new weights, denoted $V_\text{filter}$ for the convolutional layer. We let $A_i$ and $B_i$ be the transfer matrices used to determine the new weights, denoted $V_i$ for the feedforward layers. We break these calculations into two sections for clarity.\\

\textbf{Convolutional Filter Transformation}\\

Recall that the original filter size is $3$ and the new filter size is $5.$ For a single convolutional filter $\mathbf{W}_{\text{filter}}^{(i,c)}$ where $i$ is the output channel and $c$ is the input channel:

\begin{equation}
\mathbf{V}_{\text{filter}}^{(i,c)} = \mathbf{A}_{\text{conv}}^{(i,c)} \mathbf{W}_{\text{filter}}^{(i,c)} \mathbf{B}_{\text{conv}}^{(i,c)T}
\end{equation}

\begin{center}
\begin{tikzpicture}[scale=1.0]
    % A matrix (5 x 3)
    \draw[fill=matrixA, draw=black, thick] (0,1) rectangle (1.5,3.5);
    \draw[step=0.5,black,thin] (0,1) grid (1.5,3.5);
    \node at (0.75, 2.25) {$\mathbf{A}$};
    \node[below] at (0.75, .7) {3};
    \node[left] at (-0.3, 3) {5};

    % Multiplication symbol
    \node at (2.1, 2.25) {$\cdot$};

    % W matrix (3 x 3)
    \draw[fill=matrixW, draw=black, thick] (2.7,1) rectangle (4.2,3.5);
    \draw[step=0.5,black,thin] (2.7,1) grid (4.2,3.5);
    \node at (3.45, 2.25) {$\mathbf{W}$};
    \node[below] at (3.45, 0.7) {3};
    \node[left] at (2.4, 3) {3};

    % Multiplication symbol
    \node at (4.8, 2.25) {$\cdot$};

    % B^T matrix (3 x 5)
    \draw[fill=matrixB, draw=black, thick] (5.4,1.5) rectangle (7.9,3);
    \draw[step=0.5,black,thin] (5.4,1.5) grid (7.9,3);
    \node at (6.65, 2.25) {$\mathbf{B}^T$};
    \node[below] at (6.65, 1.2) {5};
    \node[left] at (5.1, 3) {3};

    % Equals symbol
    \node at (8.5, 2.25) {$=$};

    % V matrix (5 x 5)
    \draw[fill=matrixV, draw=black, thick] (9.1,1) rectangle (11.6,3.5);
    \draw[step=0.5,black,thin] (9.1,1) grid (11.6,3.5);
    \node at (10.35, 2.25) {$\mathbf{V}$};
    \node[below] at (10.35, 0.7) {5};
    \node[left] at (8.8, 3) {5};
\end{tikzpicture}
\end{center}

\textbf{Dimension verification:}
\begin{align}
\mathbf{V}_{\text{filter}} &= \mathbf{A} \mathbf{W}_{\text{filter}} \mathbf{B}^T \\
&= (5 \times 3) \cdot (3 \times 3) \cdot (3 \times 5) \\
&= (5 \times 5) \quad \checkmark
\end{align}

This transformation is applied to each filter in each convolutional layer, expanding the spatial kernel size from $3 \times 3$ to $5 \times 5$. Note that a single-channel would be used when training on the MNIST dataset. When using RGB dataset, we would use a three-channel network.


\textbf{Feed-Forward Layers}

With expanded filters (5×5 instead of 3×3), the flattened size becomes 1600:
\begin{itemize}
    \item Layer 0: $1600 \rightarrow 512$
    \item Layer 1: $512 \rightarrow 10$
\end{itemize}

\textbf{Feed Layer 0 Transformation}

\begin{equation}
\mathbf{V}_0 = \mathbf{A}_0 \mathbf{W}_0 \mathbf{B}_0^T
\end{equation}

\begin{center}
\begin{tikzpicture}[scale=0.7]
    % A0 matrix (512 x 256)
    \draw[fill=matrixA, draw=black, thick] (0,1) rectangle (2,6);
    \draw[step=0.5,black,thin] (0,1) grid (2,6);
    \node at (1, 3.5) {\Large $\mathbf{A}_0$};
    \node[below] at (1, 0.7) {256};
    \node[left] at (-0.3, 4.5) {512};

    % Multiplication symbol
    \node at (2.7, 3.5) {\huge $\cdot$};

    % W0 matrix (256 x 2304)
    \draw[fill=matrixW, draw=black, thick] (3.4,2) rectangle (9.4,5);
    \draw[step=0.5,black,thin] (3.4,2) grid (9.4,5);
    \node at (6.4, 3.5) {\Large $\mathbf{W}_0$};
    \node[below] at (6.4, 1.7) {2304};
    \node[left] at (3.1, 4.5) {256};

    % Multiplication symbol
    \node at (10.1, 3.5) {\huge $\cdot$};

    % B0^T matrix (2304 x 1600)
    \draw[fill=matrixB, draw=black, thick] (10.8,1.5) rectangle (16.8,5.5);
    \draw[step=0.5,black,thin] (10.8,1.5) grid (16.8,5.5);
    \node at (13.8, 3.5) {\Large $\mathbf{B}_0^T$};
    \node[below] at (13.8, 1.2) {1600};
    \node[left] at (10.5, 4.5) {2304};

    % Equals symbol
    \node at (17.5, 3.5) {\huge $=$};

    % V0 matrix (512 x 1600)
    \draw[fill=matrixV, draw=black, thick] (18.2,1.5) rectangle (24.2,5.5);
    \draw[step=0.5,black,thin] (18.2,1.5) grid (24.2,5.5);
    \node at (21.2, 3.5) {\Large $\mathbf{V}_0$};
    \node[below] at (21.2, 1.2) {1600};
    \node[left] at (17.9, 4.5) {512};
\end{tikzpicture}
\end{center}

\textbf{Dimension verification:}
\begin{align}
\mathbf{V}_0 &= \mathbf{A}_0 \mathbf{W}_0 \mathbf{B}_0^T \\
&= (512 \times 256) \cdot (256 \times 2304) \cdot (2304 \times 1600) \\
&= (512 \times 1600) \quad \checkmark
\end{align}

\textbf{Feed Layer 1 Transformation}

\begin{equation}
\mathbf{V}_1 = \mathbf{A}_1 \mathbf{W}_1 \mathbf{B}_1^T
\end{equation}

\begin{center}
\begin{tikzpicture}[scale=0.8]
    % A1 matrix (10 x 10)
    \draw[fill=matrixA, draw=black, thick] (0,1.5) rectangle (1.5,3.5);
    \draw[step=0.5,black,thin] (0,1.5) grid (1.5,3.5);
    \node at (0.75, 2.5) {\Large $\mathbf{A}_1$};
    \node[below] at (0.75, 1.2) {10};
    \node[left] at (-0.3, 4.5) {10};

    % Multiplication symbol
    \node at (2.2, 2.5) {\huge $\cdot$};

    % W1 matrix (10 x 256)
    \draw[fill=matrixW, draw=black, thick] (2.9,1.5) rectangle (6.9,3.5);
    \draw[step=0.5,black,thin] (2.9,1.5) grid (6.9,3.5);
    \node at (4.9, 2.5) {\Large $\mathbf{W}_1$};
    \node[below] at (4.9, 1.2) {256};
    \node[left] at (2.6, 4.5) {10};

    % Multiplication symbol
    \node at (7.6, 2.5) {\huge $\cdot$};

    % B1^T matrix (256 x 512)
    \draw[fill=matrixB, draw=black, thick] (8.3,1) rectangle (12.3,4);
    \draw[step=0.5,black,thin] (8.3,1) grid (12.3,4);
    \node at (10.3, 2.5) {\Large $\mathbf{B}_1^T$};
    \node[below] at (10.3, 0.7) {512};
    \node[left] at (8, 4.5) {256};

    % Equals symbol
    \node at (13, 2.5) {\huge $=$};

    % V1 matrix (10 x 512)
    \draw[fill=matrixV, draw=black, thick] (13.7,1) rectangle (17.7,4);
    \draw[step=0.5,black,thin] (13.7,1) grid (17.7,4);
    \node at (15.7, 2.5) {\Large $\mathbf{V}_1$};
    \node[below] at (15.7, 0.7) {512};
    \node[left] at (13.4, 4.5) {10};
\end{tikzpicture}
\end{center}

\textbf{Dimension verification:}
\begin{align}
\mathbf{V}_1 &= \mathbf{A}_1 \mathbf{W}_1 \mathbf{B}_1^T \\
&= (10 \times 10) \cdot (10 \times 256) \cdot (256 \times 512) \\
&= (10 \times 512) \quad \checkmark
\end{align}

\newpage

% \section{CNN AWB Calculation}\label{app: CNN AB}

% In this section, we complete an explicit example that determines the sizes for $A$ and $B$ matrices for each layer of a convolutional neural network to transfer learning. This includes the $A$ and $B$ matrices for both the filter and the feedforward portion of the neural network. 

% CNN processes single-channel images (e.g., MNIST) with one convolutional layer followed by feed-forward layers.

% \subsection{Feed-Forward Layers}

% \textbf{Original architecture:}
% \begin{equation}
% \text{feed\_sizes} = [1728, 64, 10]
% \end{equation}

% After conv+pool, the flattened size is 1728:
% \begin{itemize}
%     \item Layer 0: $1728 \rightarrow 64$
%     \item Layer 1: $64 \rightarrow 10$
% \end{itemize}

% \textbf{AWB architecture:}
% \begin{equation}
% \text{new\_arch} = [1875, 700, 10]
% \end{equation}

% With AWB filters:
% \begin{itemize}
%     \item Layer 0: $1875 \rightarrow 700$
%     \item Layer 1: $700 \rightarrow 10$
% \end{itemize}

% \subsection{Feed Layer 0 Transformation}

% \begin{equation}
% \mathbf{V}_0 = \mathbf{A}_0 \mathbf{W}_0 \mathbf{B}_0^T
% \end{equation}

% \begin{center}
% \begin{tikzpicture}[scale=0.7]
%     % A0 matrix (700 x 64)
%     \draw[fill=matrixA, draw=black, thick] (0,1) rectangle (1.5,6);
%     \draw[step=0.5,black,thin] (0,1) grid (1.5,6);
%     \node at (0.75, 3.5) {\Large $\mathbf{A}_0$};
%     \node[below] at (0.75, 0.7) {64};
%     \node[left] at (-0.3, 4.5) {700};

%     % Multiplication symbol
%     \node at (2.2, 3.5) {\huge $\cdot$};

%     % W0 matrix (64 x 1728)
%     \draw[fill=matrixW, draw=black, thick] (2.9,2.5) rectangle (8.9,4.5);
%     \draw[step=0.5,black,thin] (2.9,2.5) grid (8.9,4.5);
%     \node at (5.9, 3.5) {\Large $\mathbf{W}_0$};
%     \node[below] at (5.9, 2.2) {1728};
%     \node[left] at (2.6, 4.5) {64};

%     % Multiplication symbol
%     \node at (9.6, 3.5) {\huge $\cdot$};

%     % B0^T matrix (1728 x 1875)
%     \draw[fill=matrixB, draw=black, thick] (10.3,1.5) rectangle (16.3,5.5);
%     \draw[step=0.5,black,thin] (10.3,1.5) grid (16.3,5.5);
%     \node at (13.3, 3.5) {\Large $\mathbf{B}_0^T$};
%     \node[below] at (13.3, 1.2) {1875};
%     \node[left] at (10, 4.5) {1728};

%     % Equals symbol
%     \node at (17, 3.5) {\huge $=$};

%     % V0 matrix (700 x 1875)
%     \draw[fill=matrixV, draw=black, thick] (17.7,1.5) rectangle (23.7,5.5);
%     \draw[step=0.5,black,thin] (17.7,1.5) grid (23.7,5.5);
%     \node at (20.7, 3.5) {\Large $\mathbf{V}_0$};
%     \node[below] at (20.7, 1.2) {1875};
%     \node[left] at (17.4, 4.5) {700};
% \end{tikzpicture}
% \end{center}

% \textbf{Dimension verification:}
% \begin{align}
% \mathbf{V}_0 &= \mathbf{A}_0 \mathbf{W}_0 \mathbf{B}_0^T \\
% &= (700 \times 64) \cdot (64 \times 1728) \cdot (1728 \times 1875) \\
% &= (700 \times 1875) \quad \checkmark
% \end{align}

% \subsection{Feed Layer 1 Transformation}

% \begin{equation}
% \mathbf{V}_1 = \mathbf{A}_1 \mathbf{W}_1 \mathbf{B}_1^T
% \end{equation}

% \begin{center}
% \begin{tikzpicture}[scale=0.8]
%     % A1 matrix (10 x 10)
%     \draw[fill=matrixA, draw=black, thick] (0,1.5) rectangle (1.5,3.5);
%     \draw[step=0.5,black,thin] (0,1.5) grid (1.5,3.5);
%     \node at (0.75, 2.5) {\Large $\mathbf{A}_1$};
%     \node[below] at (0.75, 1.2) {10};
%     \node[left] at (-0.3, 4.5) {10};

%     % Multiplication symbol
%     \node at (2.2, 2.5) {\huge $\cdot$};

%     % W1 matrix (10 x 64)
%     \draw[fill=matrixW, draw=black, thick] (2.9,1.5) rectangle (5.9,3.5);
%     \draw[step=0.5,black,thin] (2.9,1.5) grid (5.9,3.5);
%     \node at (4.4, 2.5) {\Large $\mathbf{W}_1$};
%     \node[below] at (4.4, 1.2) {64};
%     \node[left] at (2.6, 4.5) {10};

%     % Multiplication symbol
%     \node at (6.6, 2.5) {\huge $\cdot$};

%     % B1^T matrix (64 x 700)
%     \draw[fill=matrixB, draw=black, thick] (7.3,1) rectangle (11.3,4);
%     \draw[step=0.5,black,thin] (7.3,1) grid (11.3,4);
%     \node at (9.3, 2.5) {\Large $\mathbf{B}_1^T$};
%     \node[below] at (9.3, 0.7) {700};
%     \node[left] at (7, 4.5) {64};

%     % Equals symbol
%     \node at (12, 2.5) {\huge $=$};

%     % V1 matrix (10 x 700)
%     \draw[fill=matrixV, draw=black, thick] (12.7,1) rectangle (16.7,4);
%     \draw[step=0.5,black,thin] (12.7,1) grid (16.7,4);
%     \node at (14.7, 2.5) {\Large $\mathbf{V}_1$};
%     \node[below] at (14.7, 0.7) {700};
%     \node[left] at (12.4, 4.5) {10};
% \end{tikzpicture}
% \end{center}

% \textbf{Dimension verification:}
% \begin{align}
% \mathbf{V}_1 &= \mathbf{A}_1 \mathbf{W}_1 \mathbf{B}_1^T \\
% &= (10 \times 10) \cdot (10 \times 64) \cdot (64 \times 700) \\
% &= (10 \times 700) \quad \checkmark
% \end{align}

% \newpage

\section{GCN AWB Calculation}\label{app: GCN}

Graph Convolutional Networks (GCN) consist of graph convolutional layers followed by feed-forward layers for graph classification.

\subsection{Architecture Specification}

\textbf{GCN layers:}
\begin{equation}
\text{gcn\_sizes} = [5, 64]
\end{equation}

Single GCN layer: $5 \rightarrow 64$ (5 input node features, 64 output features)

\textbf{Feed-forward layers:}
\begin{equation}
\text{feed\_sizes} = [64, 32, 16, 10]
\end{equation}

Three feed layers after graph pooling:
\begin{itemize}
    \item Layer 0: $64 \rightarrow 32$
    \item Layer 1: $32 \rightarrow 16$
    \item Layer 2: $16 \rightarrow 10$
\end{itemize}

\textbf{AWB architectures:}
\begin{align}
\text{awb\_gcn\_arch} &= [5, 64] \quad \text{(preserved)} \\
\text{awb\_fnn\_arch} &= [64, 140, 140, 10]
\end{align}

Note: GCN layer dimensions are preserved (no expansion), while feed layers expand hidden dimensions.

\subsection{GCN Layer Transformation}

\textbf{GCN forward pass:} $\mathbf{y} = \mathbf{A}_{\text{adj}} (\mathbf{x} \mathbf{W})$

where $\mathbf{A}_{\text{adj}}$ is the normalized adjacency matrix.

Since AWB preserves dimensions: $5 \rightarrow 64$ remains $5 \rightarrow 64$

\begin{equation}
\mathbf{V}_{\text{gcn}} = \mathbf{A}_{\text{gcn}} \mathbf{W}_{\text{gcn}} \mathbf{B}_{\text{gcn}}^T
\end{equation}

\begin{center}
\begin{tikzpicture}[scale=0.9]
    % A matrix (64 x 64)
    \draw[fill=matrixA, draw=black, thick] (0,1) rectangle (2.5,3.5);
    \draw[step=0.5,black,thin] (0,1) grid (2.5,3.5);
    \node at (1.25, 2.25) {$\mathbf{A}_{\text{gcn}}$};
    \node[below] at (1.25, 0.7) {64};
    \node[left] at (-0.3, 2.25) {64};

    % Multiplication symbol
    \node at (3.1, 2.25) {$\cdot$};

    % W matrix (64 x 5)
    \draw[fill=matrixW, draw=black, thick] (3.7,1.5) rectangle (5.2,3);
    \draw[step=0.5,black,thin] (3.7,1.5) grid (5.2,3);
    \node at (4.45, 2.25) {$\mathbf{W}_{\text{gcn}}$};
    \node[below] at (4.45, 1.2) {5};
    \node[left] at (3.4, 2.25) {64};

    % Multiplication symbol
    \node at (5.8, 2.25) {$\cdot$};

    % B^T matrix (5 x 64)
    \draw[fill=matrixB, draw=black, thick] (6.4,1.75) rectangle (8.9,2.75);
    \draw[step=0.5,black,thin] (6.4,1.75) grid (8.9,2.75);
    \node at (7.65, 2.25) {$\mathbf{B}_{\text{gcn}}^T$};
    \node[below] at (7.65, 1.45) {64};
    \node[left] at (6.1, 2.25) {5};

    % Equals symbol
    \node at (9.5, 2.25) {$=$};

    % V matrix (64 x 64) - note: should be (5, 64) for weight matrix
    \draw[fill=matrixV, draw=black, thick] (10.1,1.5) rectangle (12.6,3);
    \draw[step=0.5,black,thin] (10.1,1.5) grid (12.6,3);
    \node at (11.35, 2.25) {$\mathbf{V}_{\text{gcn}}$};
    \node[below] at (11.35, 1.2) {64};
    \node[left] at (9.8, 2.25) {5};
\end{tikzpicture}
\end{center}

\textbf{Dimension verification:}
\begin{align}
\mathbf{V}_{\text{gcn}} &= \mathbf{A}_{\text{gcn}} \mathbf{W}_{\text{gcn}} \mathbf{B}_{\text{gcn}}^T \\
&= (64 \times 64) \cdot (64 \times 5) \cdot (5 \times 64) \\
&= (64 \times 64) \text{ intermediate} \cdot (64 \times 5) \\
&= (5 \times 64) \quad \checkmark \quad \text{(dimensions preserved)}
\end{align}

Wait, let me recalculate. For GCN, weight matrix has shape (in\_size, out\_size) = (5, 64).

With $\mathbf{A} \in \mathbb{R}^{64 \times 64}$ and $\mathbf{B} \in \mathbb{R}^{64 \times 5}$:
\begin{align}
\mathbf{V}_{\text{gcn}} &= \mathbf{A}_{\text{gcn}} \mathbf{W}_{\text{gcn}} \mathbf{B}_{\text{gcn}}^T \\
&= (64 \times 64) \cdot (5 \times 64) \quad \text{dimension mismatch!}
\end{align}

Correction: For GCN layer preserving $(5 \times 64)$, we need:
\begin{align}
\mathbf{A}_{\text{gcn}} &\in \mathbb{R}^{5 \times 5} \\
\mathbf{B}_{\text{gcn}} &\in \mathbb{R}^{64 \times 64}
\end{align}

\begin{center}
\begin{tikzpicture}[scale=0.9]
    % A matrix (5 x 5) - actually should be (64 x 64) for output, but preserving means identity-like
    \draw[fill=matrixA, draw=black, thick] (0,1.75) rectangle (1.5,2.75);
    \draw[step=0.5,black,thin] (0,1.75) grid (1.5,2.75);
    \node at (0.75, 2.25) {$\mathbf{A}$};
    \node[below] at (0.75, 1.45) {5};
    \node[left] at (-0.3, 2.25) {5};

    % Multiplication symbol
    \node at (2.1, 2.25) {$\cdot$};

    % W matrix (5 x 64)
    \draw[fill=matrixW, draw=black, thick] (2.7,1.75) rectangle (5.7,2.75);
    \draw[step=0.5,black,thin] (2.7,1.75) grid (5.7,2.75);
    \node at (4.2, 2.25) {$\mathbf{W}$};
    \node[below] at (4.2, 1.45) {64};
    \node[left] at (2.4, 2.25) {5};

    % Multiplication symbol
    \node at (6.3, 2.25) {$\cdot$};

    % B^T matrix (64 x 64)
    \draw[fill=matrixB, draw=black, thick] (6.9,1) rectangle (9.4,3.5);
    \draw[step=0.5,black,thin] (6.9,1) grid (9.4,3.5);
    \node at (8.15, 2.25) {$\mathbf{B}^T$};
    \node[below] at (8.15, 0.7) {64};
    \node[left] at (6.6, 2.25) {64};

    % Equals symbol
    \node at (10, 2.25) {$=$};

    % V matrix (5 x 64)
    \draw[fill=matrixV, draw=black, thick] (10.6,1.75) rectangle (13.6,2.75);
    \draw[step=0.5,black,thin] (10.6,1.75) grid (13.6,2.75);
    \node at (12.1, 2.25) {$\mathbf{V}$};
    \node[below] at (12.1, 1.45) {64};
    \node[left] at (10.3, 2.25) {5};
\end{tikzpicture}
\end{center}

\textbf{Corrected dimension verification:}
\begin{align}
\mathbf{V}_{\text{gcn}} &= \mathbf{A}_{\text{gcn}} \mathbf{W}_{\text{gcn}} \mathbf{B}_{\text{gcn}}^T \\
&= (5 \times 5) \cdot (5 \times 64) \cdot (64 \times 64) \\
&= (5 \times 64) \quad \checkmark \quad \text{(dimensions preserved)}
\end{align}

\subsection{Feed Layer 0 Transformation}

Note: Feed layers use Linear3 with weight shape (out\_size, in\_size), so forward pass is $\mathbf{y} = \mathbf{x} \mathbf{W}^T + \mathbf{b}$.

\begin{equation}
\mathbf{V}_0 = \mathbf{A}_0 \mathbf{W}_0 \mathbf{B}_0^T
\end{equation}

Original: $(32 \times 64)$, AWB: $(140 \times 64)$

\begin{center}
\begin{tikzpicture}[scale=0.8]
    % A0 matrix (140 x 32)
    \draw[fill=matrixA, draw=black, thick] (0,1) rectangle (1.5,5);
    \draw[step=0.5,black,thin] (0,1) grid (1.5,5);
    \node at (0.75, 3) {\Large $\mathbf{A}_0$};
    \node[below] at (0.75, 0.7) {32};
    \node[left] at (-0.3, 3) {140};

    % Multiplication symbol
    \node at (2.2, 3) {\huge $\cdot$};

    % W0 matrix (32 x 64)
    \draw[fill=matrixW, draw=black, thick] (2.9,1.5) rectangle (5.9,4.5);
    \draw[step=0.5,black,thin] (2.9,1.5) grid (5.9,4.5);
    \node at (4.4, 3) {\Large $\mathbf{W}_0$};
    \node[below] at (4.4, 1.2) {64};
    \node[left] at (2.6, 3) {32};

    % Multiplication symbol
    \node at (6.6, 3) {\huge $\cdot$};

    % B0^T matrix (64 x 64)
    \draw[fill=matrixB, draw=black, thick] (7.3,1.5) rectangle (10.3,4.5);
    \draw[step=0.5,black,thin] (7.3,1.5) grid (10.3,4.5);
    \node at (8.8, 3) {\Large $\mathbf{B}_0^T$};
    \node[below] at (8.8, 1.2) {64};
    \node[left] at (7, 3) {64};

    % Equals symbol
    \node at (11, 3) {\huge $=$};

    % V0 matrix (140 x 64)
    \draw[fill=matrixV, draw=black, thick] (11.7,1.5) rectangle (14.7,4.5);
    \draw[step=0.5,black,thin] (11.7,1.5) grid (14.7,4.5);
    \node at (13.2, 3) {\Large $\mathbf{V}_0$};
    \node[below] at (13.2, 1.2) {64};
    \node[left] at (11.4, 3) {140};
\end{tikzpicture}
\end{center}

\textbf{Dimension verification:}
\begin{align}
\mathbf{V}_0 &= \mathbf{A}_0 \mathbf{W}_0 \mathbf{B}_0^T \\
&= (140 \times 32) \cdot (32 \times 64) \cdot (64 \times 64) \\
&= (140 \times 64) \quad \checkmark
\end{align}

\textbf{Note:} Hidden layer expanded from 32 to 140, input (64) preserved.

\subsection{Feed Layer 1 Transformation}

\begin{equation}
\mathbf{V}_1 = \mathbf{A}_1 \mathbf{W}_1 \mathbf{B}_1^T
\end{equation}

Original: $(16 \times 32)$, AWB: $(140 \times 140)$

\begin{center}
\begin{tikzpicture}[scale=0.8]
    % A1 matrix (140 x 16)
    \draw[fill=matrixA, draw=black, thick] (0,1) rectangle (1.5,5);
    \draw[step=0.5,black,thin] (0,1) grid (1.5,5);
    \node at (0.75, 3) {\Large $\mathbf{A}_1$};
    \node[below] at (0.75, 0.7) {16};
    \node[left] at (-0.3, 3) {140};

    % Multiplication symbol
    \node at (2.2, 3) {\huge $\cdot$};

    % W1 matrix (16 x 32)
    \draw[fill=matrixW, draw=black, thick] (2.9,2) rectangle (4.9,4);
    \draw[step=0.5,black,thin] (2.9,2) grid (4.9,4);
    \node at (3.9, 3) {\Large $\mathbf{W}_1$};
    \node[below] at (3.9, 1.7) {32};
    \node[left] at (2.6, 3) {16};

    % Multiplication symbol
    \node at (5.6, 3) {\huge $\cdot$};

    % B1^T matrix (32 x 140)
    \draw[fill=matrixB, draw=black, thick] (6.3,1.5) rectangle (10.3,4.5);
    \draw[step=0.5,black,thin] (6.3,1.5) grid (10.3,4.5);
    \node at (8.3, 3) {\Large $\mathbf{B}_1^T$};
    \node[below] at (8.3, 1.2) {140};
    \node[left] at (6, 3) {32};

    % Equals symbol
    \node at (11, 3) {\huge $=$};

    % V1 matrix (140 x 140)
    \draw[fill=matrixV, draw=black, thick] (11.7,1.5) rectangle (15.7,4.5);
    \draw[step=0.5,black,thin] (11.7,1.5) grid (15.7,4.5);
    \node at (13.7, 3) {\Large $\mathbf{V}_1$};
    \node[below] at (13.7, 1.2) {140};
    \node[left] at (11.4, 3) {140};
\end{tikzpicture}
\end{center}

\textbf{Dimension verification:}
\begin{align}
\mathbf{V}_1 &= \mathbf{A}_1 \mathbf{W}_1 \mathbf{B}_1^T \\
&= (140 \times 16) \cdot (16 \times 32) \cdot (32 \times 140) \\
&= (140 \times 140) \quad \checkmark
\end{align}

\subsection{Feed Layer 2 Transformation}

\begin{equation}
\mathbf{V}_2 = \mathbf{A}_2 \mathbf{W}_2 \mathbf{B}_2^T
\end{equation}

Original: $(10 \times 16)$, AWB: $(10 \times 140)$

\begin{center}
\begin{tikzpicture}[scale=0.8]
    % A2 matrix (10 x 10)
    \draw[fill=matrixA, draw=black, thick] (0,1.5) rectangle (1.5,3.5);
    \draw[step=0.5,black,thin] (0,1.5) grid (1.5,3.5);
    \node at (0.75, 2.5) {\Large $\mathbf{A}_2$};
    \node[below] at (0.75, 1.2) {10};
    \node[left] at (-0.3, 2.5) {10};

    % Multiplication symbol
    \node at (2.2, 2.5) {\huge $\cdot$};

    % W2 matrix (10 x 16)
    \draw[fill=matrixW, draw=black, thick] (2.9,1.5) rectangle (4.9,3.5);
    \draw[step=0.5,black,thin] (2.9,1.5) grid (4.9,3.5);
    \node at (3.9, 2.5) {\Large $\mathbf{W}_2$};
    \node[below] at (3.9, 1.2) {16};
    \node[left] at (2.6, 2.5) {10};

    % Multiplication symbol
    \node at (5.6, 2.5) {\huge $\cdot$};

    % B2^T matrix (16 x 140)
    \draw[fill=matrixB, draw=black, thick] (6.3,1) rectangle (10.3,4);
    \draw[step=0.5,black,thin] (6.3,1) grid (10.3,4);
    \node at (8.3, 2.5) {\Large $\mathbf{B}_2^T$};
    \node[below] at (8.3, 0.7) {140};
    \node[left] at (6, 2.5) {16};

    % Equals symbol
    \node at (11, 2.5) {\huge $=$};

    % V2 matrix (10 x 140)
    \draw[fill=matrixV, draw=black, thick] (11.7,1) rectangle (15.7,4);
    \draw[step=0.5,black,thin] (11.7,1) grid (15.7,4);
    \node at (13.7, 2.5) {\Large $\mathbf{V}_2$};
    \node[below] at (13.7, 0.7) {140};
    \node[left] at (11.4, 2.5) {10};
\end{tikzpicture}
\end{center}

\textbf{Dimension verification:}
\begin{align}
\mathbf{V}_2 &= \mathbf{A}_2 \mathbf{W}_2 \mathbf{B}_2^T \\
&= (10 \times 10) \cdot (10 \times 16) \cdot (16 \times 140) \\
&= (10 \times 140) \quad \checkmark
\end{align}

\textbf{Note:} Output dimension (10) preserved, hidden layer expanded from 16 to 140.

\newpage

\section{Summary and Comparison}

\subsection{Dimension Preservation Principle}

Across all architectures, the AWB transformation follows the invariant:

\begin{center}
\boxed{\text{Input and output dimensions preserved, hidden layers expanded}}
\end{center}

\begin{table}[h]
\centering
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Architecture} & \textbf{Input Dim} & \textbf{Output Dim} & \textbf{Hidden Expansion} \\
\midrule
MLP & $64 \rightarrow 64$ & $10 \rightarrow 10$ & $128 \rightarrow 256$ \\
CNN3D (feed) & $2304 \rightarrow 1600^*$ & $10 \rightarrow 10$ & $256 \rightarrow 512$ \\
CNN (feed) & $1728 \rightarrow 1875^*$ & $10 \rightarrow 10$ & $64 \rightarrow 700$ \\
GCN (gcn) & $5 \rightarrow 5$ & $64 \rightarrow 64$ & N/A \\
GCN (feed) & $64 \rightarrow 64$ & $10 \rightarrow 10$ & $32,16 \rightarrow 140,140$ \\
\bottomrule
\end{tabular}
\caption{Dimension preservation across architectures. $^*$Input changes due to convolutional filter expansion.}
\end{table}

\subsection{Matrix Dimension Formulas}

For a layer with original weight $\mathbf{W} \in \mathbb{R}^{m_{\text{old}} \times n_{\text{old}}}$ transforming to $\mathbf{V} \in \mathbb{R}^{m_{\text{new}} \times n_{\text{new}}}$:

\begin{align}
\mathbf{A} &\in \mathbb{R}^{m_{\text{new}} \times m_{\text{old}}} \quad \text{(output transformation)} \\
\mathbf{B} &\in \mathbb{R}^{n_{\text{new}} \times n_{\text{old}}} \quad \text{(input transformation)} \\
\mathbf{V} &= \mathbf{A} \mathbf{W} \mathbf{B}^T \in \mathbb{R}^{m_{\text{new}} \times n_{\text{new}}}
\end{align}

\subsection{Special Cases}

\textbf{Identity-like transformations:}
\begin{itemize}
    \item When dimensions are preserved: $\mathbf{A} \in \mathbb{R}^{k \times k}$ becomes identity-like
    \item Example: MLP Layer 0 input ($64 \rightarrow 64$), $\mathbf{B}_0 \in \mathbb{R}^{64 \times 64}$
\end{itemize}

\textbf{Convolutional filters:}
\begin{itemize}
    \item 2D spatial transformation: $3 \times 3 \rightarrow 5 \times 5$
    \item Applied per-channel: $(i, c)$ for output channel $i$, input channel $c$
\end{itemize}

\section{Conclusion}

The AWB transformation $\mathbf{V} = \mathbf{A} \mathbf{W} \mathbf{B}^T$ enables seamless architecture morphing while preserving the fundamental input-output mapping of the network. The key insight is that transformation matrices $\mathbf{A}$ and $\mathbf{B}$ must be carefully dimensioned to:
\begin{enumerate}
    \item Preserve input feature dimensions (first layer)
    \item Preserve output class dimensions (last layer)
    \item Allow flexible expansion of hidden layer dimensions
\end{enumerate}

This framework applies uniformly across MLP, CNN, CNN3D, and GCN architectures, with special handling for convolutional filters and graph convolutions.