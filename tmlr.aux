\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{mccloskey1989catastrophic}
\citation{luo2025empirical,lai2025reinforcement,biderman2024lora}
\citation{lin2025continual}
\citation{raghavan2021formalizing,lu2025rethinking,lin2023theory}
\citation{chakraborty2025understanding}
\citation{chakraborty2025understanding}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\citation{CLEASgao2022CLEAS}
\citation{SEALgambella2025SEAL}
\citation{chakraborty2025understanding,raghavan2021formalizing}
\citation{lu2025rethinking}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Contribution}{2}{subsection.1.1}\protected@file@percent }
\citation{kolda2009tensor}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The basic problem of continual learning}}{3}{figure.1}\protected@file@percent }
\newlabel{fig:CL1}{{1}{3}{The basic problem of continual learning}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Organization}{3}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Notation}{3}{subsection.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Continual Learning -- Motivation}{3}{section.2}\protected@file@percent }
\newlabel{sec:motivation}{{2}{3}{Continual Learning -- Motivation}{section.2}{}}
\newlabel{eq:CL_for}{{{Forgetting Loss}}{4}{Continual Learning -- Motivation}{AMS.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The basic problem of continual learning}}{4}{figure.2}\protected@file@percent }
\newlabel{fig:param}{{2}{4}{The basic problem of continual learning}{figure.2}{}}
\citation{chakraborty2025understanding}
\citation{mahanNonclosednessSetsNeural2021a}
\citation{rudin1976principles}
\citation{huang2023normalization}
\newlabel{thm:task_nonstationary_weights}{{1}{5}{Continual Learning -- Motivation}{theorem.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Neural Networks -- Members of a Class of Sobolev Space Functions}{5}{section.3}\protected@file@percent }
\newlabel{defn:sobo}{{2}{5}{Neural Networks -- Members of a Class of Sobolev Space Functions}{theorem.2}{}}
\citation{kingmaAdamMethodStochastic2017}
\citation{petersenTopologicalPropertiesSet2021a}
\citation{adegokeHigherDerivativesInverse2016}
\citation{petersenTopologicalPropertiesSet2021a}
\citation{adegokeHigherDerivativesInverse2016}
\citation{petersenTopologicalPropertiesSet2021a}
\citation{adegokeHigherDerivativesInverse2016}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Different activation functions and the corresponding Sobolev spaces. Many of these results are described in \cite  {petersenTopologicalPropertiesSet2021a} and \cite  {adegokeHigherDerivativesInverse2016} . $\chi $ is an indicator function.}}{6}{table.1}\protected@file@percent }
\newlabel{tab:Sobolev_acti}{{1}{6}{Different activation functions and the corresponding Sobolev spaces. Many of these results are described in \cite {petersenTopologicalPropertiesSet2021a} and \cite {adegokeHigherDerivativesInverse2016} . $\chi $ is an indicator function}{table.1}{}}
\newlabel{defn:NN}{{3}{6}{Neural Networks -- Members of a Class of Sobolev Space Functions}{theorem.3}{}}
\citation{mahanNonclosednessSetsNeural2021a}
\citation{son2021sobolev}
\citation{CASpasunuru2019continual}
\citation{kingmaAdamMethodStochastic2017}
\citation{kingmaAdamMethodStochastic2017}
\citation{bertsekas2012dynamic}
\citation{bertsekas2012dynamic}
\newlabel{defn:NN_learning}{{4}{7}{Neural Networks -- Members of a Class of Sobolev Space Functions}{theorem.4}{}}
\citation{raghavan2021formalizing,chakraborty2025understanding}
\newlabel{eq:arch}{{{Architecture search}}{8}{Continual Learning in $\sobolev $}{AMS.5}{}}
\newlabel{eq:clcum}{{{Cumulative CL}}{8}{Continual Learning in $\sobolev $}{AMS.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Understanding the Existence of the CL Solution}{8}{section.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The actual problem}}{8}{figure.3}\protected@file@percent }
\newlabel{fig:challenge}{{3}{8}{The actual problem}{figure.3}{}}
\citation{weaver2013measure}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Set symmetric difference $A\bigtriangleup B$ represented by the gray-shaded region }}{9}{figure.4}\protected@file@percent }
\newlabel{fig:setsim}{{4}{9}{Set symmetric difference $A\bigtriangleup B$ represented by the gray-shaded region}{figure.4}{}}
\newlabel{defintersect}{{5}{9}{Understanding the Existence of the CL Solution}{theorem.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Existence}{9}{subsection.4.1}\protected@file@percent }
\newlabel{lem:contMeasure}{{6}{10}{Existence}{theorem.6}{}}
\newlabel{boundedremark}{{4}{11}{Existence}{remark.4}{}}
\newlabel{boundedremark}{{5}{11}{Existence}{remark.5}{}}
\newlabel{defabsolutely_continuous}{{8}{12}{Existence}{theorem.8}{}}
\newlabel{thm:contMeasure}{{9}{12}{Existence}{theorem.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Reachability to a Solution in the Presence of Dissimilar Tasks}{12}{subsection.4.2}\protected@file@percent }
\citation{evans2022partial}
\newlabel{lem:taylor}{{10}{13}{Reachability to a Solution in the Presence of Dissimilar Tasks}{theorem.10}{}}
\newlabel{archfinaltaylor}{{9}{13}{Reachability to a Solution in the Presence of Dissimilar Tasks}{equation.9}{}}
\newlabel{taylorexparch}{{10}{13}{Reachability to a Solution in the Presence of Dissimilar Tasks}{equation.10}{}}
\newlabel{E_X1}{{11}{13}{Reachability to a Solution in the Presence of Dissimilar Tasks}{equation.11}{}}
\newlabel{E_w}{{12}{13}{Reachability to a Solution in the Presence of Dissimilar Tasks}{equation.12}{}}
\newlabel{E_psi}{{13}{13}{Reachability to a Solution in the Presence of Dissimilar Tasks}{equation.13}{}}
\newlabel{lem:upper_weight}{{11}{14}{Reachability to a Solution in the Presence of Dissimilar Tasks}{theorem.11}{}}
\newlabel{lem:upper_arch}{{12}{14}{Reachability to a Solution in the Presence of Dissimilar Tasks}{theorem.12}{}}
\newlabel{thm:intersection}{{13}{14}{Reachability to a Solution in the Presence of Dissimilar Tasks}{theorem.13}{}}
\citation{larson2019derivative}
\@writefile{toc}{\contentsline {section}{\numberline {5}CL Solution}{15}{section.5}\protected@file@percent }
\newlabel{eq:bilevel}{{{bi-level}}{15}{CL Solution}{AMS.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces CL solution, where we change the size of the intersection space by introducing more capacity, through choosing novel hyperparameters}}{16}{figure.5}\protected@file@percent }
\newlabel{fig:solution}{{5}{16}{CL solution, where we change the size of the intersection space by introducing more capacity, through choosing novel hyperparameters}{figure.5}{}}
\newlabel{HJB}{{14}{16}{CL Solution}{theorem.14}{}}
\newlabel{eq:HJB}{{15}{16}{CL Solution}{equation.15}{}}
\newlabel{thm:lower_HJB}{{15}{16}{CL Solution}{theorem.15}{}}
\citation{chakraborty2025understanding,raghavan2021formalizing}
\citation{larson2019derivative}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces How we do this?}}{17}{figure.6}\protected@file@percent }
\newlabel{fig:method}{{6}{17}{How we do this?}{figure.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}\underline  {Step 1:} Architecture Search}{18}{subsection.5.1}\protected@file@percent }
\newlabel{archsearch}{{5.1}{18}{\underline {Step 1:} Architecture Search}{subsection.5.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}\underline  {Step 2:} Low-Rank Transfer}{18}{subsection.5.2}\protected@file@percent }
\newlabel{LRTsec}{{5.2}{18}{\underline {Step 2:} Low-Rank Transfer}{subsection.5.2}{}}
\citation{liu2021survey}
\citation{LORAhu2022lora}
\citation{han2024parameter}
\citation{rusu2016progressive}
\citation{yoon2017lifelong}
\citation{rusu2016progressive}
\citation{yoon2017lifelong}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Neighborhood Directional Direct Search (NDDS)}}{19}{algocf.1}\protected@file@percent }
\newlabel{DDSalg}{{1}{19}{\underline {Step 1:} Architecture Search}{algocf.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Related Works}{19}{section.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Method of low-rank transfer at a single layer}}{20}{figure.7}\protected@file@percent }
\newlabel{LRT}{{7}{20}{Method of low-rank transfer at a single layer}{figure.7}{}}
\citation{CLEASgao2022CLEAS}
\citation{CASpasunuru2019continual}
\citation{SEALgambella2025SEAL}
\citation{LORAhu2022lora}
\citation{wistuba2023continual}
\citation{CLORAmuralidhara2025}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces Main Training Loop}}{21}{algocf.2}\protected@file@percent }
\newlabel{alg:three}{{2}{21}{\underline {Step 2:} Low-Rank Transfer}{algocf.2}{}}
\citation{jax}
\citation{kidger2021equinox}
\citation{raghavan2021formalizing}
\citation{raghavan2023learningcontinuallysequencegraphs}
\@writefile{toc}{\contentsline {section}{\numberline {7}Experimental Results}{22}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Datasets and Metrics:}{22}{subsection.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Implementation Details}{22}{subsection.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.1}Step 1: Baseline Continual Learning Approach}{23}{subsubsection.7.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.2}Step 2: Architecture Search:}{23}{subsubsection.7.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.3} Step 3: Knowledge transfer}{23}{subsubsection.7.2.3}\protected@file@percent }
\citation{loshchilov2017fixing}
\citation{GlorotB10}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Experiment 1: Comparing loss values on training data for baseline continual learning method and method of learning optimal task architecture}}{24}{figure.8}\protected@file@percent }
\newlabel{reg5taskstrain}{{8}{24}{Experiment 1: Comparing loss values on training data for baseline continual learning method and method of learning optimal task architecture}{figure.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Regression}{24}{subsection.7.3}\protected@file@percent }
\newlabel{sec:reg}{{7.3}{24}{Regression}{subsection.7.3}{}}
\citation{raghavan2021formalizing}
\citation{Balaprakash2018DeepHyper}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Experiment 1: Comparing loss values on test data for baseline continual learning method and method of learning optimal task architecture}}{25}{figure.9}\protected@file@percent }
\newlabel{reg5taskstest}{{9}{25}{Experiment 1: Comparing loss values on test data for baseline continual learning method and method of learning optimal task architecture}{figure.9}{}}
\citation{raghavan2021formalizing}
\citation{Lake_2015}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Regression Experiment 2: Comparing loss values on training data for baseline continual learning method and method of learning optimal task architecture}}{26}{figure.10}\protected@file@percent }
\newlabel{reg10taskstrain}{{10}{26}{Regression Experiment 2: Comparing loss values on training data for baseline continual learning method and method of learning optimal task architecture}{figure.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}Classification}{26}{subsection.7.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Regression Experiment 2: Comparing loss values on test data for baseline continual learning method and method of learning optimal task architecture}}{27}{figure.11}\protected@file@percent }
\newlabel{reg10taskstest}{{11}{27}{Regression Experiment 2: Comparing loss values on test data for baseline continual learning method and method of learning optimal task architecture}{figure.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Regression Experiment 3: Comparing loss values on training data for baseline continual learning method and method of learning optimal task architecture when noise in the data is increased}}{27}{figure.12}\protected@file@percent }
\newlabel{reg5noise}{{12}{27}{Regression Experiment 3: Comparing loss values on training data for baseline continual learning method and method of learning optimal task architecture when noise in the data is increased}{figure.12}{}}
\citation{loshchilov2017fixing}
\citation{raghavan2021formalizing}
\citation{pyg}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5}Graph Classification}{28}{subsection.7.5}\protected@file@percent }
\citation{raghavan2023learningcontinuallysequencegraphs}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Classification experiment: Comparing loss values on training data for baseline continual learning method and method of learning optimal task architecture}}{29}{figure.13}\protected@file@percent }
\newlabel{cnn}{{13}{29}{Classification experiment: Comparing loss values on training data for baseline continual learning method and method of learning optimal task architecture}{figure.13}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Conclusion}{29}{section.8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Graph classification experiment: Comparing loss values on training data for baseline continual learning method with method of learning optimal task architecture}}{30}{figure.14}\protected@file@percent }
\newlabel{gnn}{{14}{30}{Graph classification experiment: Comparing loss values on training data for baseline continual learning method with method of learning optimal task architecture}{figure.14}{}}
\bibdata{NNbib.bib}
\bibcite{adegokeHigherDerivativesInverse2016}{{1}{}{{Adegoke \& Layeni}}{{Adegoke and Layeni}}}
\bibcite{Balaprakash2018DeepHyper}{{2}{2018}{{Balaprakash et~al.}}{{Balaprakash, Salim, Uram, Vishwanath, and Wild}}}
\bibcite{bertsekas2012dynamic}{{3}{2012}{{Bertsekas}}{{}}}
\bibcite{biderman2024lora}{{4}{2024}{{Biderman et~al.}}{{Biderman, Portes, Ortiz, Paul, Greengard, Jennings, King, Havens, Chiley, Frankle, et~al.}}}
\bibcite{chakraborty2025understanding}{{5}{2025}{{Chakraborty \& Raghavan}}{{Chakraborty and Raghavan}}}
\bibcite{evans2022partial}{{6}{2022}{{Evans}}{{}}}
\bibcite{pyg}{{7}{2025}{{Fey et~al.}}{{Fey, Lenssen, and contributors}}}
\bibcite{jax}{{8}{2018}{{frostig et~al.}}{{frostig, sohl dickstein, stephens, adigun, bahri, bard, holliday, doucet, levenberg, mayne, oord, pfau, simonyan, slancman, sussex, vadgama, vanhoucke, wehnert, and zoph}}}
\bibcite{SEALgambella2025SEAL}{{9}{2025}{{Gambella et~al.}}{{Gambella, Solar, and Roveri}}}
\bibcite{CLEASgao2022CLEAS}{{10}{2022}{{Gao et~al.}}{{Gao, Luo, Klabjan, and Zhang}}}
\bibcite{GlorotB10}{{11}{2010}{{Glorot \& Bengio}}{{Glorot and Bengio}}}
\bibcite{han2024parameter}{{12}{2024}{{Han et~al.}}{{Han, Gao, Liu, Zhang, and Zhang}}}
\bibcite{LORAhu2022lora}{{13}{2022}{{Hu et~al.}}{{Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, Chen, et~al.}}}
\bibcite{huang2023normalization}{{14}{2023}{{Huang et~al.}}{{Huang, Qin, Zhou, Zhu, Liu, and Shao}}}
\bibcite{kidger2021equinox}{{15}{2021}{{Kidger \& Garcia}}{{Kidger and Garcia}}}
\bibcite{kingmaAdamMethodStochastic2017}{{16}{}{{Kingma \& Ba}}{{Kingma and Ba}}}
\bibcite{kolda2009tensor}{{17}{2009}{{Kolda \& Bader}}{{Kolda and Bader}}}
\bibcite{lai2025reinforcement}{{18}{2025}{{Lai et~al.}}{{Lai, Zhao, Feng, Ma, Liu, Zhao, Lin, Yi, Xie, Zhang, et~al.}}}
\bibcite{Lake_2015}{{19}{2015}{{Lake et~al.}}{{Lake, Salakhutdinov, and Tenenbaum}}}
\bibcite{larson2019derivative}{{20}{2019}{{Larson et~al.}}{{Larson, Menickelly, and Wild}}}
\bibcite{lin2025continual}{{21}{2025}{{Lin et~al.}}{{Lin, Zettlemoyer, Ghosh, Yih, Markosyan, Berges, and O{\u {g}}uz}}}
\bibcite{lin2023theory}{{22}{2023}{{Lin et~al.}}{{Lin, Ju, Liang, and Shroff}}}
\bibcite{liu2021survey}{{23}{2021}{{Liu et~al.}}{{Liu, Sun, Xue, Zhang, Yen, and Tan}}}
\bibcite{loshchilov2017fixing}{{24}{2017}{{Loshchilov et~al.}}{{Loshchilov, Hutter, et~al.}}}
\bibcite{lu2025rethinking}{{25}{2025}{{Lu et~al.}}{{Lu, Yuan, Feng, and Sun}}}
\bibcite{luo2025empirical}{{26}{2025}{{Luo et~al.}}{{Luo, Yang, Meng, Li, Zhou, and Zhang}}}
\bibcite{mahanNonclosednessSetsNeural2021a}{{27}{}{{Mahan et~al.}}{{Mahan, King, and Cloninger}}}
\bibcite{mccloskey1989catastrophic}{{28}{1989}{{McCloskey \& Cohen}}{{McCloskey and Cohen}}}
\bibcite{CLORAmuralidhara2025}{{29}{2025}{{Muralidhara et~al.}}{{Muralidhara, Stricker, and Schuster}}}
\bibcite{CASpasunuru2019continual}{{30}{2019}{{Pasunuru \& Bansal}}{{Pasunuru and Bansal}}}
\bibcite{petersenTopologicalPropertiesSet2021a}{{31}{}{{Petersen et~al.}}{{Petersen, Raslan, and Voigtlaender}}}
\bibcite{raghavan2021formalizing}{{32}{2021}{{Raghavan \& Balaprakash}}{{Raghavan and Balaprakash}}}
\bibcite{raghavan2023learningcontinuallysequencegraphs}{{33}{2023}{{Raghavan \& Balaprakash}}{{Raghavan and Balaprakash}}}
\bibcite{rudin1976principles}{{34}{1976}{{Rudin}}{{}}}
\bibcite{rusu2016progressive}{{35}{2016}{{Rusu et~al.}}{{Rusu, Rabinowitz, Desjardins, Soyer, Kirkpatrick, Kavukcuoglu, Pascanu, and Hadsell}}}
\bibcite{son2021sobolev}{{36}{2021}{{Son et~al.}}{{Son, Jang, Han, and Hwang}}}
\bibcite{weaver2013measure}{{37}{2013}{{Weaver}}{{}}}
\bibcite{wistuba2023continual}{{38}{2023}{{Wistuba et~al.}}{{Wistuba, Sivaprasad, Balles, and Zapper}}}
\bibcite{yoon2017lifelong}{{39}{2017}{{Yoon et~al.}}{{Yoon, Yang, Lee, and Hwang}}}
\bibstyle{tmlr}
\@writefile{toc}{\contentsline {section}{\numberline {A}Appendix}{33}{appendix.A}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {B}Proofs}{33}{appendix.B}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1}Proof of Lemma~\ref {lem:upper_weight}}{33}{subsection.B.1}\protected@file@percent }
\newlabel{sec:upper_weight}{{B.1}{33}{Proof of Lemma~\ref {lem:upper_weight}}{subsection.B.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2}Proof of Lemma~\ref {lem:upper_arch}}{34}{subsection.B.2}\protected@file@percent }
\newlabel{sec:upper_arch}{{B.2}{34}{Proof of Lemma~\ref {lem:upper_arch}}{subsection.B.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.3}Proof of Proposition~\ref {HJB}}{36}{subsection.B.3}\protected@file@percent }
\newlabel{sec:HJB}{{B.3}{36}{Proof of Proposition~\ref {HJB}}{subsection.B.3}{}}
\newlabel{b}{{16}{36}{Proof of Proposition~\ref {HJB}}{equation.16}{}}
\newlabel{a}{{17}{36}{Proof of Proposition~\ref {HJB}}{equation.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.4}Proof of Theorem~\ref {thm:lower_HJB}}{36}{subsection.B.4}\protected@file@percent }
\newlabel{sec:lower_HJB}{{B.4}{36}{Proof of Theorem~\ref {thm:lower_HJB}}{subsection.B.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {C}Introduction}{37}{appendix.C}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {C.1}AWB Transformation Principle}{37}{subsection.C.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {C.2}Key Invariant}{38}{subsection.C.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {C.3}Notation}{38}{subsection.C.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {D}MLP AWB Calculation}{39}{appendix.D}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {D.1}Architecture Specification}{39}{subsection.D.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {D.2}Original Weight Matrices}{39}{subsection.D.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {D.3}AWB Transformation Matrices}{39}{subsection.D.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {D.4}Layer 0 Transformation}{40}{subsection.D.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {D.5}Layer 1 Transformation}{40}{subsection.D.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {D.6}Layer 2 Transformation}{40}{subsection.D.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {D.7}Summary}{41}{subsection.D.7}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces MLP AWB transformation dimensions}}{41}{table.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {E}CNN3D AWB Calculation}{42}{appendix.E}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {E.1}Feed-Forward Layers}{42}{subsection.E.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {E.2}Feed Layer 0 Transformation}{42}{subsection.E.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {E.3}Feed Layer 1 Transformation}{42}{subsection.E.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {E.4}Convolutional Filter Transformation}{43}{subsection.E.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {F}CNN AWB Calculation}{44}{appendix.F}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {F.1}Feed-Forward Layers}{44}{subsection.F.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {F.2}Feed Layer 0 Transformation}{44}{subsection.F.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {F.3}Feed Layer 1 Transformation}{44}{subsection.F.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {G}GCN AWB Calculation}{46}{appendix.G}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {G.1}Architecture Specification}{46}{subsection.G.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {G.2}GCN Layer Transformation}{46}{subsection.G.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {G.3}Feed Layer 0 Transformation}{47}{subsection.G.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {G.4}Feed Layer 1 Transformation}{48}{subsection.G.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {G.5}Feed Layer 2 Transformation}{48}{subsection.G.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {H}Summary and Comparison}{49}{appendix.H}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {H.1}Dimension Preservation Principle}{49}{subsection.H.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Dimension preservation across architectures. $^*$Input changes due to convolutional filter expansion.}}{49}{table.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {H.2}Matrix Dimension Formulas}{49}{subsection.H.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {H.3}Special Cases}{49}{subsection.H.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {I}Conclusion}{49}{appendix.I}\protected@file@percent }
\citation{pontryagin2018mathematical}
\citation{kingma2014adam}
\citation{kirkpatrick2017overcoming}
\citation{zenke2017continual}
\citation{pascanu2013difficulty}
\citation{loshchilov2016sgdr}
\citation{rolnick2019experience}
\citation{li2017learning}
\citation{kolda2003optimization}
\citation{glorot2010understanding}
\@writefile{toc}{\contentsline {section}{\numberline {J}Standard CL Implementation}{50}{appendix.J}\protected@file@percent }
\newlabel{app:standard_cl}{{J}{50}{Standard CL Implementation}{appendix.J}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {J.1}Overview}{50}{subsection.J.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {J.2}Main Algorithm}{52}{subsection.J.2}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {3}{\ignorespaces Complete Hamiltonian Continual Learning Pipeline}}{52}{algocf.3}\protected@file@percent }
\newlabel{alg:complete_hcl}{{3}{52}{Main Algorithm}{algocf.3}{}}
\citation{bishop1995training}
\citation{pontryagin2018mathematical}
\citation{goyal2017accurate}
\citation{kirkpatrick2017overcoming}
\citation{zenke2017continual}
\@writefile{toc}{\contentsline {subsection}{\numberline {J.3}Supporting Algorithms}{53}{subsection.J.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {J.3.1}Hamiltonian Gradient Computation}{53}{subsubsection.J.3.1}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {4}{\ignorespaces Hamiltonian Gradient Computation}}{53}{algocf.4}\protected@file@percent }
\newlabel{alg:hamiltonian}{{4}{53}{Hamiltonian Gradient Computation}{algocf.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {J.3.2}Task Transition with Warmup}{53}{subsubsection.J.3.2}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {5}{\ignorespaces Task Transition with Warmup}}{53}{algocf.5}\protected@file@percent }
\newlabel{alg:warmup}{{5}{53}{Task Transition with Warmup}{algocf.5}{}}
\citation{rolnick2019experience}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {J.3.3}Adaptive Hyperparameter Computation}{54}{subsubsection.J.3.3}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {6}{\ignorespaces Adaptive Learning Rate and Gradient Weights}}{54}{algocf.6}\protected@file@percent }
\newlabel{alg:adaptive}{{6}{54}{Adaptive Hyperparameter Computation}{algocf.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {J.3.4}Balanced Experience Replay}{54}{subsubsection.J.3.4}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {7}{\ignorespaces Balanced Experience Replay Buffer Sampling}}{54}{algocf.7}\protected@file@percent }
\newlabel{alg:replay}{{7}{54}{Balanced Experience Replay}{algocf.7}{}}
\citation{kolda2003optimization}
\citation{loshchilov2016sgdr}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {J.3.5}AWB Architecture Change Decision}{55}{subsubsection.J.3.5}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {8}{\ignorespaces AWB Architecture Change Decision}}{55}{algocf.8}\protected@file@percent }
\newlabel{alg:awb_decision}{{8}{55}{AWB Architecture Change Decision}{algocf.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {J.3.6}Architecture Search}{55}{subsubsection.J.3.6}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {9}{\ignorespaces NDDS Architecture Search}}{55}{algocf.9}\protected@file@percent }
\newlabel{alg:arch_search}{{9}{55}{Architecture Search}{algocf.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {J.4}Hyperparameter Reference}{55}{subsection.J.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {J.5}Implementation Notes}{55}{subsection.J.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {J.5.1}Gradient Normalization}{55}{subsubsection.J.5.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Default Hyperparameters for Standard CL Implementation}}{56}{table.4}\protected@file@percent }
\newlabel{tab:hyperparameters}{{4}{56}{Default Hyperparameters for Standard CL Implementation}{table.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {J.5.2}Learning Rate Schedules}{56}{subsubsection.J.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {J.5.3}JAX Implementation}{56}{subsubsection.J.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {J.5.4}Model Partitioning}{57}{subsubsection.J.5.4}\protected@file@percent }
\gdef \@abspage@last{57}
