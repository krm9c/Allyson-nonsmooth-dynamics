\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{mccloskey1989catastrophic}
\citation{luo2025empirical,lai2025reinforcement,biderman2024lora}
\citation{lin2025continual}
\citation{raghavan2021formalizing,lu2025rethinking,lin2023theory}
\citation{chakraborty2025understanding}
\citation{chakraborty2025understanding}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\citation{CLEASgao2022CLEAS}
\citation{SEALgambella2025SEAL}
\citation{chakraborty2025understanding,raghavan2021formalizing}
\citation{lu2025rethinking}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Contribution}{2}{subsection.1.1}\protected@file@percent }
\citation{kolda2009tensor}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The basic problem of continual learning}}{3}{figure.1}\protected@file@percent }
\newlabel{fig:CL1}{{1}{3}{The basic problem of continual learning}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Organization}{3}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Notation}{3}{subsection.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Continual Learning -- Motivation}{3}{section.2}\protected@file@percent }
\newlabel{sec:motivation}{{2}{3}{Continual Learning -- Motivation}{section.2}{}}
\newlabel{eq:CL_for}{{{Forgetting Loss}}{4}{Continual Learning -- Motivation}{AMS.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The basic problem of continual learning}}{4}{figure.2}\protected@file@percent }
\newlabel{fig:param}{{2}{4}{The basic problem of continual learning}{figure.2}{}}
\citation{chakraborty2025understanding}
\citation{mahanNonclosednessSetsNeural2021a}
\citation{rudin1976principles}
\citation{huang2023normalization}
\newlabel{thm:task_nonstationary_weights}{{1}{5}{Continual Learning -- Motivation}{theorem.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Neural Networks -- Members of a Class of Sobolev Space Functions}{5}{section.3}\protected@file@percent }
\newlabel{defn:sobo}{{2}{5}{Neural Networks -- Members of a Class of Sobolev Space Functions}{theorem.2}{}}
\citation{kingmaAdamMethodStochastic2017}
\citation{petersenTopologicalPropertiesSet2021a}
\citation{adegokeHigherDerivativesInverse2016}
\citation{petersenTopologicalPropertiesSet2021a}
\citation{adegokeHigherDerivativesInverse2016}
\citation{petersenTopologicalPropertiesSet2021a}
\citation{adegokeHigherDerivativesInverse2016}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Different activation functions and the corresponding Sobolev spaces. Many of these results are described in \cite  {petersenTopologicalPropertiesSet2021a} and \cite  {adegokeHigherDerivativesInverse2016} . $\chi $ is an indicator function.}}{6}{table.1}\protected@file@percent }
\newlabel{tab:Sobolev_acti}{{1}{6}{Different activation functions and the corresponding Sobolev spaces. Many of these results are described in \cite {petersenTopologicalPropertiesSet2021a} and \cite {adegokeHigherDerivativesInverse2016} . $\chi $ is an indicator function}{table.1}{}}
\newlabel{defn:NN}{{3}{6}{Neural Networks -- Members of a Class of Sobolev Space Functions}{theorem.3}{}}
\citation{mahanNonclosednessSetsNeural2021a}
\citation{son2021sobolev}
\citation{CASpasunuru2019continual}
\citation{kingmaAdamMethodStochastic2017}
\citation{kingmaAdamMethodStochastic2017}
\citation{bertsekas2012dynamic}
\citation{bertsekas2012dynamic}
\newlabel{defn:NN_learning}{{4}{7}{Neural Networks -- Members of a Class of Sobolev Space Functions}{theorem.4}{}}
\citation{raghavan2021formalizing,chakraborty2025understanding}
\newlabel{eq:arch}{{{Architecture search}}{8}{Continual Learning in $\sobolev $}{AMS.5}{}}
\newlabel{eq:clcum}{{{Cumulative CL}}{8}{Continual Learning in $\sobolev $}{AMS.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Understanding the Existence of the CL Solution}{8}{section.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The actual problem}}{8}{figure.3}\protected@file@percent }
\newlabel{fig:challenge}{{3}{8}{The actual problem}{figure.3}{}}
\citation{weaver2013measure}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Set symmetric difference $A\bigtriangleup B$ represented by the gray-shaded region }}{9}{figure.4}\protected@file@percent }
\newlabel{fig:setsim}{{4}{9}{Set symmetric difference $A\bigtriangleup B$ represented by the gray-shaded region}{figure.4}{}}
\newlabel{defintersect}{{5}{9}{Understanding the Existence of the CL Solution}{theorem.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Existence}{9}{subsection.4.1}\protected@file@percent }
\newlabel{lem:contMeasure}{{6}{10}{Existence}{theorem.6}{}}
\newlabel{boundedremark}{{4}{11}{Existence}{remark.4}{}}
\newlabel{boundedremark}{{5}{11}{Existence}{remark.5}{}}
\newlabel{defabsolutely_continuous}{{8}{12}{Existence}{theorem.8}{}}
\newlabel{thm:contMeasure}{{9}{12}{Existence}{theorem.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Reachability to a Solution in the Presence of Dissimilar Tasks}{12}{subsection.4.2}\protected@file@percent }
\citation{evans2022partial}
\newlabel{lem:taylor}{{10}{13}{Reachability to a Solution in the Presence of Dissimilar Tasks}{theorem.10}{}}
\newlabel{archfinaltaylor}{{9}{13}{Reachability to a Solution in the Presence of Dissimilar Tasks}{equation.9}{}}
\newlabel{taylorexparch}{{10}{13}{Reachability to a Solution in the Presence of Dissimilar Tasks}{equation.10}{}}
\newlabel{E_X1}{{11}{13}{Reachability to a Solution in the Presence of Dissimilar Tasks}{equation.11}{}}
\newlabel{E_w}{{12}{13}{Reachability to a Solution in the Presence of Dissimilar Tasks}{equation.12}{}}
\newlabel{E_psi}{{13}{13}{Reachability to a Solution in the Presence of Dissimilar Tasks}{equation.13}{}}
\newlabel{lem:upper_weight}{{11}{14}{Reachability to a Solution in the Presence of Dissimilar Tasks}{theorem.11}{}}
\newlabel{lem:upper_arch}{{12}{14}{Reachability to a Solution in the Presence of Dissimilar Tasks}{theorem.12}{}}
\newlabel{thm:intersection}{{13}{14}{Reachability to a Solution in the Presence of Dissimilar Tasks}{theorem.13}{}}
\citation{larson2019derivative}
\@writefile{toc}{\contentsline {section}{\numberline {5}CL Solution}{15}{section.5}\protected@file@percent }
\newlabel{eq:bilevel}{{{bi-level}}{15}{CL Solution}{AMS.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces CL solution, where we change the size of the intersection space by introducing more capacity, through choosing novel hyperparameters}}{16}{figure.5}\protected@file@percent }
\newlabel{fig:solution}{{5}{16}{CL solution, where we change the size of the intersection space by introducing more capacity, through choosing novel hyperparameters}{figure.5}{}}
\newlabel{HJB}{{14}{16}{CL Solution}{theorem.14}{}}
\newlabel{eq:HJB}{{15}{16}{CL Solution}{equation.15}{}}
\newlabel{thm:lower_HJB}{{15}{16}{CL Solution}{theorem.15}{}}
\citation{chakraborty2025understanding,raghavan2021formalizing}
\citation{larson2019derivative}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces How we do this?}}{17}{figure.6}\protected@file@percent }
\newlabel{fig:method}{{6}{17}{How we do this?}{figure.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}\underline  {Step 1:} Architecture Search}{18}{subsection.5.1}\protected@file@percent }
\newlabel{archsearch}{{5.1}{18}{\underline {Step 1:} Architecture Search}{subsection.5.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}\underline  {Step 2:} Low-Rank Transfer}{18}{subsection.5.2}\protected@file@percent }
\newlabel{LRTsec}{{5.2}{18}{\underline {Step 2:} Low-Rank Transfer}{subsection.5.2}{}}
\citation{liu2021survey}
\citation{LORAhu2022lora}
\citation{han2024parameter}
\citation{rusu2016progressive}
\citation{yoon2017lifelong}
\citation{rusu2016progressive}
\citation{yoon2017lifelong}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Neighborhood Directional Direct Search (NDDS)}}{19}{algocf.1}\protected@file@percent }
\newlabel{DDSalg}{{1}{19}{\underline {Step 1:} Architecture Search}{algocf.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Related Works}{19}{section.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Method of low-rank transfer at a single layer}}{20}{figure.7}\protected@file@percent }
\newlabel{LRT}{{7}{20}{Method of low-rank transfer at a single layer}{figure.7}{}}
\citation{CLEASgao2022CLEAS}
\citation{CASpasunuru2019continual}
\citation{SEALgambella2025SEAL}
\citation{LORAhu2022lora}
\citation{wistuba2023continual}
\citation{CLORAmuralidhara2025}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces Main Training Loop}}{21}{algocf.2}\protected@file@percent }
\newlabel{alg:three}{{2}{21}{\underline {Step 2:} Low-Rank Transfer}{algocf.2}{}}
\citation{lecun1998mnist}
\citation{Lake_2015}
\citation{pyg}
\citation{lopez2017gradient,chaudhry2018riemannian,diaz2018dont}
\citation{lopez2017gradient}
\@writefile{toc}{\contentsline {section}{\numberline {7}Experimental Results}{22}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Datasets and Metrics}{22}{subsection.7.1}\protected@file@percent }
\citation{chaudhry2018riemannian}
\citation{jax}
\citation{kidger2021equinox}
\citation{raghavan2021formalizing}
\citation{raghavan2023learningcontinuallysequencegraphs}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Experimental Conditions}{23}{subsection.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Implementation Details}{23}{subsection.7.3}\protected@file@percent }
\citation{GlorotB10}
\citation{loshchilov2017fixing}
\citation{GlorotB10}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.1}Condition C1 and C2: Baseline Training}{24}{subsubsection.7.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.2}Condition C3: Architecture Search}{24}{subsubsection.7.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.3}Condition C4: Knowledge Transfer via AWB}{24}{subsubsection.7.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}Regression}{24}{subsection.7.4}\protected@file@percent }
\newlabel{sec:reg}{{7.4}{24}{Regression}{subsection.7.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Experiment 1: Comparing loss values on training data for baseline continual learning method and method of learning optimal task architecture}}{25}{figure.8}\protected@file@percent }
\newlabel{reg5taskstrain}{{8}{25}{Experiment 1: Comparing loss values on training data for baseline continual learning method and method of learning optimal task architecture}{figure.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Experiment 1: Comparing loss values on test data for baseline continual learning method and method of learning optimal task architecture}}{25}{figure.9}\protected@file@percent }
\newlabel{reg5taskstest}{{9}{25}{Experiment 1: Comparing loss values on test data for baseline continual learning method and method of learning optimal task architecture}{figure.9}{}}
\citation{raghavan2021formalizing}
\citation{Balaprakash2018DeepHyper}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Sine regression (2 tasks): Comparison of conditions C1--C4. (a) Test MSE on experience replay. (b) Hamiltonian loss. (c) Average MSE showing C4 achieves 33\% improvement over baseline.}}{27}{figure.10}\protected@file@percent }
\newlabel{fig:sine_2task_comprehensive}{{10}{27}{Sine regression (2 tasks): Comparison of conditions C1--C4. (a) Test MSE on experience replay. (b) Hamiltonian loss. (c) Average MSE showing C4 achieves 33\% improvement over baseline}{figure.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Regression Experiment 2: Comparing loss values on training data for baseline continual learning method and method of learning optimal task architecture}}{27}{figure.11}\protected@file@percent }
\newlabel{reg10taskstrain}{{11}{27}{Regression Experiment 2: Comparing loss values on training data for baseline continual learning method and method of learning optimal task architecture}{figure.11}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Continual learning metrics for sine regression (10 tasks, 3 seeds). Values reported as mean $\pm $ std. Best values are highlighted in \textbf  {bold}. Lower is better for Avg MSE and Forgetting; higher (more positive) is better for BWT and FWT.}}{27}{table.2}\protected@file@percent }
\newlabel{tab:sine_cl_metrics}{{2}{27}{Continual learning metrics for sine regression (10 tasks, 3 seeds). Values reported as mean $\pm $ std. Best values are highlighted in \textbf {bold}. Lower is better for Avg MSE and Forgetting; higher (more positive) is better for BWT and FWT}{table.2}{}}
\citation{raghavan2021formalizing}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Regression Experiment 2: Comparing loss values on test data for baseline continual learning method and method of learning optimal task architecture}}{28}{figure.12}\protected@file@percent }
\newlabel{reg10taskstest}{{12}{28}{Regression Experiment 2: Comparing loss values on test data for baseline continual learning method and method of learning optimal task architecture}{figure.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Sine regression (10 tasks, 3 seeds): Comparison of conditions C1--C4. Shaded regions show $\pm $1 std. (a) Test MSE on experience replay. (b) Hamiltonian loss. (c) Gradient norm. (d) Average MSE. (e) Backward transfer (BWT). (f) Forgetting.}}{29}{figure.13}\protected@file@percent }
\newlabel{fig:sine_10task_comprehensive}{{13}{29}{Sine regression (10 tasks, 3 seeds): Comparison of conditions C1--C4. Shaded regions show $\pm $1 std. (a) Test MSE on experience replay. (b) Hamiltonian loss. (c) Gradient norm. (d) Average MSE. (e) Backward transfer (BWT). (f) Forgetting}{figure.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Regression Experiment 3: Comparing loss values on training data for baseline continual learning method and method of learning optimal task architecture when noise in the data is increased}}{29}{figure.14}\protected@file@percent }
\newlabel{reg5noise}{{14}{29}{Regression Experiment 3: Comparing loss values on training data for baseline continual learning method and method of learning optimal task architecture when noise in the data is increased}{figure.14}{}}
\citation{lecun1998mnist}
\citation{loshchilov2017fixing}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Noisy sine regression (5 tasks, 3 seeds): Comparison of conditions C1--C4 with noise introduced at each task. Shaded regions show $\pm $1 std. (a) Test MSE on experience replay. (b) Hamiltonian loss. (c) Gradient norm. (d) Average MSE. (e) Backward transfer (BWT). (f) Forgetting. C4 (AWB Full) achieves 4\% lower MSE than the baseline while maintaining positive backward transfer.}}{30}{figure.15}\protected@file@percent }
\newlabel{fig:sine_noise_comprehensive}{{15}{30}{Noisy sine regression (5 tasks, 3 seeds): Comparison of conditions C1--C4 with noise introduced at each task. Shaded regions show $\pm $1 std. (a) Test MSE on experience replay. (b) Hamiltonian loss. (c) Gradient norm. (d) Average MSE. (e) Backward transfer (BWT). (f) Forgetting. C4 (AWB Full) achieves 4\% lower MSE than the baseline while maintaining positive backward transfer}{figure.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5}MNIST Classification}{30}{subsection.7.5}\protected@file@percent }
\citation{Lake_2015}
\citation{loshchilov2017fixing}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Hamiltonian loss evolution with architecture changes marked at task boundaries. Top: C3 (architecture search without transfer). Bottom: C4 (AWB full with transfer). Labels show network architecture at each task transition. C4 demonstrates smoother transitions and lower overall loss due to the AWB transfer mechanism.}}{31}{figure.16}\protected@file@percent }
\newlabel{fig:arch_evolution}{{16}{31}{Hamiltonian loss evolution with architecture changes marked at task boundaries. Top: C3 (architecture search without transfer). Bottom: C4 (AWB full with transfer). Labels show network architecture at each task transition. C4 demonstrates smoother transitions and lower overall loss due to the AWB transfer mechanism}{figure.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6}Omniglot Classification}{31}{subsection.7.6}\protected@file@percent }
\citation{raghavan2021formalizing}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces MNIST classification (2 tasks): Comparison of conditions C1--C4. (a) Test accuracy on experience replay. (b) Hamiltonian loss. (c) Average accuracy showing C4 achieves 94.5\% vs 92.8\% for baseline.}}{32}{figure.17}\protected@file@percent }
\newlabel{fig:mnist_comprehensive}{{17}{32}{MNIST classification (2 tasks): Comparison of conditions C1--C4. (a) Test accuracy on experience replay. (b) Hamiltonian loss. (c) Average accuracy showing C4 achieves 94.5\% vs 92.8\% for baseline}{figure.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7}Graph Classification}{32}{subsection.7.7}\protected@file@percent }
\citation{pyg}
\citation{raghavan2023learningcontinuallysequencegraphs}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Classification experiment: Comparing loss values on training data for baseline continual learning method and method of learning optimal task architecture}}{33}{figure.18}\protected@file@percent }
\newlabel{cnn}{{18}{33}{Classification experiment: Comparing loss values on training data for baseline continual learning method and method of learning optimal task architecture}{figure.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Graph classification experiment: Comparing loss values on training data for baseline continual learning method with method of learning optimal task architecture}}{34}{figure.19}\protected@file@percent }
\newlabel{gnn}{{19}{34}{Graph classification experiment: Comparing loss values on training data for baseline continual learning method with method of learning optimal task architecture}{figure.19}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Conclusion}{34}{section.8}\protected@file@percent }
\bibdata{NNbib.bib}
\bibcite{adegokeHigherDerivativesInverse2016}{{1}{}{{Adegoke \& Layeni}}{{Adegoke and Layeni}}}
\bibcite{Balaprakash2018DeepHyper}{{2}{2018}{{Balaprakash et~al.}}{{Balaprakash, Salim, Uram, Vishwanath, and Wild}}}
\bibcite{bertsekas2012dynamic}{{3}{2012}{{Bertsekas}}{{}}}
\bibcite{biderman2024lora}{{4}{2024}{{Biderman et~al.}}{{Biderman, Portes, Ortiz, Paul, Greengard, Jennings, King, Havens, Chiley, Frankle, et~al.}}}
\bibcite{chakraborty2025understanding}{{5}{2025}{{Chakraborty \& Raghavan}}{{Chakraborty and Raghavan}}}
\bibcite{evans2022partial}{{6}{2022}{{Evans}}{{}}}
\bibcite{pyg}{{7}{2025}{{Fey et~al.}}{{Fey, Lenssen, and contributors}}}
\bibcite{jax}{{8}{2018}{{frostig et~al.}}{{frostig, sohl dickstein, stephens, adigun, bahri, bard, holliday, doucet, levenberg, mayne, oord, pfau, simonyan, slancman, sussex, vadgama, vanhoucke, wehnert, and zoph}}}
\bibcite{SEALgambella2025SEAL}{{9}{2025}{{Gambella et~al.}}{{Gambella, Solar, and Roveri}}}
\bibcite{CLEASgao2022CLEAS}{{10}{2022}{{Gao et~al.}}{{Gao, Luo, Klabjan, and Zhang}}}
\bibcite{GlorotB10}{{11}{2010}{{Glorot \& Bengio}}{{Glorot and Bengio}}}
\bibcite{han2024parameter}{{12}{2024}{{Han et~al.}}{{Han, Gao, Liu, Zhang, and Zhang}}}
\bibcite{LORAhu2022lora}{{13}{2022}{{Hu et~al.}}{{Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, Chen, et~al.}}}
\bibcite{huang2023normalization}{{14}{2023}{{Huang et~al.}}{{Huang, Qin, Zhou, Zhu, Liu, and Shao}}}
\bibcite{kidger2021equinox}{{15}{2021}{{Kidger \& Garcia}}{{Kidger and Garcia}}}
\bibcite{kingmaAdamMethodStochastic2017}{{16}{}{{Kingma \& Ba}}{{Kingma and Ba}}}
\bibcite{kolda2009tensor}{{17}{2009}{{Kolda \& Bader}}{{Kolda and Bader}}}
\bibcite{lai2025reinforcement}{{18}{2025}{{Lai et~al.}}{{Lai, Zhao, Feng, Ma, Liu, Zhao, Lin, Yi, Xie, Zhang, et~al.}}}
\bibcite{Lake_2015}{{19}{2015}{{Lake et~al.}}{{Lake, Salakhutdinov, and Tenenbaum}}}
\bibcite{larson2019derivative}{{20}{2019}{{Larson et~al.}}{{Larson, Menickelly, and Wild}}}
\bibcite{lin2025continual}{{21}{2025}{{Lin et~al.}}{{Lin, Zettlemoyer, Ghosh, Yih, Markosyan, Berges, and O{\u {g}}uz}}}
\bibcite{lin2023theory}{{22}{2023}{{Lin et~al.}}{{Lin, Ju, Liang, and Shroff}}}
\bibcite{liu2021survey}{{23}{2021}{{Liu et~al.}}{{Liu, Sun, Xue, Zhang, Yen, and Tan}}}
\bibcite{loshchilov2017fixing}{{24}{2017}{{Loshchilov et~al.}}{{Loshchilov, Hutter, et~al.}}}
\bibcite{lu2025rethinking}{{25}{2025}{{Lu et~al.}}{{Lu, Yuan, Feng, and Sun}}}
\bibcite{luo2025empirical}{{26}{2025}{{Luo et~al.}}{{Luo, Yang, Meng, Li, Zhou, and Zhang}}}
\bibcite{mahanNonclosednessSetsNeural2021a}{{27}{}{{Mahan et~al.}}{{Mahan, King, and Cloninger}}}
\bibcite{mccloskey1989catastrophic}{{28}{1989}{{McCloskey \& Cohen}}{{McCloskey and Cohen}}}
\bibcite{CLORAmuralidhara2025}{{29}{2025}{{Muralidhara et~al.}}{{Muralidhara, Stricker, and Schuster}}}
\bibcite{CASpasunuru2019continual}{{30}{2019}{{Pasunuru \& Bansal}}{{Pasunuru and Bansal}}}
\bibcite{petersenTopologicalPropertiesSet2021a}{{31}{}{{Petersen et~al.}}{{Petersen, Raslan, and Voigtlaender}}}
\bibcite{raghavan2021formalizing}{{32}{2021}{{Raghavan \& Balaprakash}}{{Raghavan and Balaprakash}}}
\bibcite{raghavan2023learningcontinuallysequencegraphs}{{33}{2023}{{Raghavan \& Balaprakash}}{{Raghavan and Balaprakash}}}
\bibcite{rudin1976principles}{{34}{1976}{{Rudin}}{{}}}
\bibcite{rusu2016progressive}{{35}{2016}{{Rusu et~al.}}{{Rusu, Rabinowitz, Desjardins, Soyer, Kirkpatrick, Kavukcuoglu, Pascanu, and Hadsell}}}
\bibcite{son2021sobolev}{{36}{2021}{{Son et~al.}}{{Son, Jang, Han, and Hwang}}}
\bibcite{weaver2013measure}{{37}{2013}{{Weaver}}{{}}}
\bibcite{wistuba2023continual}{{38}{2023}{{Wistuba et~al.}}{{Wistuba, Sivaprasad, Balles, and Zapper}}}
\bibcite{yoon2017lifelong}{{39}{2017}{{Yoon et~al.}}{{Yoon, Yang, Lee, and Hwang}}}
\bibstyle{tmlr}
\@writefile{toc}{\contentsline {section}{\numberline {A}Appendix}{37}{appendix.A}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {B}Proofs}{37}{appendix.B}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1}Proof of Lemma~\ref {lem:upper_weight}}{37}{subsection.B.1}\protected@file@percent }
\newlabel{sec:upper_weight}{{B.1}{37}{Proof of Lemma~\ref {lem:upper_weight}}{subsection.B.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2}Proof of Lemma~\ref {lem:upper_arch}}{38}{subsection.B.2}\protected@file@percent }
\newlabel{sec:upper_arch}{{B.2}{38}{Proof of Lemma~\ref {lem:upper_arch}}{subsection.B.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.3}Proof of Proposition~\ref {HJB}}{40}{subsection.B.3}\protected@file@percent }
\newlabel{sec:HJB}{{B.3}{40}{Proof of Proposition~\ref {HJB}}{subsection.B.3}{}}
\newlabel{b}{{16}{40}{Proof of Proposition~\ref {HJB}}{equation.16}{}}
\newlabel{a}{{17}{40}{Proof of Proposition~\ref {HJB}}{equation.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.4}Proof of Theorem~\ref {thm:lower_HJB}}{40}{subsection.B.4}\protected@file@percent }
\newlabel{sec:lower_HJB}{{B.4}{40}{Proof of Theorem~\ref {thm:lower_HJB}}{subsection.B.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {C}Introduction}{41}{appendix.C}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {C.1}AWB Transformation Principle}{41}{subsection.C.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {C.2}Key Invariant}{42}{subsection.C.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {C.3}Notation}{42}{subsection.C.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {D}MLP AWB Calculation}{43}{appendix.D}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {D.1}Architecture Specification}{43}{subsection.D.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {D.2}Original Weight Matrices}{43}{subsection.D.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {D.3}AWB Transformation Matrices}{43}{subsection.D.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {D.4}Layer 0 Transformation}{44}{subsection.D.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {D.5}Layer 1 Transformation}{44}{subsection.D.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {D.6}Layer 2 Transformation}{44}{subsection.D.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {D.7}Summary}{45}{subsection.D.7}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces MLP AWB transformation dimensions}}{45}{table.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {E}CNN3D AWB Calculation}{46}{appendix.E}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {E.1}Feed-Forward Layers}{46}{subsection.E.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {E.2}Feed Layer 0 Transformation}{46}{subsection.E.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {E.3}Feed Layer 1 Transformation}{46}{subsection.E.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {E.4}Convolutional Filter Transformation}{47}{subsection.E.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {F}CNN AWB Calculation}{48}{appendix.F}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {F.1}Feed-Forward Layers}{48}{subsection.F.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {F.2}Feed Layer 0 Transformation}{48}{subsection.F.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {F.3}Feed Layer 1 Transformation}{48}{subsection.F.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {G}GCN AWB Calculation}{50}{appendix.G}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {G.1}Architecture Specification}{50}{subsection.G.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {G.2}GCN Layer Transformation}{50}{subsection.G.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {G.3}Feed Layer 0 Transformation}{51}{subsection.G.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {G.4}Feed Layer 1 Transformation}{52}{subsection.G.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {G.5}Feed Layer 2 Transformation}{52}{subsection.G.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {H}Summary and Comparison}{53}{appendix.H}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {H.1}Dimension Preservation Principle}{53}{subsection.H.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Dimension preservation across architectures. $^*$Input changes due to convolutional filter expansion.}}{53}{table.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {H.2}Matrix Dimension Formulas}{53}{subsection.H.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {H.3}Special Cases}{53}{subsection.H.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {I}Conclusion}{53}{appendix.I}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {J}Standard CL Implementation}{54}{appendix.J}\protected@file@percent }
\newlabel{app:standard_cl}{{J}{54}{Standard CL Implementation}{appendix.J}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {J.1}Experimental Conditions Overview}{54}{subsection.J.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {J.2}Supporting Algorithms}{54}{subsection.J.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {J.2.1}Task Warmup}{54}{subsubsection.J.2.1}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {7}{\ignorespaces Task Warmup}}{54}{algocf.7}\protected@file@percent }
\newlabel{alg:warmup}{{7}{54}{Task Warmup}{algocf.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {J.2.2}Adaptive Gradient Weights}{54}{subsubsection.J.2.2}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {8}{\ignorespaces Adaptive Gradient Weights}}{54}{algocf.8}\protected@file@percent }
\newlabel{alg:adaptive}{{8}{54}{Adaptive Gradient Weights}{algocf.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {J.2.3}Architecture Search}{54}{subsubsection.J.2.3}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {9}{\ignorespaces NDDS Architecture Search}}{54}{algocf.9}\protected@file@percent }
\newlabel{alg:arch_search}{{9}{54}{Architecture Search}{algocf.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {J.2.4}Train A/B Matrices}{55}{subsubsection.J.2.4}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {10}{\ignorespaces Train A/B Matrices (W Frozen)}}{55}{algocf.10}\protected@file@percent }
\newlabel{alg:train_ab}{{10}{55}{Train A/B Matrices}{algocf.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {J.2.5}Hamiltonian Gradient}{55}{subsubsection.J.2.5}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {11}{\ignorespaces Hamiltonian Gradient}}{55}{algocf.11}\protected@file@percent }
\newlabel{alg:hamiltonian}{{11}{55}{Hamiltonian Gradient}{algocf.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {J.3}Hyperparameter Reference}{55}{subsection.J.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {J.4}Implementation Notes}{55}{subsection.J.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {J.4.1}Gradient Normalization}{55}{subsubsection.J.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {J.4.2}Learning Rate Schedules}{55}{subsubsection.J.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {J.4.3}JAX Implementation}{55}{subsubsection.J.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {J.4.4}Model Partitioning}{55}{subsubsection.J.4.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Default Hyperparameters}}{56}{table.5}\protected@file@percent }
\newlabel{tab:hyperparameters}{{5}{56}{Default Hyperparameters}{table.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Sine Regression Hyperparameters}}{56}{table.6}\protected@file@percent }
\newlabel{tab:sine_hyperparams}{{6}{56}{Sine Regression Hyperparameters}{table.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces MNIST Classification Hyperparameters}}{57}{table.7}\protected@file@percent }
\newlabel{tab:mnist_hyperparams}{{7}{57}{MNIST Classification Hyperparameters}{table.7}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {3}{\ignorespaces *}}{58}{algocf.3}\protected@file@percent }
\newlabel{alg:c1}{{3}{58}{Experimental Conditions Overview}{algocf.3}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {4}{\ignorespaces *}}{58}{algocf.4}\protected@file@percent }
\newlabel{alg:c2}{{4}{58}{Experimental Conditions Overview}{algocf.4}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {5}{\ignorespaces *}}{58}{algocf.5}\protected@file@percent }
\newlabel{alg:c3}{{5}{58}{Experimental Conditions Overview}{algocf.5}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {6}{\ignorespaces *}}{58}{algocf.6}\protected@file@percent }
\newlabel{alg:c4}{{6}{58}{Experimental Conditions Overview}{algocf.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Four experimental conditions for Hamiltonian Continual Learning. (a) C1 uses fixed architecture and constant hyperparameters. (b) C2 adds heuristics: task warmup, cosine LR decay, and adaptive gradient weights. (c) C3 adds architecture search with random reinitialization. (d) C4 replaces random reinitialization with AWB transfer to preserve learned knowledge.}}{58}{figure.20}\protected@file@percent }
\newlabel{fig:alg_conditions}{{20}{58}{Four experimental conditions for Hamiltonian Continual Learning. (a) C1 uses fixed architecture and constant hyperparameters. (b) C2 adds heuristics: task warmup, cosine LR decay, and adaptive gradient weights. (c) C3 adds architecture search with random reinitialization. (d) C4 replaces random reinitialization with AWB transfer to preserve learned knowledge}{figure.20}{}}
\gdef \@abspage@last{58}
