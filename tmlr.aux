\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{mccloskey1989catastrophic}
\citation{luo2025empirical,lai2025reinforcement,biderman2024lora}
\citation{lin2025continual}
\citation{raghavan2021formalizing,lu2025rethinking,lin2023theory}
\citation{chakraborty2025understanding}
\citation{chakraborty2025understanding}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\citation{CLEASgao2022CLEAS}
\citation{SEALgambella2025SEAL}
\citation{chakraborty2025understanding,raghavan2021formalizing}
\citation{lu2025rethinking}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Contribution}{2}{subsection.1.1}\protected@file@percent }
\citation{kolda2009tensor}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The basic problem of continual learning}}{3}{figure.1}\protected@file@percent }
\newlabel{fig:CL1}{{1}{3}{The basic problem of continual learning}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Organization}{3}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Notation}{3}{subsection.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Continual Learning -- Motivation}{3}{section.2}\protected@file@percent }
\newlabel{sec:motivation}{{2}{3}{Continual Learning -- Motivation}{section.2}{}}
\newlabel{eq:CL_for}{{{Forgetting Loss}}{4}{Continual Learning -- Motivation}{AMS.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The basic problem of continual learning}}{4}{figure.2}\protected@file@percent }
\newlabel{fig:param}{{2}{4}{The basic problem of continual learning}{figure.2}{}}
\citation{chakraborty2025understanding}
\citation{mahanNonclosednessSetsNeural2021a}
\citation{rudin1976principles}
\citation{huang2023normalization}
\newlabel{thm:task_nonstationary_weights}{{1}{5}{Continual Learning -- Motivation}{theorem.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Neural Networks -- Members of a Class of Sobolev Space Functions}{5}{section.3}\protected@file@percent }
\newlabel{defn:sobo}{{2}{5}{Neural Networks -- Members of a Class of Sobolev Space Functions}{theorem.2}{}}
\citation{kingmaAdamMethodStochastic2017}
\citation{petersenTopologicalPropertiesSet2021a}
\citation{adegokeHigherDerivativesInverse2016}
\citation{petersenTopologicalPropertiesSet2021a}
\citation{adegokeHigherDerivativesInverse2016}
\citation{petersenTopologicalPropertiesSet2021a}
\citation{adegokeHigherDerivativesInverse2016}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Different activation functions and the corresponding Sobolev spaces. Many of these results are described in \cite  {petersenTopologicalPropertiesSet2021a} and \cite  {adegokeHigherDerivativesInverse2016} . $\chi $ is an indicator function.}}{6}{table.1}\protected@file@percent }
\newlabel{tab:Sobolev_acti}{{1}{6}{Different activation functions and the corresponding Sobolev spaces. Many of these results are described in \cite {petersenTopologicalPropertiesSet2021a} and \cite {adegokeHigherDerivativesInverse2016} . $\chi $ is an indicator function}{table.1}{}}
\newlabel{defn:NN}{{3}{6}{Neural Networks -- Members of a Class of Sobolev Space Functions}{theorem.3}{}}
\citation{mahanNonclosednessSetsNeural2021a}
\citation{son2021sobolev}
\citation{CASpasunuru2019continual}
\citation{kingmaAdamMethodStochastic2017}
\citation{kingmaAdamMethodStochastic2017}
\citation{bertsekas2012dynamic}
\citation{bertsekas2012dynamic}
\newlabel{defn:NN_learning}{{4}{7}{Neural Networks -- Members of a Class of Sobolev Space Functions}{theorem.4}{}}
\citation{raghavan2021formalizing,chakraborty2025understanding}
\newlabel{eq:arch}{{{Architecture search}}{8}{Continual Learning in $\sobolev $}{AMS.5}{}}
\newlabel{eq:clcum}{{{Cumulative CL}}{8}{Continual Learning in $\sobolev $}{AMS.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Understanding the Existence of the CL Solution}{8}{section.4}\protected@file@percent }
\citation{weaver2013measure}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The actual problem}}{9}{figure.3}\protected@file@percent }
\newlabel{fig:challenge}{{3}{9}{The actual problem}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Set symmetric difference $A\bigtriangleup B$ represented by the gray-shaded region }}{9}{figure.4}\protected@file@percent }
\newlabel{fig:setsim}{{4}{9}{Set symmetric difference $A\bigtriangleup B$ represented by the gray-shaded region}{figure.4}{}}
\newlabel{defintersect}{{5}{9}{Understanding the Existence of the CL Solution}{theorem.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Existence}{10}{subsection.4.1}\protected@file@percent }
\newlabel{lem:contMeasure}{{6}{10}{Existence}{theorem.6}{}}
\newlabel{boundedremark}{{4}{11}{Existence}{remark.4}{}}
\newlabel{boundedremark}{{5}{11}{Existence}{remark.5}{}}
\newlabel{defabsolutely_continuous}{{8}{12}{Existence}{theorem.8}{}}
\newlabel{thm:contMeasure}{{9}{12}{Existence}{theorem.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Reachability to a Solution in the Presence of Dissimilar Tasks}{12}{subsection.4.2}\protected@file@percent }
\citation{evans2022partial}
\newlabel{lem:taylor}{{10}{13}{Reachability to a Solution in the Presence of Dissimilar Tasks}{theorem.10}{}}
\newlabel{archfinaltaylor}{{9}{13}{Reachability to a Solution in the Presence of Dissimilar Tasks}{equation.9}{}}
\newlabel{taylorexparch}{{10}{13}{Reachability to a Solution in the Presence of Dissimilar Tasks}{equation.10}{}}
\newlabel{E_X1}{{11}{13}{Reachability to a Solution in the Presence of Dissimilar Tasks}{equation.11}{}}
\newlabel{E_w}{{12}{13}{Reachability to a Solution in the Presence of Dissimilar Tasks}{equation.12}{}}
\newlabel{E_psi}{{13}{13}{Reachability to a Solution in the Presence of Dissimilar Tasks}{equation.13}{}}
\newlabel{lem:upper_weight}{{11}{14}{Reachability to a Solution in the Presence of Dissimilar Tasks}{theorem.11}{}}
\newlabel{lem:upper_arch}{{12}{14}{Reachability to a Solution in the Presence of Dissimilar Tasks}{theorem.12}{}}
\newlabel{thm:intersection}{{13}{14}{Reachability to a Solution in the Presence of Dissimilar Tasks}{theorem.13}{}}
\citation{larson2019derivative}
\@writefile{toc}{\contentsline {section}{\numberline {5}CL Solution}{15}{section.5}\protected@file@percent }
\newlabel{eq:bilevel}{{{bi-level}}{15}{CL Solution}{AMS.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces CL solution, where we change the size of the intersection space by introducing more capacity, through choosing novel hyperparameters}}{16}{figure.5}\protected@file@percent }
\newlabel{fig:solution}{{5}{16}{CL solution, where we change the size of the intersection space by introducing more capacity, through choosing novel hyperparameters}{figure.5}{}}
\newlabel{HJB}{{14}{16}{CL Solution}{theorem.14}{}}
\newlabel{eq:HJB}{{15}{16}{CL Solution}{equation.15}{}}
\newlabel{thm:lower_HJB}{{15}{16}{CL Solution}{theorem.15}{}}
\citation{chakraborty2025understanding,raghavan2021formalizing}
\citation{larson2019derivative}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces How we do this?}}{17}{figure.6}\protected@file@percent }
\newlabel{fig:method}{{6}{17}{How we do this?}{figure.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}\underline  {Step 1:} Architecture Search}{18}{subsection.5.1}\protected@file@percent }
\newlabel{archsearch}{{5.1}{18}{\underline {Step 1:} Architecture Search}{subsection.5.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}\underline  {Step 2:} Low-Rank Transfer}{18}{subsection.5.2}\protected@file@percent }
\newlabel{LRTsec}{{5.2}{18}{\underline {Step 2:} Low-Rank Transfer}{subsection.5.2}{}}
\citation{liu2021survey}
\citation{LORAhu2022lora}
\citation{han2024parameter}
\citation{rusu2016progressive}
\citation{yoon2017lifelong}
\citation{rusu2016progressive}
\citation{yoon2017lifelong}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Neighborhood Directional Direct Search (NDDS)}}{19}{algocf.1}\protected@file@percent }
\newlabel{DDSalg}{{1}{19}{\underline {Step 1:} Architecture Search}{algocf.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Related Works}{19}{section.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Method of low-rank transfer at a single layer}}{20}{figure.7}\protected@file@percent }
\newlabel{LRT}{{7}{20}{Method of low-rank transfer at a single layer}{figure.7}{}}
\citation{CLEASgao2022CLEAS}
\citation{CASpasunuru2019continual}
\citation{SEALgambella2025SEAL}
\citation{LORAhu2022lora}
\citation{wistuba2023continual}
\citation{CLORAmuralidhara2025}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces Main Training Loop}}{21}{algocf.2}\protected@file@percent }
\newlabel{alg:three}{{2}{21}{\underline {Step 2:} Low-Rank Transfer}{algocf.2}{}}
\citation{lecun1998mnist}
\citation{pyg}
\citation{lopez2017gradient,chaudhry2018riemannian,diaz2018dont}
\citation{lopez2017gradient}
\@writefile{toc}{\contentsline {section}{\numberline {7}Experimental Results}{22}{section.7}\protected@file@percent }
\newlabel{sec: experiments}{{7}{22}{Experimental Results}{section.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Datasets and Metrics}{22}{subsection.7.1}\protected@file@percent }
\citation{chaudhry2018riemannian}
\citation{raghavan2021formalizing}
\citation{raghavan2023learningcontinuallysequencegraphs}
\citation{jax}
\citation{kidger2021equinox}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Experimental Conditions}{23}{subsection.7.2}\protected@file@percent }
\citation{raghavan2021formalizing}
\citation{raghavan2023learningcontinuallysequencegraphs}
\citation{GlorotB10}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Implementation Details}{24}{subsection.7.3}\protected@file@percent }
\newlabel{sec:implementdetails}{{7.3}{24}{Implementation Details}{subsection.7.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.1}Experimental Infrastructure}{24}{subsubsection.7.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.2}Experimental Conditions Implementation Details}{24}{subsubsection.7.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Conditions C1 and C2: Baseline Training }{24}{section*.10}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Condition C3: Architecture Search }{24}{section*.11}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Condition C4: Knowledge Transfer via AWB }{24}{section*.12}\protected@file@percent }
\citation{loshchilov2017fixing}
\citation{GlorotB10}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}Regression Experiments}{25}{subsection.7.4}\protected@file@percent }
\newlabel{sec:reg}{{7.4}{25}{Regression Experiments}{subsection.7.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.1}Overview of Experiments}{25}{subsubsection.7.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.2}Experiment 1}{25}{subsubsection.7.4.2}\protected@file@percent }
\citation{Balaprakash2018DeepHyper}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Sine regression (2 tasks): Comparison of conditions C1--C4. (a) Test MSE on experience replay. (b) Hamiltonian loss. (c) Average MSE showing C4 achieves 33\% improvement over baseline.}}{26}{figure.8}\protected@file@percent }
\newlabel{fig:sine_2task_comprehensive}{{8}{26}{Sine regression (2 tasks): Comparison of conditions C1--C4. (a) Test MSE on experience replay. (b) Hamiltonian loss. (c) Average MSE showing C4 achieves 33\% improvement over baseline}{figure.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.3}Experiment 2}{26}{subsubsection.7.4.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Continual learning metrics for sine regression (10 tasks, 3 seeds). Values reported as mean $\pm $ std. Best values are highlighted in \textbf  {bold}. Improvement equates to reduced Avg MSE and Forgetting values and increased BWT and FWT.}}{27}{table.2}\protected@file@percent }
\newlabel{tab:sine_cl_metrics}{{2}{27}{Continual learning metrics for sine regression (10 tasks, 3 seeds). Values reported as mean $\pm $ std. Best values are highlighted in \textbf {bold}. Improvement equates to reduced Avg MSE and Forgetting values and increased BWT and FWT}{table.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Sine regression (10 tasks, 3 seeds): Comparison of conditions C1--C4. Shaded regions show $\pm $1 std. (a) Test MSE on experience replay. (b) Hamiltonian loss. (c) Gradient norm. (d) Average MSE. (e) Backward transfer (BWT). (f) Forgetting.}}{27}{figure.9}\protected@file@percent }
\newlabel{fig:sine_10task_comprehensive}{{9}{27}{Sine regression (10 tasks, 3 seeds): Comparison of conditions C1--C4. Shaded regions show $\pm $1 std. (a) Test MSE on experience replay. (b) Hamiltonian loss. (c) Gradient norm. (d) Average MSE. (e) Backward transfer (BWT). (f) Forgetting}{figure.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.4}Experiment 3}{27}{subsubsection.7.4.4}\protected@file@percent }
\citation{lecun1998mnist}
\citation{loshchilov2017fixing}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Noisy sine regression (5 tasks, 3 seeds): Comparison of conditions C1--C4 with noise introduced at each task. Shaded regions show $\pm $1 std. (a) Test MSE on experience replay. (b) Hamiltonian loss. (c) Gradient norm. (d) Average MSE. (e) Backward transfer (BWT). (f) Forgetting. C4 (AWB Full) achieves 4\% lower MSE than the baseline while maintaining positive backward transfer.}}{28}{figure.10}\protected@file@percent }
\newlabel{fig:sine_noise_comprehensive}{{10}{28}{Noisy sine regression (5 tasks, 3 seeds): Comparison of conditions C1--C4 with noise introduced at each task. Shaded regions show $\pm $1 std. (a) Test MSE on experience replay. (b) Hamiltonian loss. (c) Gradient norm. (d) Average MSE. (e) Backward transfer (BWT). (f) Forgetting. C4 (AWB Full) achieves 4\% lower MSE than the baseline while maintaining positive backward transfer}{figure.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5}MNIST Classification Experiments}{28}{subsection.7.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Hamiltonian loss evolution with architecture changes marked at task boundaries. Top: C3 (architecture search without transfer). Bottom: C4 (AWB full with transfer). Labels show network architecture at each task transition. C4 demonstrates smoother transitions and lower overall loss due to the AWB transfer mechanism.}}{29}{figure.11}\protected@file@percent }
\newlabel{fig:arch_evolution}{{11}{29}{Hamiltonian loss evolution with architecture changes marked at task boundaries. Top: C3 (architecture search without transfer). Bottom: C4 (AWB full with transfer). Labels show network architecture at each task transition. C4 demonstrates smoother transitions and lower overall loss due to the AWB transfer mechanism}{figure.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces MNIST classification (2 tasks): Comparison of conditions C1--C4. (a) Test accuracy on experience replay. (b) Hamiltonian loss. (c) Average accuracy showing C4 achieves 94.5\% vs 92.8\% for baseline.}}{29}{figure.12}\protected@file@percent }
\newlabel{fig:mnist_comprehensive}{{12}{29}{MNIST classification (2 tasks): Comparison of conditions C1--C4. (a) Test accuracy on experience replay. (b) Hamiltonian loss. (c) Average accuracy showing C4 achieves 94.5\% vs 92.8\% for baseline}{figure.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Classification experiment: Comparing loss values on training data for baseline continual learning method and method of learning optimal task architecture for the mnist dataset}}{30}{figure.13}\protected@file@percent }
\newlabel{cnn}{{13}{30}{Classification experiment: Comparing loss values on training data for baseline continual learning method and method of learning optimal task architecture for the mnist dataset}{figure.13}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.5.1}Mnist-ten tasks}{30}{subsubsection.7.5.1}\protected@file@percent }
\newlabel{cnn}{{7.5.1}{30}{Mnist-ten tasks}{subsubsection.7.5.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.5.2}The accuracy drop analysis}{30}{subsubsection.7.5.2}\protected@file@percent }
\newlabel{sec:transform_analysis}{{7.5.2}{30}{The accuracy drop analysis}{subsubsection.7.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Classification experiment: More insight into the jump observed in the mnist data set experiment}}{31}{figure.14}\protected@file@percent }
\newlabel{fig:diagnostics}{{14}{31}{Classification experiment: More insight into the jump observed in the mnist data set experiment}{figure.14}{}}
\citation{pyg}
\citation{raghavan2023learningcontinuallysequencegraphs}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Task difficulty varies with transform angle. Top: transformed digit samples for each task. Task 3 (coral highlight) falls in a difficult region wh +ere combined rotation and shear severely distort digit shapes, resulting in lower accuracy.}}{32}{figure.15}\protected@file@percent }
\newlabel{fig:transform_heatmap}{{15}{32}{Task difficulty varies with transform angle. Top: transformed digit samples for each task. Task 3 (coral highlight) falls in a difficult region wh +ere combined rotation and shear severely distort digit shapes, resulting in lower accuracy}{figure.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6}Graph Classification}{32}{subsection.7.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Comparison of how different number of epochs effect the performance of the smoothing.}}{33}{figure.16}\protected@file@percent }
\newlabel{fig:AB_ablation}{{16}{33}{Comparison of how different number of epochs effect the performance of the smoothing}{figure.16}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.6.1}Synthetic Graph with Domain Shift}{33}{subsubsection.7.6.1}\protected@file@percent }
\newlabel{sec:synthetic_graph}{{7.6.1}{33}{Synthetic Graph with Domain Shift}{subsubsection.7.6.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Graph classification experiment: Comparing loss values on training data for baseline continual learning method with method of learning optimal task architecture}}{34}{figure.17}\protected@file@percent }
\newlabel{gnn}{{17}{34}{Graph classification experiment: Comparing loss values on training data for baseline continual learning method with method of learning optimal task architecture}{figure.17}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Accuracy range summary for synthetic graph experiments. AWB Full achieves significantly higher mean accuracy with a tighter range, indicating consistent performance despite domain shift.}}{34}{table.3}\protected@file@percent }
\newlabel{tab:accuracy_range}{{3}{34}{Accuracy range summary for synthetic graph experiments. AWB Full achieves significantly higher mean accuracy with a tighter range, indicating consistent performance despite domain shift}{table.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Synthetic graph classification with domain shift (10 tasks, 3 seeds). Top row: (a) Test accuracy on experience replay shows AWB Full maintaining high accuracy throughout training; (b) Hamiltonian loss; (c) Gradient norm. Bottom row: (d) AWB Full achieves 89.3\% average accuracy vs 79.4\% for Baseline; (e) Backward transfer; (f) Forgetting measure.}}{35}{figure.18}\protected@file@percent }
\newlabel{fig:synthetic_graph_results}{{18}{35}{Synthetic graph classification with domain shift (10 tasks, 3 seeds). Top row: (a) Test accuracy on experience replay shows AWB Full maintaining high accuracy throughout training; (b) Hamiltonian loss; (c) Gradient norm. Bottom row: (d) AWB Full achieves 89.3\% average accuracy vs 79.4\% for Baseline; (e) Backward transfer; (f) Forgetting measure}{figure.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Per-task accuracy vs domain shift. Grouped bars show final accuracy on each task after training all 10 tasks. The dashed line indicates increasing feature noise ($\sigma $). AWB Full maintains high accuracy despite increasing perturbation, while Baseline shows flat but lower performance.}}{35}{figure.19}\protected@file@percent }
\newlabel{fig:task_accuracy_noise}{{19}{35}{Per-task accuracy vs domain shift. Grouped bars show final accuracy on each task after training all 10 tasks. The dashed line indicates increasing feature noise ($\sigma $). AWB Full maintains high accuracy despite increasing perturbation, while Baseline shows flat but lower performance}{figure.19}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Forgetting statistics for synthetic graph experiments. While AWB Full shows forgetting on more tasks, the absolute magnitude remains small (3.6\% average), representing a favorable trade-off for the 12.5\% accuracy improvement.}}{36}{table.4}\protected@file@percent }
\newlabel{tab:forgetting_stats}{{4}{36}{Forgetting statistics for synthetic graph experiments. While AWB Full shows forgetting on more tasks, the absolute magnitude remains small (3.6\% average), representing a favorable trade-off for the 12.5\% accuracy improvement}{table.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Conclusion}{36}{section.8}\protected@file@percent }
\bibdata{NNbib.bib}
\bibcite{adegokeHigherDerivativesInverse2016}{{1}{}{{Adegoke \& Layeni}}{{Adegoke and Layeni}}}
\bibcite{Balaprakash2018DeepHyper}{{2}{2018}{{Balaprakash et~al.}}{{Balaprakash, Salim, Uram, Vishwanath, and Wild}}}
\bibcite{bertsekas2012dynamic}{{3}{2012}{{Bertsekas}}{{}}}
\bibcite{biderman2024lora}{{4}{2024}{{Biderman et~al.}}{{Biderman, Portes, Ortiz, Paul, Greengard, Jennings, King, Havens, Chiley, Frankle, et~al.}}}
\bibcite{chakraborty2025understanding}{{5}{2025}{{Chakraborty \& Raghavan}}{{Chakraborty and Raghavan}}}
\bibcite{evans2022partial}{{6}{2022}{{Evans}}{{}}}
\bibcite{pyg}{{7}{2025}{{Fey et~al.}}{{Fey, Lenssen, and contributors}}}
\bibcite{jax}{{8}{2018}{{frostig et~al.}}{{frostig, sohl dickstein, stephens, adigun, bahri, bard, holliday, doucet, levenberg, mayne, oord, pfau, simonyan, slancman, sussex, vadgama, vanhoucke, wehnert, and zoph}}}
\bibcite{SEALgambella2025SEAL}{{9}{2025}{{Gambella et~al.}}{{Gambella, Solar, and Roveri}}}
\bibcite{CLEASgao2022CLEAS}{{10}{2022}{{Gao et~al.}}{{Gao, Luo, Klabjan, and Zhang}}}
\bibcite{GlorotB10}{{11}{2010}{{Glorot \& Bengio}}{{Glorot and Bengio}}}
\bibcite{han2024parameter}{{12}{2024}{{Han et~al.}}{{Han, Gao, Liu, Zhang, and Zhang}}}
\bibcite{LORAhu2022lora}{{13}{2022}{{Hu et~al.}}{{Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, Chen, et~al.}}}
\bibcite{huang2023normalization}{{14}{2023}{{Huang et~al.}}{{Huang, Qin, Zhou, Zhu, Liu, and Shao}}}
\bibcite{kidger2021equinox}{{15}{2021}{{Kidger \& Garcia}}{{Kidger and Garcia}}}
\bibcite{kingmaAdamMethodStochastic2017}{{16}{}{{Kingma \& Ba}}{{Kingma and Ba}}}
\bibcite{kolda2009tensor}{{17}{2009}{{Kolda \& Bader}}{{Kolda and Bader}}}
\bibcite{lai2025reinforcement}{{18}{2025}{{Lai et~al.}}{{Lai, Zhao, Feng, Ma, Liu, Zhao, Lin, Yi, Xie, Zhang, et~al.}}}
\bibcite{Lake_2015}{{19}{2015}{{Lake et~al.}}{{Lake, Salakhutdinov, and Tenenbaum}}}
\bibcite{larson2019derivative}{{20}{2019}{{Larson et~al.}}{{Larson, Menickelly, and Wild}}}
\bibcite{lin2025continual}{{21}{2025}{{Lin et~al.}}{{Lin, Zettlemoyer, Ghosh, Yih, Markosyan, Berges, and O{\u {g}}uz}}}
\bibcite{lin2023theory}{{22}{2023}{{Lin et~al.}}{{Lin, Ju, Liang, and Shroff}}}
\bibcite{liu2021survey}{{23}{2021}{{Liu et~al.}}{{Liu, Sun, Xue, Zhang, Yen, and Tan}}}
\bibcite{loshchilov2017fixing}{{24}{2017}{{Loshchilov et~al.}}{{Loshchilov, Hutter, et~al.}}}
\bibcite{lu2025rethinking}{{25}{2025}{{Lu et~al.}}{{Lu, Yuan, Feng, and Sun}}}
\bibcite{luo2025empirical}{{26}{2025}{{Luo et~al.}}{{Luo, Yang, Meng, Li, Zhou, and Zhang}}}
\bibcite{mahanNonclosednessSetsNeural2021a}{{27}{}{{Mahan et~al.}}{{Mahan, King, and Cloninger}}}
\bibcite{mccloskey1989catastrophic}{{28}{1989}{{McCloskey \& Cohen}}{{McCloskey and Cohen}}}
\bibcite{CLORAmuralidhara2025}{{29}{2025}{{Muralidhara et~al.}}{{Muralidhara, Stricker, and Schuster}}}
\bibcite{CASpasunuru2019continual}{{30}{2019}{{Pasunuru \& Bansal}}{{Pasunuru and Bansal}}}
\bibcite{petersenTopologicalPropertiesSet2021a}{{31}{}{{Petersen et~al.}}{{Petersen, Raslan, and Voigtlaender}}}
\bibcite{raghavan2021formalizing}{{32}{2021}{{Raghavan \& Balaprakash}}{{Raghavan and Balaprakash}}}
\bibcite{raghavan2023learningcontinuallysequencegraphs}{{33}{2023}{{Raghavan \& Balaprakash}}{{Raghavan and Balaprakash}}}
\bibcite{rudin1976principles}{{34}{1976}{{Rudin}}{{}}}
\bibcite{rusu2016progressive}{{35}{2016}{{Rusu et~al.}}{{Rusu, Rabinowitz, Desjardins, Soyer, Kirkpatrick, Kavukcuoglu, Pascanu, and Hadsell}}}
\bibcite{son2021sobolev}{{36}{2021}{{Son et~al.}}{{Son, Jang, Han, and Hwang}}}
\bibcite{weaver2013measure}{{37}{2013}{{Weaver}}{{}}}
\bibcite{wistuba2023continual}{{38}{2023}{{Wistuba et~al.}}{{Wistuba, Sivaprasad, Balles, and Zapper}}}
\bibcite{yoon2017lifelong}{{39}{2017}{{Yoon et~al.}}{{Yoon, Yang, Lee, and Hwang}}}
\bibstyle{tmlr}
\@writefile{toc}{\contentsline {section}{\numberline {A}Appendix}{39}{appendix.A}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {B}Proofs}{39}{appendix.B}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1}Proof of Lemma~\ref {lem:upper_weight}}{39}{subsection.B.1}\protected@file@percent }
\newlabel{sec:upper_weight}{{B.1}{39}{Proof of Lemma~\ref {lem:upper_weight}}{subsection.B.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2}Proof of Lemma~\ref {lem:upper_arch}}{40}{subsection.B.2}\protected@file@percent }
\newlabel{sec:upper_arch}{{B.2}{40}{Proof of Lemma~\ref {lem:upper_arch}}{subsection.B.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.3}Proof of Proposition~\ref {HJB}}{42}{subsection.B.3}\protected@file@percent }
\newlabel{sec:HJB}{{B.3}{42}{Proof of Proposition~\ref {HJB}}{subsection.B.3}{}}
\newlabel{b}{{16}{42}{Proof of Proposition~\ref {HJB}}{equation.16}{}}
\newlabel{a}{{17}{42}{Proof of Proposition~\ref {HJB}}{equation.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.4}Proof of Theorem~\ref {thm:lower_HJB}}{42}{subsection.B.4}\protected@file@percent }
\newlabel{sec:lower_HJB}{{B.4}{42}{Proof of Theorem~\ref {thm:lower_HJB}}{subsection.B.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {C}Feedforawrd Neural Network $AWB$ Matrix Calculation Example}{44}{appendix.C}\protected@file@percent }
\newlabel{app: MLP AB}{{C}{44}{Feedforawrd Neural Network $AWB$ Matrix Calculation Example}{appendix.C}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.1}Original Architecture and Weights Matrix Specifications}{44}{subsection.C.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {C.2}Optimal Architecture Specifications}{44}{subsection.C.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {C.3}Determining Appropriate $A$ and $B$ Matrices}{44}{subsection.C.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {C.4}Computing and Verifying Matrix $V$}{45}{subsection.C.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {C.5}Summary}{46}{subsection.C.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces FFN transformation dimensions}}{46}{table.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {D}CNN3D AWB Calculation}{47}{appendix.D}\protected@file@percent }
\newlabel{app: CNN3d AB}{{D}{47}{CNN3D AWB Calculation}{appendix.D}{}}
\newlabel{app: CNN AB}{{D}{47}{CNN3D AWB Calculation}{appendix.D}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D.1}Original Architecture}{47}{subsection.D.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {D.2}Optimal Architecture Specifications}{47}{subsection.D.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {D.3}Determining Appropriate $A$ and $B$ Matrices}{47}{subsection.D.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {E}GCN AWB Calculation}{50}{appendix.E}\protected@file@percent }
\newlabel{app: GCN}{{E}{50}{GCN AWB Calculation}{appendix.E}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {E.1}Architecture Specification}{50}{subsection.E.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {E.2}GCN Layer Transformation}{50}{subsection.E.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {E.3}Feed Layer 0 Transformation}{51}{subsection.E.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {E.4}Feed Layer 1 Transformation}{52}{subsection.E.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {E.5}Feed Layer 2 Transformation}{52}{subsection.E.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {F}Summary and Comparison}{53}{appendix.F}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {F.1}Dimension Preservation Principle}{53}{subsection.F.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Dimension preservation across architectures. $^*$Input changes due to convolutional filter expansion.}}{53}{table.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {F.2}Matrix Dimension Formulas}{53}{subsection.F.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {F.3}Special Cases}{53}{subsection.F.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {G}Conclusion}{53}{appendix.G}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {H}Standard CL Implementation}{54}{appendix.H}\protected@file@percent }
\newlabel{app:standard_cl}{{H}{54}{Standard CL Implementation}{appendix.H}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {H.1}Experimental Conditions Overview \& Algorithm}{54}{subsection.H.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {H.2}Supporting Algorithms to Figure~\ref {fig:alg_conditions}}{54}{subsection.H.2}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {7}{\ignorespaces Task Warmup}}{54}{algocf.7}\protected@file@percent }
\newlabel{alg:warmup}{{7}{54}{Supporting Algorithms to Figure~\ref {fig:alg_conditions}}{algocf.7}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {8}{\ignorespaces Adaptive Gradient Weights}}{54}{algocf.8}\protected@file@percent }
\newlabel{alg:adaptive}{{8}{54}{Supporting Algorithms to Figure~\ref {fig:alg_conditions}}{algocf.8}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {9}{\ignorespaces NDDS Architecture Search}}{54}{algocf.9}\protected@file@percent }
\newlabel{alg:arch_search}{{9}{54}{Supporting Algorithms to Figure~\ref {fig:alg_conditions}}{algocf.9}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {10}{\ignorespaces Train A/B Matrices (W Frozen)}}{55}{algocf.10}\protected@file@percent }
\newlabel{alg:train_ab}{{10}{55}{Supporting Algorithms to Figure~\ref {fig:alg_conditions}}{algocf.10}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {11}{\ignorespaces Hamiltonian Gradient}}{55}{algocf.11}\protected@file@percent }
\newlabel{alg:hamiltonian}{{11}{55}{Supporting Algorithms to Figure~\ref {fig:alg_conditions}}{algocf.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {H.3}Hyperparameter Reference}{55}{subsection.H.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {H.4}Implementation Notes}{55}{subsection.H.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {H.4.1}Gradient Normalization}{55}{subsubsection.H.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {H.4.2}Learning Rate Schedules}{55}{subsubsection.H.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {H.4.3}JAX Implementation}{55}{subsubsection.H.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {H.4.4}Model Partitioning}{55}{subsubsection.H.4.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Default Hyperparameters}}{56}{table.7}\protected@file@percent }
\newlabel{tab:hyperparameters}{{7}{56}{Default Hyperparameters}{table.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces Sine Regression Hyperparameters}}{56}{table.8}\protected@file@percent }
\newlabel{tab:sine_hyperparams}{{8}{56}{Sine Regression Hyperparameters}{table.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {9}{\ignorespaces MNIST Classification Hyperparameters}}{57}{table.9}\protected@file@percent }
\newlabel{tab:mnist_hyperparams}{{9}{57}{MNIST Classification Hyperparameters}{table.9}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {3}{\ignorespaces *}}{58}{algocf.3}\protected@file@percent }
\newlabel{alg:c1}{{3}{58}{Experimental Conditions Overview \& Algorithm}{algocf.3}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {4}{\ignorespaces *}}{58}{algocf.4}\protected@file@percent }
\newlabel{alg:c2}{{4}{58}{Experimental Conditions Overview \& Algorithm}{algocf.4}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {5}{\ignorespaces *}}{58}{algocf.5}\protected@file@percent }
\newlabel{alg:c3}{{5}{58}{Experimental Conditions Overview \& Algorithm}{algocf.5}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {6}{\ignorespaces *}}{58}{algocf.6}\protected@file@percent }
\newlabel{alg:c4}{{6}{58}{Experimental Conditions Overview \& Algorithm}{algocf.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Four experimental conditions for Hamiltonian Continual Learning. (a) C1 uses fixed architecture and constant hyperparameters. (b) C2 adds heuristics: task warmup, cosine LR decay, and adaptive gradient weights. (c) C3 adds architecture search with random reinitialization. (d) C4 replaces random reinitialization with AWB transfer to preserve learned knowledge.}}{58}{figure.20}\protected@file@percent }
\newlabel{fig:alg_conditions}{{20}{58}{Four experimental conditions for Hamiltonian Continual Learning. (a) C1 uses fixed architecture and constant hyperparameters. (b) C2 adds heuristics: task warmup, cosine LR decay, and adaptive gradient weights. (c) C3 adds architecture search with random reinitialization. (d) C4 replaces random reinitialization with AWB transfer to preserve learned knowledge}{figure.20}{}}
\gdef \@abspage@last{58}
